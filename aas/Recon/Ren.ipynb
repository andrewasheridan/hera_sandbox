{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hera stuff\n",
    "import hera_cal as hc\n",
    "import pyuvdata\n",
    "import aipy\n",
    "import uvtools\n",
    "\n",
    "# general\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, io # flushing notebook output\n",
    "import time # for random seed\n",
    "\n",
    "# NN\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data for a JD. \n",
    "data_directory = '../zen_data/'\n",
    "miriad_path = data_directory + 'zen.2458098.58037.xx.HH.uv'\n",
    "\n",
    "uvd = pyuvdata.UVData()\n",
    "uvd.read_miriad(miriad_path)\n",
    "\n",
    "# gains for same data \n",
    "calfits_path = data_directory + 'zen.2458098.58037.xx.HH.uv.abs.calfits'\n",
    "gains, flags = hc.io.load_cal(calfits_path)\n",
    "\n",
    "# get the redundancies for that data\n",
    "aa = hc.utils.get_aa_from_uv(uvd)\n",
    "info = hc.omni.aa_to_info(aa)\n",
    "red_bls = np.array(info.get_reds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes the antennas are bad, make sure they are good\n",
    "def ants_good(bl, gain_keys):\n",
    "    \"\"\"Returns True if both antennas are in gains.keys\"\"\"\n",
    "    ants = [a[0] for a in gain_keys] # gain_keys is a list of tuples like (ant, pol)\n",
    "    if bl[0] in ants and bl[1] in ants:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def gen_flatness_from_good_red_bls(good_red_bls):\n",
    "\n",
    "    num_keys = 0\n",
    "    while num_keys < 2:\n",
    "\n",
    "        data = {bl: uvd.get_data(bl) for bl in np.random.choice(good_red_bls) if ants_good(bl, gains.keys())}\n",
    "        num_keys = len(data.keys())\n",
    "        \n",
    "    rnd_keys = random.sample(data.keys(), 2)\n",
    "    \n",
    "    a, b = rnd_keys[0][0], rnd_keys[0][1]\n",
    "    c, d = rnd_keys[1][0], rnd_keys[1][1]\n",
    "    S = data[a, b]*data[c, d].conjugate() * gains[(a,'x')].conjugate() * gains[(b,'x')] * gains[(c,'x')] * gains[(d,'x')].conjugate()\n",
    " \n",
    "    return S\n",
    "\n",
    "def gen_mega_flatness(good_red_bls, num = 250):\n",
    "    mega_S = []\n",
    "    for _ in range(num):\n",
    "        mega_S.append(gen_flatness_from_good_red_bls(good_red_bls))\n",
    "    return np.concatenate(mega_S, axis = 0)\n",
    "\n",
    "def gen_plot(predicted_values, actual_values, output_scaler):\n",
    "    \"\"\"Create a prediction plot and save to byte string.\"\"\"\n",
    "    \n",
    "    prediction_unscaled = output_scaler.inverse_transform(predicted_values)\n",
    "    actual_unscaled = output_scaler.inverse_transform(actual_values)\n",
    "\n",
    "    sorting_idx = np.argsort(actual_unscaled.T[0])\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize = (5, 3), dpi = 144)\n",
    "\n",
    "    ax.plot(prediction_unscaled.T[0][sorting_idx],\n",
    "            linestyle = 'none', marker = '.', markersize = 1,\n",
    "            color = 'darkblue')\n",
    "    \n",
    "    ax.plot(actual_unscaled.T[0][sorting_idx],\n",
    "            linestyle = 'none', marker = '.', markersize = 1, alpha = 0.50,\n",
    "            color = '#E50000')       \n",
    "    \n",
    "    ax.set_title('std: %.9f' %np.std(prediction_unscaled.T[0][sorting_idx] - actual_unscaled.T[0][sorting_idx]))\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', dpi = 144)\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "\n",
    "    return buf.getvalue()\n",
    "\n",
    "def get_acc(predicted_values, actual_values, output_scaler, val = 0.0005):\n",
    "    prediction_unscaled = output_scaler.inverse_transform(predicted_values)\n",
    "    actual_unscaled = output_scaler.inverse_transform(actual_values)\n",
    "    perc = abs(actual_unscaled.T[0] - prediction_unscaled.T[0])\n",
    "    acc = np.array(perc < val, dtype = int)\n",
    "    return np.mean(acc)\n",
    "\n",
    "def clean_phase(num_entries = 15000, tau_low = -0.04, tau_high = 0.04):\n",
    "\n",
    "    # random.uniform is half open [low, high) not sure if that matters\n",
    "    tau = np.random.uniform(low = tau_low, high = tau_high, size = num_entries) \n",
    "    nu = np.arange(1024)\n",
    "    \n",
    "    clean_phase_data = np.empty((num_entries, 1024))\n",
    "    for i in range(num_entries):\n",
    "        clean_phase_data[i] = np.angle(np.exp(2*np.pi*1j * (tau[i]*nu + np.random.uniform())))\n",
    "    \n",
    "    return clean_phase_data, tau.reshape(-1, 1)\n",
    "\n",
    "def get_bad_red_idxs(red_bls, gain_keys):\n",
    "    x = 0\n",
    "    bad_indices = []\n",
    "    for i, bls in enumerate(red_bls):\n",
    "        for bl in bls:\n",
    "            if ants_good(bl, gain_keys) == True:\n",
    "                pass\n",
    "            else:\n",
    "                x += 1\n",
    "        if len(bls) == x:\n",
    "            bad_indices.append(i)\n",
    "        x = 0\n",
    "    return bad_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Random good redundant bls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 11, 16, 36]\n"
     ]
    }
   ],
   "source": [
    "bad_indices = get_bad_red_idxs(red_bls, gains.keys()); print(bad_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_red_bls = np.delete(red_bls, bad_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get a random good red bls\n",
    "random_red_bls = lambda : np.random.choice(good_red_bls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, layer_nodes, number_of_inputs, number_of_outputs, learning_rate, sigma = 0.00625):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        with tf.variable_scope('input_X'):\n",
    "            self.X  = tf.placeholder(tf.float32, shape = (None, number_of_inputs))\n",
    "    \n",
    "        with tf.variable_scope('input_y'):\n",
    "            self.y  = tf.placeholder(tf.float32, shape = (None, 1))\n",
    "            \n",
    "        with tf.variable_scope('keep'):\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "            \n",
    "        layers = []\n",
    "        with tf.variable_scope('input_layer'):\n",
    "            b = tf.get_variable(name = 'biases_input',\n",
    "                                shape = [layer_nodes[0]],\n",
    "                                initializer = tf.zeros_initializer())\n",
    "            \n",
    "            w = tf.get_variable(name = 'weights_input',\n",
    "                                shape  = [number_of_inputs, layer_nodes[0]],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            layer = tf.nn.leaky_relu(tf.matmul(self.X, w) + b)\n",
    "            layers.append(layer)\n",
    "        \n",
    "        with tf.variable_scope('dropout'):\n",
    "            dropout = tf.nn.dropout(layers[0], self.keep_prob)            \n",
    "            layers.append(dropout)\n",
    "            \n",
    "        # hidden layers\n",
    "        for i in range(len(layer_nodes)):\n",
    "            if i > 0:\n",
    "                with tf.variable_scope('layer_%d' %i):\n",
    "                    b = tf.get_variable(name = 'biases_%d' %i,\n",
    "                                        shape = [layer_nodes[i]],\n",
    "                                        initializer = tf.zeros_initializer())\n",
    "                    \n",
    "                    w = tf.get_variable(name = 'weights_%d' %i,\n",
    "                                        shape  = [layer_nodes[i-1], layer_nodes[i]],\n",
    "                                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "                    \n",
    "                    layer = tf.nn.relu6(tf.matmul(layers[i], w) + b)\n",
    "                    layers.append(layer)\n",
    "                    \n",
    "        with tf.variable_scope('output_layer'):\n",
    "            b = tf.get_variable(name = \"biases_out\",\n",
    "                                shape = [number_of_outputs],\n",
    "                                initializer = tf.zeros_initializer())\n",
    "            \n",
    "            w = tf.get_variable(name = \"weights_out\",\n",
    "                                shape  = [layer_nodes[-1], number_of_outputs],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.prediction = tf.nn.relu6(tf.matmul(layers[-1], w) + b)\n",
    "            \n",
    "        with tf.variable_scope('cost'):\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.squared_difference(self.prediction, self.y))\n",
    "            \n",
    "        with tf.variable_scope('acc_test'):\n",
    "            self.acc_test = tf.placeholder(tf.float32)\n",
    "            \n",
    "        with tf.variable_scope('acc_2'):\n",
    "            \n",
    "            dist = tf.contrib.distributions.Normal(0., sigma)\n",
    "            self.acc = tf.divide(1.,1. + dist.prob(self.prediction - self.y))\n",
    "\n",
    "     \n",
    "        with tf.variable_scope('train'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=1e-08).minimize(-dist.prob(self.prediction - self.y))\n",
    "\n",
    "        with tf.variable_scope('image'):\n",
    "            self.image_buf = tf.placeholder(tf.string, shape=[])\n",
    "            epoch_image = tf.expand_dims(tf.image.decode_png(self.image_buf, channels=4), 0)\n",
    "            \n",
    "        with tf.variable_scope('logging'):                                                                                \n",
    "            \n",
    "            tf.summary.image('prediction_vs_actual', epoch_image)\n",
    "            tf.summary.histogram('predictions', self.prediction)\n",
    "            tf.summary.scalar('cost', self.cost)\n",
    "            tf.summary.scalar('inverse_gaussian', tf.reduce_mean(self.acc))\n",
    "            tf.summary.scalar('accuracy_5ns', self.acc_test)\n",
    "            self.summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs  = 1024\n",
    "number_of_outputs =  1\n",
    "layer_nodes = [2048,2048,2048]\n",
    "learning_rate = 0.00001\n",
    "sigma = 0.00625\n",
    "NN_test_G = NN(layer_nodes, number_of_inputs, number_of_outputs, learning_rate, sigma)\n",
    "\n",
    "testing_data_percentage = 0.20\n",
    "keep_prob_rate = 0.50\n",
    "model_save_interval = 10\n",
    "num_epochs = 50\n",
    "num_batches = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/Ren-F/trained_model.ckpt-19\n",
      "epoch:    0 -- testing_cost: 0.0000000000 -- batch: 86"
     ]
    }
   ],
   "source": [
    "network = NN_test_G\n",
    "log_direc = 'logs/Ren-G'\n",
    "saver = tf.train.Saver()\n",
    "# inputs, outputs = \n",
    "with tf.Session() as session:\n",
    "\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "    saver.restore(session,'logs/Ren-F/trained_model.ckpt-19')\n",
    "    \n",
    "    training_writer = tf.summary.FileWriter(log_direc + '/training', session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(log_direc + '/testing', session.graph)\n",
    "    model_save_location = log_direc + '/trained_model.ckpt'    \n",
    "    testing_cost = 0\n",
    "\n",
    "    nu = np.arange(1024)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        \n",
    "\n",
    "#         inputs, outputs = clean_phase(num_entries = 15000, tau_low = -0.040, tau_high = 0.040)\n",
    "#         if 0 <= epoch < 30:\n",
    "#             inputs, outputs = clean_phase(num_entries = 10000, tau_low = -(0.010 + (epoch/1000.)), tau_high = 0.010 + epoch/1000.)\n",
    "#         if 30 <= epoch < 60:\n",
    "#             inputs, outputs = clean_phase(num_entries = 10000, tau_low = -0.040, tau_high = 0.040)\n",
    "#         if 60 <= epoch:\n",
    "        mega_delay = np.random.uniform(low = -.040, high = 0.040, size = (60 * 500, 1))\n",
    "        inputs = np.angle((gen_mega_flatness(good_red_bls, num = 500)) * np.exp(2*np.pi*1j*(mega_delay*nu + np.random.uniform(size = (60*500, 1)))))\n",
    "        outputs = mega_delay\n",
    "        # scale data\n",
    "        \n",
    "        input_scaler  = MinMaxScaler(feature_range = (0,1))\n",
    "        output_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "        scaled_input  =  input_scaler.fit_transform(inputs)\n",
    "        scaled_output = output_scaler.fit_transform(outputs)\n",
    "        \n",
    "        #split data \n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_input, scaled_output,\n",
    "                                                test_size = testing_data_percentage,\n",
    "                                                random_state = np.random.seed(int(time.time())))\n",
    "        \n",
    "\n",
    "        # break training data into batches\n",
    "        split_permutation = np.random.permutation(X_train.shape[0])\n",
    "        X_train_batches = np.vsplit(X_train[split_permutation], num_batches)\n",
    "        y_train_batches = np.vsplit(y_train[split_permutation], num_batches)\n",
    "\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            session.run(network.optimizer,\n",
    "                        feed_dict = {network.X: X_train_batches[i], \n",
    "                                     network.y: y_train_batches[i], \n",
    "                                     network.keep_prob : keep_prob_rate})\n",
    "            sys.stdout.write('\\repoch: {:4.0f} -- testing_cost: {:2.10f} -- batch: {:1.0f}'.format(epoch, testing_cost,i))\n",
    "\n",
    "        # training summaries\n",
    "        prediction_scaled_test = session.run(network.prediction,\n",
    "                                             feed_dict = {network.X: X_train,\n",
    "                                                          network.keep_prob : 1.00})\n",
    "\n",
    "        training_cost, training_summary = session.run([network.cost, network.summary],\n",
    "                                                      feed_dict = {network.X: X_train,\n",
    "                                                                   network.y: y_train,\n",
    "                                                                   network.keep_prob : 1.00,\n",
    "                                                                   network.image_buf: gen_plot(prediction_scaled_test,\n",
    "                                                                                               y_train, output_scaler),\n",
    "                                                                   network.acc_test : get_acc(prediction_scaled_test,\n",
    "                                                                                               y_train, output_scaler)})\n",
    "        training_writer.add_summary(training_summary, epoch)\n",
    "        training_writer.flush()  \n",
    "\n",
    "         # testing summaries\n",
    "        prediction_scaled_test = session.run(network.prediction,\n",
    "                                             feed_dict = {network.X: X_test,\n",
    "                                                          network.keep_prob : 1.00})\n",
    "\n",
    "        testing_cost, testing_summary = session.run([network.cost, network.summary],\n",
    "                                                    feed_dict = {network.X: X_test,\n",
    "                                                                 network.y: y_test,\n",
    "                                                                 network.keep_prob : 1.00,\n",
    "                                                                 network.image_buf: gen_plot(prediction_scaled_test,\n",
    "                                                                                             y_test, output_scaler),\n",
    "                                                                 network.acc_test: get_acc(prediction_scaled_test,\n",
    "                                                                                             y_test, output_scaler)})\n",
    "\n",
    "        testing_writer.add_summary(testing_summary, epoch)\n",
    "        testing_writer.flush()\n",
    "\n",
    "        # save model\n",
    "        if (epoch + 1) % model_save_interval == 0:\n",
    "            saver.save(session, model_save_location, epoch)\n",
    "\n",
    "    # save last model\n",
    "    saver.save(session, model_save_location, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three large layers (2048 each). epoch time ~90 seconds.\n",
    "\n",
    "test_A: no learning. changing optimizer minimzation function to cost\n",
    "\n",
    "test_B: 20 epochs brought pred near actl.\n",
    "\n",
    "test_C: loading test_B weights, changing minimizatin to acc. preds drop to zero in first epoch.\n",
    "\n",
    "test_D: loading test_B weights, increased sigma. preds o=dont drop to zero, though ineffecgive\n",
    "\n",
    "test_E: loading test_B weights, changing minimizatin to cost. increased learning rate 5x compared to B. Not great.\n",
    "\n",
    "test_F: loading test_B weights, dirty data. okay i guess.\n",
    "\n",
    "test_G: F-19, negative normal minimize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
