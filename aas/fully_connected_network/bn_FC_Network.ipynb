{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import prod, savez, load\n",
    "from pprint import pprint\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Network(object):\n",
    "    \"\"\"A neural network of fully connected layers.\n",
    "    \n",
    "    First layer has Leaky_ReLU activation with a trainable alpha. Other layers have ReLU activation.\n",
    "    Input sample has dropout at rate 1 - sample_keep_prob.\n",
    "    activations have dropout at rate 1 - fcl_keep_prob.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 layer_nodes,\n",
    "                 dtype = tf.float32,\n",
    "                 num_freq_channels = 1024,\n",
    "                 learning_rate = 0.0001,\n",
    "                 cost_name = 'MSE',\n",
    "                 threshold = 0.00625,\n",
    "                 g_shift = 0.01,\n",
    "                 log_dir = 'logs/'):\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self.params = {\"num_freq_channels\" : num_freq_channels,\n",
    "                       \"learning_rate\" : learning_rate,\n",
    "                       \"layer_nodes\" :layer_nodes,\n",
    "                       \"cost_name\" : cost_name,\n",
    "                       \"threshold\" : threshold,\n",
    "                       \"g_shift\" : g_shift,\n",
    "                       \"dtype\" : dtype,\n",
    "                       \"name\" : name}\n",
    "\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old network, but keep the current network name.\"\"\"\n",
    "    \n",
    "        sys.stdout.write('\\rLoading Netowrk Parameters')\n",
    "        a = load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        name = self.params['name']\n",
    "        self.params = d['data1arr_0'][()]\n",
    "        \n",
    "        # make sure to set the name for the current network \n",
    "        self.params['name'] = name\n",
    "        \n",
    "        \n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self.params)\n",
    "        \n",
    "    def _save_params(self):\n",
    "        param_direc = self.log_dir + self.params['name'] + '/params/'\n",
    "        \"\"\"Not safe - will overwrite existing file.\"\"\"\n",
    "        if not os.path.exists(param_direc):\n",
    "            os.makedirs(param_direc)\n",
    "        savez(param_direc + self.__class__.__name__, self.params)   \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def create_graph(self):\n",
    "        sys.stdout.write('\\rCreating Network Graph')\n",
    "        self._save_params()\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layer_names = ['layer_{}'.format(i) for i in range(len(self.params['layer_nodes']))]\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        with tf.variable_scope('keep_probs'):\n",
    "\n",
    "            self.sample_keep_prob = tf.placeholder(self.params['dtype'], name = 'sample_keep_prob')\n",
    "\n",
    "            self.fcl_keep_prob = tf.placeholder(self.params['dtype'], name = 'fcl_keep_prob')  \n",
    "            \n",
    "        with tf.variable_scope('sample'):\n",
    "\n",
    "            self.X = tf.placeholder(self.params['dtype'], shape = [None, 1024], name = 'X')\n",
    "            self.X = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('input_layer'):\n",
    "            \n",
    "            b = tf.get_variable(name = 'biases', shape = [self.params['layer_nodes'][0]],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            w = tf.get_variable(name = 'weights', shape  = [1024, self.params['layer_nodes'][0]],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.leaky_relu_alpha = tf.get_variable(shape = [],\n",
    "                                                    name = 'leaky_relu_alpha',\n",
    "                                                    dtype = self.dtype,\n",
    "                                                    constraint = lambda x: tf.clip_by_value(x, 0, 0.3),\n",
    "                                                    initializer = tf.zeros_initializer())\n",
    "            \n",
    "            layer = tf.nn.leaky_relu(tf.matmul(self.X, w) + b, alpha = self.leaky_relu_alpha)\n",
    "            layer = tf.nn.dropout(layer, self.fcl_keep_prob)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        for i in range(len(self.params['layer_nodes'])):\n",
    "            if i > 0:\n",
    "                with tf.variable_scope('layer_%d' %(i)):\n",
    "                    layer = tf.contrib.layers.fully_connected(self.layers[i-1], self.params['layer_nodes'][i])\n",
    "                    if self.params['layer_nodes'][i] >= 256:\n",
    "                        layer = tf.nn.dropout(layer, self.fcl_keep_prob)\n",
    "                    self.layers.append(layer)\n",
    "\n",
    "                    \n",
    "        with tf.variable_scope('prediction'):\n",
    "            self.predictions = tf.contrib.layers.fully_connected(self.layers[-1], 1)\n",
    "\n",
    "\n",
    "        with tf.variable_scope('targets'):\n",
    "            self.targets  = tf.placeholder(self.params['dtype'], shape = (None, 1), name = 'targets')\n",
    "\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('costs'):\n",
    "\n",
    "            error = tf.subtract(self.targets, self.predictions, name = 'error')\n",
    "            squared_error = tf.square(error, name = 'squared_difference')\n",
    "\n",
    "            with tf.variable_scope('mean_inverse_shifted_gaussian'):\n",
    "                with tf.variable_scope('normal_distribution'):\n",
    "                    \n",
    "                    sigma = tf.constant(self.params['threshold'], name = 'sigma')\n",
    "                    normal_dist = tf.contrib.distributions.Normal(0.0, sigma, name = 'normal_dist')\n",
    "                    gaussian_prob = normal_dist.prob(error, name = 'gaussian_prob')\n",
    "                    shift = tf.constant(self.params['g_shift'], name = 'shift')\n",
    "                    shifted_gaussian = tf.add(gaussian_prob, shift, name = 'shifted_gaussian')        \n",
    "\n",
    "                self.MISG = tf.reduce_mean(tf.divide(1.0, shifted_gaussian), name = 'mean_inverse_shifted_gaussian')\n",
    "\n",
    "            with tf.variable_scope('mean_squared_error'):\n",
    "                self.MSE = tf.reduce_mean(squared_error)\n",
    "        \n",
    "        with tf.variable_scope('train'):\n",
    "            cost = self.MSE if self.params['cost_name'] == 'MSE' else self.MISG\n",
    "            LR = tf.constant(self.params['learning_rate'], name = 'learning_rate')\n",
    "            self.optimizer = tf.train.AdamOptimizer(LR, epsilon=1e-08).minimize(cost)\n",
    "            \n",
    "        with tf.variable_scope('logging'):  \n",
    "\n",
    "            with tf.variable_scope('image'):\n",
    "                self.image_buf = tf.placeholder(tf.string, shape=[])\n",
    "                epoch_image = tf.expand_dims(tf.image.decode_png(self.image_buf, channels=4), 0)\n",
    "\n",
    "            with tf.variable_scope('percent_within_threshold'):\n",
    "                self.PWT = 100.*tf.reduce_mean(tf.cast(tf.less_equal(tf.abs(self.targets - self.predictions), sigma), self.params['dtype']) )\n",
    "\n",
    "            tf.summary.histogram(name = 'targets', values = self.targets)\n",
    "            tf.summary.histogram(name = 'predictions',values =  self.predictions)\n",
    "            tf.summary.scalar(name = 'leaky_relu_alpha', tensor = self.leaky_relu_alpha)\n",
    "            tf.summary.scalar(name = 'MSE', tensor = self.MSE)\n",
    "            tf.summary.scalar(name = 'MISG', tensor = self.MISG)\n",
    "            tf.summary.scalar(name = 'PWT', tensor = self.PWT)\n",
    "            tf.summary.image('prediction_vs_actual', epoch_image)\n",
    "            self.summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
