{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bn_FC_Network(object):\n",
    "    \"\"\"A neural network of fully connected layers.\n",
    "    \n",
    "    First layer has Leaky_ReLU activation with a trainable alpha. Other layers have ReLU activation.\n",
    "    Input sample has dropout at rate 1 - sample_keep_prob.\n",
    "    activations have dropout at rate 1 - fcl_keep_prob.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 layer_nodes,\n",
    "                 dtype = tf.float32,\n",
    "                 num_freq_channels = 1024,\n",
    "                 initial_learning_rate = 0.0001,\n",
    "                 cost_name = 'MSE',\n",
    "                 threshold = 0.00625,\n",
    "                 g_shift = 0.01,\n",
    "                 log_dir = 'logs/'):\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self.name = name\n",
    "        self.layer_nodes = layer_nodes\n",
    "        self.dtype = dtype\n",
    "        self.num_freq_channels = num_freq_channels\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.cost_name = cost_name\n",
    "        self.threshold = threshold\n",
    "        self.g_shift = g_shift\n",
    "\n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self._gen_params_dict())\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old network, but keep the current network name.\"\"\"\n",
    "    \n",
    "        sys.stdout.write('\\rLoading Netowrk Parameters')\n",
    "        a = load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        name = self.name\n",
    "        params = d['data1arr_0'][()]\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "        self.name = name\n",
    "\n",
    "    def _gen_params_dict(self):\n",
    "        d = self.__dict__\n",
    "        return {key : d[key] for key in d.keys() if key[0] != '_' if 'tensorflow' not in str(type(d[key]))}\n",
    "        \n",
    "    def _save_params(self):\n",
    "        direc = self.log_dir + self.name + '/params/'\n",
    "        \"\"\"Not safe - will overwrite existing file.\"\"\"\n",
    "        if not os.path.exists(direc):\n",
    "            os.makedirs(direc)\n",
    "            \n",
    "        np.savez(direc + self.__class__.__name__, self._gen_params_dict())   \n",
    "                \n",
    "        \n",
    "    def create_graph(self):\n",
    "        self._save_params()\n",
    "        sys.stdout.write('\\rCreating Network Graph')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self._layers = []\n",
    "        self._layer_names = ['layer_{}'.format(i) for i in range(len(self.layer_nodes))]\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        is_training = tf.placeholder(dtype = tf.bool, shape = [], name = 'is_training')\n",
    "        \n",
    "        with tf.variable_scope('keep_probs'):\n",
    "\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "\n",
    "            self.fcl_keep_prob = tf.placeholder(self.dtype, name = 'fcl_keep_prob')  \n",
    "            \n",
    "        with tf.variable_scope('sample'):\n",
    "\n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1024], name = 'X')\n",
    "            self.X = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('input_layer'):\n",
    "            \n",
    "            b = tf.get_variable(name = 'biases', shape = [self.layer_nodes[0]],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            w = tf.get_variable(name = 'weights', shape  = [1024, self.layer_nodes[0]],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.leaky_relu_alpha = tf.get_variable(shape = [],\n",
    "                                                    name = 'leaky_relu_alpha',\n",
    "                                                    dtype = self.dtype,\n",
    "                                                    constraint = lambda x: tf.clip_by_value(x, 0, 0.3),\n",
    "                                                    initializer = tf.zeros_initializer())\n",
    "            \n",
    "            layer = tf.nn.leaky_relu(tf.matmul(self.X, w) + b, alpha = self.leaky_relu_alpha)\n",
    "            layer = tf.contrib.layers.batch_norm(layer, is_training = is_training)\n",
    "            layer = tf.nn.dropout(layer, self.fcl_keep_prob)\n",
    "            self._layers.append(layer)\n",
    "            \n",
    "        for i in range(len(self.layer_nodes)):\n",
    "            if i > 0:\n",
    "                with tf.variable_scope('layer_%d' %(i)):\n",
    "                    layer = tf.contrib.layers.fully_connected(self._layers[i-1], self.layer_nodes[i])\n",
    "                    layer = tf.contrib.layers.batch_norm(layer, is_training = is_training)\n",
    "                    if self.layer_nodes[i] >= 256:\n",
    "                        layer = tf.nn.dropout(layer, self.fcl_keep_prob)\n",
    "                    self._layers.append(layer)\n",
    "\n",
    "                    \n",
    "        with tf.variable_scope('prediction'):\n",
    "            self.predictions = tf.contrib.layers.fully_connected(self._layers[-1], 1)\n",
    "\n",
    "\n",
    "        with tf.variable_scope('targets'):\n",
    "            self.targets  = tf.placeholder(self.dtype, shape = (None, 1), name = 'targets')\n",
    "\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('costs'):\n",
    "\n",
    "            error = tf.subtract(self.targets, self.predictions, name = 'error')\n",
    "            squared_error = tf.square(error, name = 'squared_difference')\n",
    "\n",
    "            with tf.variable_scope('mean_inverse_shifted_gaussian'):\n",
    "                with tf.variable_scope('normal_distribution'):\n",
    "                    \n",
    "                    sigma = tf.constant(self.threshold, name = 'sigma')\n",
    "                    normal_dist = tf.contrib.distributions.Normal(0.0, sigma, name = 'normal_dist')\n",
    "                    gaussian_prob = normal_dist.prob(error, name = 'gaussian_prob')\n",
    "                    shift = tf.constant(self.g_shift, name = 'shift')\n",
    "                    shifted_gaussian = tf.add(gaussian_prob, shift, name = 'shifted_gaussian')        \n",
    "\n",
    "                self.MISG = tf.reduce_mean(tf.divide(1.0, shifted_gaussian), name = 'mean_inverse_shifted_gaussian')\n",
    "\n",
    "            with tf.variable_scope('mean_squared_error'):\n",
    "                self.MSE = tf.reduce_mean(squared_error)\n",
    "       \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            with tf.variable_scope('train'):\n",
    "                cost = self.MSE if self.cost_name == 'MSE' else self.MISG\n",
    "                LR = tf.constant(self.initial_learning_rate, name = 'learning_rate')\n",
    "                self.optimizer = tf.train.AdamOptimizer(LR, epsilon=1e-08).minimize(cost)\n",
    "            \n",
    "        with tf.variable_scope('logging'):  \n",
    "\n",
    "            with tf.variable_scope('image'):\n",
    "                self.image_buf = tf.placeholder(tf.string, shape=[])\n",
    "                epoch_image = tf.expand_dims(tf.image.decode_png(self.image_buf, channels=4), 0)\n",
    "\n",
    "            with tf.variable_scope('percent_within_threshold'):\n",
    "                self.PWT = 100.*tf.reduce_mean(tf.cast(tf.less_equal(tf.abs(self.targets - self.predictions), sigma), self.dtype) )\n",
    "\n",
    "            tf.summary.histogram(name = 'targets', values = self.targets)\n",
    "            tf.summary.histogram(name = 'predictions',values =  self.predictions)\n",
    "            tf.summary.scalar(name = 'leaky_relu_alpha', tensor = self.leaky_relu_alpha)\n",
    "            tf.summary.scalar(name = 'MSE', tensor = self.MSE)\n",
    "            tf.summary.scalar(name = 'MISG', tensor = self.MISG)\n",
    "            tf.summary.scalar(name = 'PWT', tensor = self.PWT)\n",
    "            tf.summary.image('prediction_vs_actual', epoch_image)\n",
    "            self.summary = tf.summary.merge_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
