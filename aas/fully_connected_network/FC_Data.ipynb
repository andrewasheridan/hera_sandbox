{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyuvdata\n",
    "import hera_cal as hc\n",
    "import uvtools\n",
    "import random\n",
    "import threading\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_data(miriad_path, calfits_path):\n",
    "    \"\"\"TODO: docstring\"\"\"\n",
    "\n",
    "    uvd = pyuvdata.UVData()\n",
    "    uvd.read_miriad(miriad_path)\n",
    "\n",
    "    # get the redundancies for that data\n",
    "    aa = hc.utils.get_aa_from_uv(uvd)\n",
    "    info = hc.omni.aa_to_info(aa)\n",
    "    red_bls = np.array(info.get_reds())\n",
    "\n",
    "    # gains for same data \n",
    "    gains, flags = hc.io.load_cal(calfits_path)\n",
    "    \n",
    "    return red_bls, gains, uvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_red_bls(red_bls, gain_keys, min_group_len = 4):\n",
    "    \"\"\"Select all the good antennas from red_bls\n",
    "    \n",
    "    Each baseline group in red_bls has its bad separations removed. Groups with less than min_group_len are removed.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        red_bls : list of lists - Each sublist is a group of redundant separations for a unique baseline.\n",
    "        gain_keys : dict - gains.keys() from hc.io.load_cal()\n",
    "        min_group_len: int - Minimum number of separations in a 'good' sublist.\n",
    "                            (Default = 4, so that both training and testing can take two seps)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list of lists - Each sublist is a len >=4 list of separations of good antennas\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def ants_good(sep):\n",
    "        \"\"\"Returns True if both antennas are good.\n",
    "\n",
    "        Because we are using data from firstcal (is this right? I have trouble rememebring the names and properties of the different data sources)\n",
    "        we can check for known good or bad antennas by looking to see if the antenna is represented in gains.keys(). If the antenna is present, its good.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            sep : tuple - antenna indices\n",
    "\n",
    "        Returns :\n",
    "\n",
    "            bool - True if both antennas are in gain_keys\n",
    "        \"\"\"\n",
    "\n",
    "        ants = [a[0] for a in gain_keys]\n",
    "\n",
    "        if sep[0] in ants and sep[1] in ants:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    good_redundant_baselines = []\n",
    "    \n",
    "    for group in red_bls:\n",
    "        \n",
    "        new_group = []\n",
    "        \n",
    "        for sep in group:\n",
    "            \n",
    "            # only retain seps made from good antennas\n",
    "            if ants_good(sep) == True:\n",
    "                new_group.append(sep)\n",
    "                \n",
    "        new_group_len = len(new_group)\n",
    "        \n",
    "        # make sure groups are large enough that both the training set and the testing set can take two seps \n",
    "        if new_group_len >= min_group_len:\n",
    "            \n",
    "            # Make sure groups are made from even number of seps\n",
    "            #\n",
    "            # I honestly dont recall why I did this. ¯\\_(ツ)_/¯ \n",
    "            if new_group_len % 2 != 0:\n",
    "                new_group.pop()\n",
    "                \n",
    "            good_redundant_baselines.append(sorted(new_group))\n",
    "            \n",
    "    return good_redundant_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_red_bls(red_bls, training_percent = 0.80):\n",
    "    \"\"\"Slit a list of redundant baselines into a training set and a testing set. \n",
    "    \n",
    "    Each of the two sets has at least one pair of baselines from every group from red_bls. However, separations from one set will not appear the other.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        red_bls : list of lists - Each sublist is a group of redundant separations for a unique baseline. Each sublist must have at least 4 separations.\n",
    "        training_percent : float - *Approximate* portion of the separations that will appear in the training set.\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        tuple of dicts - train_red_bls_dict, test_red_bls_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    training_redundant_baselines_dict = {}\n",
    "    testing_redundant_baselines_dict = {}\n",
    "\n",
    "    # make sure that each set has at least 2 seps from each group\n",
    "    #\n",
    "    thinned_groups_dict = {}\n",
    "    for group in red_bls:\n",
    "\n",
    "        # group key is the sep with the lowest antenna indicies\n",
    "        key = sorted(group)[0]\n",
    "        \n",
    "        # pop off seps from group and append them to train or test groups\n",
    "        random.shuffle(group)\n",
    "        training_group = []\n",
    "        training_group.append(group.pop())\n",
    "        training_group.append(group.pop())\n",
    "\n",
    "        testing_group = []\n",
    "        testing_group.append(group.pop())\n",
    "        testing_group.append(group.pop())\n",
    "        \n",
    "        # add the new train & test groups into the dicts\n",
    "        training_redundant_baselines_dict[key] = training_group\n",
    "        testing_redundant_baselines_dict[key] = testing_group\n",
    "        \n",
    "        # if there are still more seps in the group, save them into a dict for later assignment\n",
    "        if len(group) != 0:\n",
    "            thinned_groups_dict[key] = group\n",
    "\n",
    "    # Shuffle and split the group keys into two sets using \n",
    "    #\n",
    "    thinned_dict_keys = thinned_groups_dict.keys()\n",
    "    random.shuffle(thinned_dict_keys)\n",
    "    \n",
    "    \"\"\"Because we are ensuring that each set has some seps from every group, the ratio of train / test gets reduced a few percent.\n",
    "    This accounts for that with an arbitrary shift found by trial and error.\"\"\"\n",
    "    training_percent = np.min([0.95, training_percent + 0.15])\n",
    "\n",
    "    \n",
    "    # why did i call this extra?\n",
    "    # these are the keys that each set will extract seps from thinned_groups_dict with\n",
    "    training_redundant_baselines_extra, testing_redundant_baselines_extra = np.split(thinned_dict_keys, [int(len(thinned_dict_keys)*training_percent)])\n",
    "\n",
    "    # extract seps from thinned_groups_dict and apply to same key in training set\n",
    "    for key in training_redundant_baselines_extra:\n",
    "        key = tuple(key)\n",
    "        group = thinned_groups_dict[key]\n",
    "        training_group = training_redundant_baselines_dict[key]\n",
    "        training_group.extend(group)\n",
    "        training_redundant_baselines_dict[key] = training_group\n",
    "\n",
    "    # extract seps from thinned_groups_dict and apply to same key in testing set\n",
    "    for key in testing_redundant_baselines_extra:\n",
    "        key = tuple(key)\n",
    "        group = thinned_groups_dict[key]\n",
    "        testing_group = testing_redundant_baselines_dict[key]\n",
    "        testing_group.extend(group)\n",
    "        testing_redundant_baselines_dict[key] = testing_group\n",
    "        \n",
    "    return training_redundant_baselines_dict, testing_redundant_baselines_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_gen_test_train_red_bls_dicts(red_bls = None, gain_keys = None, training_percent = 0.80):\n",
    "\n",
    "    if len(glob.glob(\"*.npz\")) == 2:\n",
    "        \n",
    "        training_redundant_baselines_dict = loadnpz('training_redundant_baselines_dict.npz')[()]\n",
    "        testing_redundant_baselines_dict = loadnpz('testing_redundant_baselines_dict.npz')[()]\n",
    "    else:\n",
    "        \n",
    "        assert type(red_bls) != None, \"Provide a list of redundant baselines\"\n",
    "        assert type(gain_keys) != None, \"Provide a list of gain keys\"\n",
    "        \n",
    "        good_redundant_baselines = get_good_red_bls(red_bls, gain_keys)\n",
    "        training_redundant_baselines_dict, testing_redundant_baselines_dict = train_test_split_red_bls(good_redundant_baselines, training_percent = training_percent)\n",
    "\n",
    "        np.savez('training_redundant_baselines_dict', training_redundant_baselines_dict)\n",
    "        np.savez('testing_redundant_baselines_dict', testing_redundant_baselines_dict)\n",
    "\n",
    "    return training_redundant_baselines_dict, training_redundant_baselines_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "def loadnpz(filename):\n",
    "    \"\"\"Loads up npzs. For dicts do loadnpz(fn)[()]\"\"\"\n",
    "    a = load(filename)\n",
    "    d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "    return d['data1arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seps_data(red_bls_dict, uvd):\n",
    "    \"\"\"Get the data and data.conjugate() for all the seps in a redundant baselines dictionary.\"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    for key in red_bls_dict.keys():\n",
    "        for sep in red_bls_dict[key]:\n",
    "            data[sep] = uvd.get_data(sep)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Creator(object):\n",
    "    \"\"\"Creates data in an alternate thread.\n",
    "\n",
    "    ## usage:\n",
    "    ## data_maker = data_creator(num_flatnesses=250, mode = 'train')\n",
    "    ## data_maker.gen_data() (before loop)\n",
    "    ## inputs, targets = data_maker.get_data() (start of loop)\n",
    "    ## data_maker.gen_data() (immediately after get_data())\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"TODO:\n",
    "    \n",
    "    Comments, arg / return explanation & genreal cleanup\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_flatnesses, bl_data = None, bl_dict = None, gains = None, clean = False):\n",
    "        \n",
    "        \n",
    "        self._num = num_flatnesses\n",
    "        self._clean = clean\n",
    "                    \n",
    "        self._bl_data = bl_data\n",
    "        self._bl_data_c = None\n",
    "        \n",
    "        self._bl_dict = bl_dict\n",
    "        \n",
    "        self._gains = gains\n",
    "        self._gains_c = None\n",
    "        \n",
    "        self._epoch_batch = []\n",
    "        self._nu = np.arange(1024)\n",
    "        \n",
    "    def _gen_data(self):\n",
    "        \n",
    "        # scaling tools\n",
    "        # the NN likes data in the range (0,1)\n",
    "        angle_tx  = lambda x: (np.asarray(x) + np.pi) / (2. * np.pi)\n",
    "        angle_itx = lambda x: np.asarray(x) * 2. * np.pi - np.pi\n",
    "\n",
    "        abs_min_max_delay = 0.040\n",
    "        delay_tx  = lambda x: (np.array(x) + abs_min_max_delay) / (2. * abs_min_max_delay)\n",
    "        delay_itx = lambda x: np.array(x) * 2. * abs_min_max_delay - abs_min_max_delay\n",
    "\n",
    "        if self._clean == True:\n",
    "            \n",
    "            targets = np.random.uniform(low = -.040, high = 0.040, size = (self._num * 60, 1))\n",
    "            inputs = np.angle(np.exp(-2j * np.pi * (targets * self._nu + np.random.uniform())))\n",
    "            \n",
    "            self._epoch_batch.append((angle_tx(inputs), delay_tx(targets)))\n",
    "            \n",
    "        else:\n",
    "            assert type(self._bl_data) != None, \"Provide visibility data\"\n",
    "            assert type(self._bl_dict) != None, \"Provide dict of baselines\"\n",
    "            assert type(self._gains)   != None, \"Provide antenna gains\"\n",
    "            \n",
    "            if self._bl_data_c == None:\n",
    "                self._bl_data_c = {key : self._bl_data[key].conjugate() for key in self._bl_data.keys()}\n",
    "                \n",
    "            if self._gains_c == None:\n",
    "                self._gains_c = {key : self._gains[key].conjugate() for key in self._gains.keys()}\n",
    "                \n",
    "\n",
    "            def _flatness(seps):\n",
    "                \"\"\"Create a flatness from a given pair of seperations, their data & their gains.\"\"\"\n",
    "\n",
    "                a, b = seps[0][0], seps[0][1]\n",
    "                c, d = seps[1][0], seps[1][1]\n",
    "\n",
    "                return self._bl_data[seps[0]]*self._bl_data_c[seps[1]] * self._gains_c[(a,'x')] * self._gains[(b,'x')] * self._gains[(c,'x')] * self._gains_c[(d,'x')]\n",
    "\n",
    "            inputs = []\n",
    "            for _ in range(self._num):\n",
    "\n",
    "                unique_baseline = random.sample(self._bl_dict.keys(), 1)[0]\n",
    "                two_seps = [random.sample(self._bl_dict[unique_baseline], 2)][0]\n",
    "                \n",
    "                inputs.append(_flatness(two_seps))\n",
    "\n",
    "            targets = np.random.uniform(low = -.040, high = 0.040, size = (self._num * 60, 1))\n",
    "            inputs = np.angle(np.array(inputs).reshape(-1,1024) * np.exp(-2j * np.pi * (targets * self._nu + np.random.uniform())))\n",
    "\n",
    "            self._epoch_batch.append((angle_tx(inputs), delay_tx(targets)))\n",
    "\n",
    "    def gen_data(self):\n",
    "        \n",
    "        self._thread = threading.Thread(target = self._gen_data, args=())\n",
    "        self._thread.start()\n",
    "\n",
    "    def get_data(self, timeout = 10):\n",
    "        \n",
    "        if len(self._epoch_batch) == 0:\n",
    "            self._thread.join(timeout)\n",
    "            \n",
    "        return self._epoch_batch.pop(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
