{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FCN.ipynb`\n",
    "\n",
    "Create a fully connected neural network that determines the slope angle of phase data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### GPU Check\n",
    "# # check available GPU in terminal: watch -d -n 0.5 nvidia-smi\n",
    "# # select one GPU to train on:\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import io, os, sys, glob\n",
    "\n",
    "import pyuvdata\n",
    "import hera_cal as hc\n",
    "import uvtools\n",
    "\n",
    "import threading\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_data(miriad_path, calfits_path):\n",
    "    \"\"\"TODO: docstring\"\"\"\n",
    "\n",
    "    uvd = pyuvdata.UVData()\n",
    "    uvd.read_miriad(miriad_path)\n",
    "\n",
    "    # get the redundancies for that data\n",
    "    aa = hc.utils.get_aa_from_uv(uvd)\n",
    "    info = hc.omni.aa_to_info(aa)\n",
    "    red_bls = np.array(info.get_reds())\n",
    "\n",
    "    # gains for same data \n",
    "    gains, flags = hc.io.load_cal(calfits_path)\n",
    "    \n",
    "    return red_bls, gains, uvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_red_bls(red_bls, gain_keys, min_group_len = 4):\n",
    "    \"\"\"Select all the good antennas from red_bls\n",
    "    \n",
    "    Each baseline group in red_bls has its bad separations removed. Groups with less than min_group_len are removed.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        red_bls : list of lists - Each sublist is a group of redundant separations for a unique baseline.\n",
    "        gain_keys : dict - gains.keys() from hc.io.load_cal()\n",
    "        min_group_len: int - Minimum number of separations in a 'good' sublist.\n",
    "                            (Default = 4, so that both training and testing can take two seps)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list of lists - Each sublist is a len >=4 list of separations of good antennas\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def ants_good(sep):\n",
    "        \"\"\"Returns True if both antennas are good.\n",
    "\n",
    "        Because we are using data from firstcal (is this right? I have trouble rememebring the names and properties of the different data sources)\n",
    "        we can check for known good or bad antennas by looking to see if the antenna is represented in gains.keys(). If the antenna is present, its good.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            sep : tuple - antenna indices\n",
    "\n",
    "        Returns :\n",
    "\n",
    "            bool - True if both antennas are in gain_keys\n",
    "        \"\"\"\n",
    "\n",
    "        ants = [a[0] for a in gain_keys]\n",
    "\n",
    "        if sep[0] in ants and sep[1] in ants:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    good_redundant_baselines = []\n",
    "    \n",
    "    for group in red_bls:\n",
    "        \n",
    "        new_group = []\n",
    "        \n",
    "        for sep in group:\n",
    "            \n",
    "            # only retain seps made from good antennas\n",
    "            if ants_good(sep) == True:\n",
    "                new_group.append(sep)\n",
    "                \n",
    "        new_group_len = len(new_group)\n",
    "        \n",
    "        # make sure groups are large enough that both the training set and the testing set can take two seps \n",
    "        if new_group_len >= min_group_len:\n",
    "            \n",
    "            # Make sure groups are made from even number of seps\n",
    "            #\n",
    "            # I honestly dont recall why I did this. ¯\\_(ツ)_/¯ \n",
    "            if new_group_len % 2 != 0:\n",
    "                new_group.pop()\n",
    "                \n",
    "            good_redundant_baselines.append(sorted(new_group))\n",
    "            \n",
    "    return good_redundant_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_red_bls(red_bls, training_percent = 0.80):\n",
    "    \"\"\"Slit a list of redundant baselines into a training set and a testing set. \n",
    "    \n",
    "    Each of the two sets has at least one pair of baselines from every group from red_bls. However, separations from one set will not appear the other.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        red_bls : list of lists - Each sublist is a group of redundant separations for a unique baseline. Each sublist must have at least 4 separations.\n",
    "        training_percent : float - *Approximate* portion of the separations that will appear in the training set.\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        tuple of dicts - train_red_bls_dict, test_red_bls_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    training_redundant_baselines_dict = {}\n",
    "    testing_redundant_baselines_dict = {}\n",
    "\n",
    "    # make sure that each set has at least 2 seps from each group\n",
    "    #\n",
    "    thinned_groups_dict = {}\n",
    "    for group in red_bls:\n",
    "\n",
    "        # group key is the sep with the lowest antenna indicies\n",
    "        key = sorted(group)[0]\n",
    "        \n",
    "        # pop off seps from group and append them to train or test groups\n",
    "        random.shuffle(group)\n",
    "        training_group = []\n",
    "        training_group.append(group.pop())\n",
    "        training_group.append(group.pop())\n",
    "\n",
    "        testing_group = []\n",
    "        testing_group.append(group.pop())\n",
    "        testing_group.append(group.pop())\n",
    "        \n",
    "        # add the new train & test groups into the dicts\n",
    "        training_redundant_baselines_dict[key] = training_group\n",
    "        testing_redundant_baselines_dict[key] = testing_group\n",
    "        \n",
    "        # if there are still more seps in the group, save them into a dict for later assignment\n",
    "        if len(group) != 0:\n",
    "            thinned_groups_dict[key] = group\n",
    "\n",
    "    # Shuffle and split the group keys into two sets using \n",
    "    #\n",
    "    thinned_dict_keys = thinned_groups_dict.keys()\n",
    "    random.shuffle(thinned_dict_keys)\n",
    "    \n",
    "    \"\"\"Because we are ensuring that each set has some seps from every group, the ratio of train / test gets reduced a few percent.\n",
    "    This accounts for that with an arbitrary shift found by trial and error.\"\"\"\n",
    "    training_percent = np.min([0.95, training_percent + 0.15])\n",
    "\n",
    "    \n",
    "    # why did i call this extra?\n",
    "    # these are the keys that each set will extract seps from thinned_groups_dict with\n",
    "    training_redundant_baselines_extra, testing_redundant_baselines_extra = np.split(thinned_dict_keys, [int(len(thinned_dict_keys)*training_percent)])\n",
    "\n",
    "    # extract seps from thinned_groups_dict and apply to same key in training set\n",
    "    for key in training_redundant_baselines_extra:\n",
    "        key = tuple(key)\n",
    "        group = thinned_groups_dict[key]\n",
    "        training_group = training_redundant_baselines_dict[key]\n",
    "        training_group.extend(group)\n",
    "        training_redundant_baselines_dict[key] = training_group\n",
    "\n",
    "    # extract seps from thinned_groups_dict and apply to same key in testing set\n",
    "    for key in testing_redundant_baselines_extra:\n",
    "        key = tuple(key)\n",
    "        group = thinned_groups_dict[key]\n",
    "        testing_group = testing_redundant_baselines_dict[key]\n",
    "        testing_group.extend(group)\n",
    "        testing_redundant_baselines_dict[key] = testing_group\n",
    "        \n",
    "    return training_redundant_baselines_dict, testing_redundant_baselines_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_gen_test_train_red_bls_dicts(red_bls = None, gain_keys = None, training_percent = 0.80):\n",
    "\n",
    "    if len(glob.glob(\"*.npz\")) == 2:\n",
    "        \n",
    "        training_redundant_baselines_dict = loadnpz('training_redundant_baselines_dict.npz')[()]\n",
    "        testing_redundant_baselines_dict = loadnpz('testing_redundant_baselines_dict.npz')[()]\n",
    "    else:\n",
    "        \n",
    "        assert type(red_bls) != None, \"Provide a list of redundant baselines\"\n",
    "        assert type(gain_keys) != None, \"Provide a list of gain keys\"\n",
    "        \n",
    "        good_redundant_baselines = get_good_red_bls(red_bls, gain_keys)\n",
    "        training_redundant_baselines_dict, testing_redundant_baselines_dict = train_test_split_red_bls(good_redundant_baselines, training_percent = training_percent)\n",
    "\n",
    "        np.savez('training_redundant_baselines_dict', training_redundant_baselines_dict)\n",
    "        np.savez('testing_redundant_baselines_dict', testing_redundant_baselines_dict)\n",
    "\n",
    "    return training_redundant_baselines_dict, training_redundant_baselines_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "def loadnpz(filename):\n",
    "    \"\"\"Loads up npzs. For dicts do loadnpz(fn)[()]\"\"\"\n",
    "    a = load(filename)\n",
    "    d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "    return d['data1arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seps_data(red_bls_dict, uvd):\n",
    "    \"\"\"Get the data and data.conjugate() for all the seps in a redundant baselines dictionary.\"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    for key in red_bls_dict.keys():\n",
    "        for sep in red_bls_dict[key]:\n",
    "            data[sep] = uvd.get_data(sep)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Creator(object):\n",
    "    \"\"\"Creates data in an alternate thread.\n",
    "\n",
    "    ## usage:\n",
    "    ## data_maker = data_creator(num_flatnesses=250, mode = 'train')\n",
    "    ## data_maker.gen_data() (before loop)\n",
    "    ## inputs, targets = data_maker.get_data() (start of loop)\n",
    "    ## data_maker.gen_data() (immediately after get_data())\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"TODO:\n",
    "    \n",
    "    Comments, arg / return explanation & genreal cleanup\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_flatnesses, bl_data = None, bl_dict = None, gains = None, clean = False):\n",
    "        \n",
    "        \n",
    "        self._num = num_flatnesses\n",
    "        self._clean = clean\n",
    "                    \n",
    "        self._bl_data = bl_data\n",
    "        self._bl_data_c = None\n",
    "        \n",
    "        self._bl_dict = bl_dict\n",
    "        \n",
    "        self._gains = gains\n",
    "        self._gains_c = None\n",
    "        \n",
    "        self._epoch_batch = []\n",
    "        self._nu = np.arange(1024)\n",
    "        \n",
    "    def _gen_data(self):\n",
    "        \n",
    "        # scaling tools\n",
    "        # the NN likes data in the range (0,1)\n",
    "        angle_tx  = lambda x: (np.asarray(x) + np.pi) / (2. * np.pi)\n",
    "        angle_itx = lambda x: np.asarray(x) * 2. * np.pi - np.pi\n",
    "\n",
    "        abs_min_max_delay = 0.040\n",
    "        delay_tx  = lambda x: (np.array(x) + abs_min_max_delay) / (2. * abs_min_max_delay)\n",
    "        delay_itx = lambda x: np.array(x) * 2. * abs_min_max_delay - abs_min_max_delay\n",
    "\n",
    "        if self._clean == True:\n",
    "            \n",
    "            targets = np.random.uniform(low = -.040, high = 0.040, size = (self._num * 60, 1))\n",
    "            inputs = np.angle(np.exp(-2j * np.pi * (targets * self._nu + np.random.uniform())))\n",
    "            \n",
    "            self._epoch_batch.append((angle_tx(inputs), delay_tx(targets)))\n",
    "            \n",
    "        else:\n",
    "            assert type(self._bl_data) != None, \"Provide visibility data\"\n",
    "            assert type(self._bl_dict) != None, \"Provide dict of baselines\"\n",
    "            assert type(self._gains)   != None, \"Provide antenna gains\"\n",
    "            \n",
    "            if self._bl_data_c == None:\n",
    "                self._bl_data_c = {key : self._bl_data[key].conjugate() for key in self._bl_data.keys()}\n",
    "                \n",
    "            if self._gains_c == None:\n",
    "                self._gains_c = {key : self._gains[key].conjugate() for key in self._gains.keys()}\n",
    "                \n",
    "\n",
    "            def _flatness(seps):\n",
    "                \"\"\"Create a flatness from a given pair of seperations, their data & their gains.\"\"\"\n",
    "\n",
    "                a, b = seps[0][0], seps[0][1]\n",
    "                c, d = seps[1][0], seps[1][1]\n",
    "\n",
    "                return self._bl_data[seps[0]]*self._bl_data_c[seps[1]] * self._gains_c[(a,'x')] * self._gains[(b,'x')] * self._gains[(c,'x')] * self._gains_c[(d,'x')]\n",
    "\n",
    "            inputs = []\n",
    "            for _ in range(self._num):\n",
    "\n",
    "                unique_baseline = random.sample(self._bl_dict.keys(), 1)[0]\n",
    "                two_seps = [random.sample(self._bl_dict[unique_baseline], 2)][0]\n",
    "                \n",
    "                inputs.append(_flatness(two_seps))\n",
    "\n",
    "            targets = np.random.uniform(low = -.040, high = 0.040, size = (self._num * 60, 1))\n",
    "            inputs = np.angle(np.array(inputs).reshape(-1,1024) * np.exp(-2j * np.pi * (targets * self._nu + np.random.uniform())))\n",
    "\n",
    "            self._epoch_batch.append((angle_tx(inputs), delay_tx(targets)))\n",
    "\n",
    "    def gen_data(self):\n",
    "        \n",
    "        self._thread = threading.Thread(target = self._gen_data, args=())\n",
    "        self._thread.start()\n",
    "\n",
    "    def get_data(self, timeout = 10):\n",
    "        \n",
    "        if len(self._epoch_batch) == 0:\n",
    "            self._thread.join(timeout)\n",
    "            \n",
    "        return self._epoch_batch.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,\n",
    "          num_flatnesses,\n",
    "          num_epochs,\n",
    "          batch_size,\n",
    "          log_dir,\n",
    "          model_save_interval,\n",
    "          train_info,\n",
    "          test_info,\n",
    "          gains,\n",
    "          clean = False,\n",
    "          pretrained_model_path = None,\n",
    "          sample_keep_prob = 0.80,\n",
    "          fcl_keep_prob = 0.50):\n",
    "    \n",
    "    \"\"\"TODO: Docstring, comments, cleanup..\"\"\"\n",
    "    \n",
    "    \n",
    "    def gen_plot(predicted_values, actual_values):\n",
    "        \"\"\"Create a prediction plot and save to byte string.\"\"\"\n",
    "        \n",
    "\n",
    "        abs_min_max_delay = 0.040\n",
    "        delay_tx  = lambda x: (np.array(x) + abs_min_max_delay) / (2. * abs_min_max_delay)\n",
    "        delay_itx = lambda x: np.array(x) * 2. * abs_min_max_delay - abs_min_max_delay\n",
    "\n",
    "        prediction_unscaled = delay_itx(predicted_values)\n",
    "        actual_unscaled = delay_itx(actual_values)\n",
    "\n",
    "        sorting_idx = np.argsort(actual_unscaled.T[0])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = (5, 3), dpi = 144)\n",
    "\n",
    "        ax.plot(prediction_unscaled.T[0][sorting_idx],\n",
    "                linestyle = 'none', marker = '.', markersize = 1,\n",
    "                color = 'darkblue')\n",
    "\n",
    "        ax.plot(actual_unscaled.T[0][sorting_idx],\n",
    "                linestyle = 'none', marker = '.', markersize = 1, alpha = 0.50,\n",
    "                color = '#E50000')       \n",
    "\n",
    "        ax.set_title('std: %.9f' %np.std(prediction_unscaled.T[0][sorting_idx] - actual_unscaled.T[0][sorting_idx]))\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi = 144)\n",
    "        plt.close(fig)\n",
    "        buf.seek(0)\n",
    "\n",
    "        return buf.getvalue()\n",
    "\n",
    "    num = num_flatnesses\n",
    "    num_entries = num * 60\n",
    "\n",
    "    MISG = []\n",
    "    MSE = []\n",
    "    \n",
    "    \n",
    "    train_data, train_dict = train_info\n",
    "    train_batcher = Data_Creator(num, bl_data = train_data, bl_dict = train_dict, gains = gains, clean = clean)\n",
    "    train_batcher.gen_data()\n",
    "\n",
    "    test_data, test_dict = test_info\n",
    "    test_batcher = Data_Creator(num, bl_data = test_data, bl_dict = test_dict, gains = gains, clean = clean)\n",
    "\n",
    "    test_batcher.gen_data()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        if pretrained_model_path == None:\n",
    "            session.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            saver.restore(session, pretrained_model_path)\n",
    "\n",
    "        training_writer = tf.summary.FileWriter(log_dir + '/training', session.graph)\n",
    "        testing_writer = tf.summary.FileWriter(log_dir + '/testing', session.graph)\n",
    "        model_save_location = log_dir + '/trained_model.ckpt'   \n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            training_inputs, training_targets = train_batcher.get_data(); train_batcher.gen_data()\n",
    "            testing_inputs, testing_targets = test_batcher.get_data(); test_batcher.gen_data()  \n",
    "\n",
    "            for j in range(int(num_entries/batch_size)):\n",
    "\n",
    "                training_inputs_batch = training_inputs[j*batch_size:(j + 1)*batch_size].reshape(-1,1024)\n",
    "                training_targets_batch = training_targets[j*batch_size:(j + 1)*batch_size].reshape(-1,1)\n",
    "                \n",
    "\n",
    "                session.run([network.optimizer], feed_dict = {network.X: training_inputs_batch,\n",
    "                                                              network.targets: training_targets_batch,\n",
    "                                                              network.sample_keep_prob : sample_keep_prob,\n",
    "                                                              network.fcl_keep_prob : fcl_keep_prob}) \n",
    "            # Prediction: Scaled Train(ing results)   \n",
    "            PST = session.run(network.predictions,\n",
    "                              feed_dict = {network.X: training_inputs.reshape(-1,1024),\n",
    "                                           network.sample_keep_prob : 1.,\n",
    "                                           network.fcl_keep_prob : 1.}) \n",
    "\n",
    "\n",
    "            training_MISG, training_MSE, training_summary = session.run([network.MISG, network.MSE, network.summary],\n",
    "                                                                        feed_dict = {network.X: training_inputs.reshape(-1,1024),\n",
    "                                                                        network.targets: training_targets.reshape(-1,1),\n",
    "                                                                        network.sample_keep_prob : 1.,\n",
    "                                                                        network.fcl_keep_prob : 1.,\n",
    "                                                                        network.image_buf: gen_plot(PST,training_targets)}) \n",
    "\n",
    "            sys.stdout.write('\\r' + \"Epoch: \" + str(epoch) + \". Training: MISG = {:.6f}, MSE = {:.6f}\".format(training_MISG, training_MSE))\n",
    "\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            training_writer.flush()  \n",
    "\n",
    "            # Prediction: Scaled test(ing results)   \n",
    "            PSt = session.run(network.predictions,\n",
    "                              feed_dict = {network.X: testing_inputs.reshape(-1,1024),\n",
    "                                           network.sample_keep_prob : 1.,\n",
    "                                           network.fcl_keep_prob : 1.}) \n",
    "\n",
    "            testing_MISG, testing_MSE, testing_summary = session.run([network.MISG, network.MSE, network.summary],\n",
    "                                                                      feed_dict = {network.X: testing_inputs.reshape(-1,1024),\n",
    "                                                                                   network.targets: testing_targets.reshape(-1,1),\n",
    "                                                                                   network.sample_keep_prob : 1.,\n",
    "                                                                                   network.fcl_keep_prob : 1.,\n",
    "                                                                                   network.image_buf: gen_plot(PSt,testing_targets)}) \n",
    "\n",
    "            sys.stdout.write('\\r' + \"Epoch: \" + str(epoch) + \". Testing: MISG = {:.6f}, MSE = {:.6f}\".format(testing_MISG, testing_MSE))\n",
    "\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            testing_writer.flush()  \n",
    "\n",
    "            MISG.append((training_MISG, testing_MISG))\n",
    "            MSE.append((training_MSE, testing_MSE))\n",
    "\n",
    "            if (epoch + 1) % model_save_interval == 0:\n",
    "                saver.save(session, model_save_location, epoch)\n",
    "\n",
    "\n",
    "\n",
    "        print('\\rDone')\n",
    "\n",
    "        training_writer.close()\n",
    "        testing_writer.close()\n",
    "\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(object):\n",
    "    \"\"\"A neural network of fully connected layers.\n",
    "    \n",
    "    First layer has Leaky_ReLU activation with a trainable alpha. Other layers have ReLU activation.\n",
    "    Input sample has dropout at rate 1 - sample_keep_prob.\n",
    "    activations have dropout at rate 1 - fcl_keep_prob.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 layer_nodes,\n",
    "                 learning_rate = 0.0001,\n",
    "                 dtype = tf.float32):\n",
    "        \n",
    "        self.layer_nodes = layer_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.layers = []\n",
    "        self.layer_names = ['layer_{}'.format(i) for i in range(len(self.layer_nodes))]\n",
    "        \n",
    "    def create_graph(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        with tf.variable_scope('keep_probs'):\n",
    "\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "\n",
    "            self.fcl_keep_prob = tf.placeholder(self.dtype, name = 'fcl_keep_prob')  \n",
    "            \n",
    "        with tf.variable_scope('sample'):\n",
    "\n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1024], name = 'X')\n",
    "            self.X = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('input_layer'):\n",
    "            \n",
    "            b = tf.get_variable(name = 'bias', shape = [layer_nodes[0]],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            w = tf.get_variable(name = 'weights', shape  = [1024, layer_nodes[0]],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.leaky_relu_alpha = tf.get_variable(name = 'leaky_relu_alpha',\n",
    "                                                    shape = [1],\n",
    "                                                    dtype = self.dtype)\n",
    "            \n",
    "            layer = tf.nn.leaky_relu(tf.matmul(self.X, w) + b, alpha = self.leaky_relu_alpha)\n",
    "            layer = tf.nn.dropout(layer, self.fcl_keep_prob)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        for i in range(len(layer_nodes)):\n",
    "            if i > 0:\n",
    "                with tf.variable_scope('layer_%d' %(i)):\n",
    "                    layer = tf.contrib.layers.fully_connected(self.layers[i-1], layer_nodes[i])\n",
    "                    layer = tf.nn.dropout(layer, self.fcl_keep_prob)\n",
    "                    self.layers.append(layer)\n",
    "\n",
    "                    \n",
    "        with tf.variable_scope('prediction'):\n",
    "            self.predictions = tf.contrib.layers.fully_connected(self.layers[-1], 1)\n",
    "\n",
    "\n",
    "        with tf.variable_scope('targets'):\n",
    "            self.targets  = tf.placeholder(self.dtype, shape = (None, 1), name = 'targets')\n",
    "\n",
    "            \n",
    "            \n",
    "        with tf.variable_scope('costs'):\n",
    "\n",
    "            error = tf.subtract(self.targets, self.predictions, name = 'error')\n",
    "            squared_error = tf.square(error, name = 'squared_difference')\n",
    "\n",
    "            with tf.variable_scope('mean_inverse_shifted_gaussian'):\n",
    "                with tf.variable_scope('normal_distribution'):\n",
    "                    sigma = tf.constant(0.00625, name = 'sigma')\n",
    "                    normal_dist = tf.contrib.distributions.Normal(0.0, sigma, name = 'normal_dist')\n",
    "                    gaussian_prob = normal_dist.prob(error, name = 'gaussian_prob')\n",
    "                    shift = tf.constant(0.01, name = 'shift')\n",
    "                    shifted_gaussian = tf.add(gaussian_prob, shift, name = 'shifted_gaussian')        \n",
    "\n",
    "                self.MISG = tf.reduce_mean(tf.divide(1.0, shifted_gaussian), name = 'mean_inverse_shifted_gaussian')\n",
    "\n",
    "            with tf.variable_scope('mean_squared_error'):\n",
    "                self.MSE = tf.reduce_mean(squared_error)\n",
    "        \n",
    "        with tf.variable_scope('train'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate, epsilon=1e-08).minimize(self.MISG)\n",
    "            \n",
    "        with tf.variable_scope('logging'):  \n",
    "\n",
    "            with tf.variable_scope('image'):\n",
    "                self.image_buf = tf.placeholder(tf.string, shape=[])\n",
    "                epoch_image = tf.expand_dims(tf.image.decode_png(self.image_buf, channels=4), 0)\n",
    "\n",
    "            with tf.variable_scope('percent_within_threshold'):\n",
    "                self.PWT = tf.reduce_mean(tf.cast(tf.less_equal(tf.abs(self.targets - self.predictions), sigma), self.dtype) )\n",
    "\n",
    "            tf.summary.histogram(name = 'targets', values = self.targets)\n",
    "            tf.summary.histogram(name = 'predictions',values =  self.predictions)\n",
    "            tf.summary.scalar(name = 'MSE', tensor = self.MSE)\n",
    "            tf.summary.scalar(name = 'MISG', tensor = self.MISG)\n",
    "            tf.summary.scalar(name = 'PWT', tensor = self.PWT)\n",
    "            tf.summary.image('prediction_vs_actual', epoch_image)\n",
    "            self.summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the redundant baselines and their gains and data from miriad and calfits files\n",
    "red_bls, gains, uvd = load_relevant_data('../zen_data/zen.2458098.58037.xx.HH.uv','../zen_data/zen.2458098.58037.xx.HH.uv.abs.calfits')\n",
    "\n",
    "# seperate trining and testing redundant baselines \n",
    "# if we have not already done this, load them from disk\n",
    "training_redundant_baselines_dict, testing_redundant_baselines_dict = get_or_gen_test_train_red_bls_dicts(red_bls, gains.keys())\n",
    "\n",
    "# seperate the visiblites\n",
    "training_baselines_data = get_seps_data(training_redundant_baselines_dict, uvd)\n",
    "testing_baselines_data = get_seps_data(testing_redundant_baselines_dict, uvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create network\n",
    "layer_nodes = [1024,512,256,128,64,32,16]\n",
    "network = FCN(layer_nodes, learning_rate = 0.00001)\n",
    "network.create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17. Testing: MISG = 92.477112, MSE = 0.127349"
     ]
    }
   ],
   "source": [
    "train(network,\n",
    "      5,\n",
    "      50,\n",
    "      2,\n",
    "      'logs/test',\n",
    "      5,\n",
    "      (training_baselines_data, training_redundant_baselines_dict),\n",
    "      (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "      gains,\n",
    "      clean = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
