{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flat_CNN.ipynb`\n",
    "\n",
    "Create a convolutional neural network that uses filters of height = 1 to determine the slope angle of phase data. Each sample input will be an array of angle data, shape = (1x1024) (One time x 1024 frequency channels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import io, os, sys\n",
    "\n",
    "import pyuvdata\n",
    "import hera_cal as hc\n",
    "import uvtools\n",
    "\n",
    "import threading\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data for a JD. \n",
    "data_directory = '../zen_data/'\n",
    "miriad_path = data_directory + 'zen.2458098.58037.xx.HH.uv'\n",
    "\n",
    "uvd = pyuvdata.UVData()\n",
    "uvd.read_miriad(miriad_path)\n",
    "\n",
    "# get the redundancies for that data\n",
    "aa = hc.utils.get_aa_from_uv(uvd)\n",
    "info = hc.omni.aa_to_info(aa)\n",
    "red_bls = np.array(info.get_reds())\n",
    "\n",
    "# gains for same data \n",
    "calfits_path = data_directory + 'zen.2458098.58037.xx.HH.uv.abs.calfits'\n",
    "gains, flags = hc.io.load_cal(calfits_path)\n",
    "gains_c = {key : gains[key].conjugate() for key in gains.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ants_good(bl, gain_keys):\n",
    "    \"\"\"Returns True if BOTH antennas from bl are in gain_keys.\"\"\"\n",
    "    ants = [a[0] for a in gain_keys] \n",
    "    if bl[0] in ants and bl[1] in ants:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def elemental_baselines(red_bls):\n",
    "    bls = []\n",
    "    for sublist in sorted(red_bls):\n",
    "        bls.append(sorted(sublist)[0])\n",
    "    return sorted(bls)\n",
    "\n",
    "def good_redundant_baselines_dict(good_redundant_baselines, elemental_baselines):\n",
    "    bls_dict = {}\n",
    "    for sublist in good_redundant_baselines:\n",
    "        for elemental in elemental_baselines:\n",
    "            if elemental in sublist:\n",
    "                bls_dict[elemental] = sorted(sublist)\n",
    "                break\n",
    "    return bls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run ONCE\n",
    "\n",
    "# # eliminate baselines that are made up of only bad antennas\n",
    "# # Also we need at least two baselines per group\n",
    "# good_redundant_baselines = []\n",
    "# for group in red_bls:\n",
    "#     new_group = []\n",
    "#     for bl in group:\n",
    "#         if ants_good(bl, gains.keys()) == True:\n",
    "#             new_group.append(bl)\n",
    "#     if len(new_group) > 1:\n",
    "#         good_redundant_baselines.append(new_group)\n",
    "\n",
    "# training_percent = 0.80\n",
    "# training_redundant_baselines, testing_redundant_baselines = np.split(np.random.permutation(good_redundant_baselines),\n",
    "#                                                                      [int(len(good_redundant_baselines)*training_percent)])\n",
    "# np.savez('training_redundant_baselines', training_redundant_baselines)\n",
    "# np.savez('testing_redundant_baselines', testing_redundant_baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "def loadnpz(filename):\n",
    "    a = load(filename)\n",
    "    d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "    return d['data1arr_0']\n",
    "\n",
    "training_redundant_baselines = loadnpz('training_redundant_baselines.npz')\n",
    "testing_redundant_baselines = loadnpz('testing_redundant_baselines.npz')\n",
    "\n",
    "# list of the elemental baseline for each group\n",
    "training_elemental_baselines = elemental_baselines(training_redundant_baselines)\n",
    "testing_elemental_baselines = elemental_baselines(testing_redundant_baselines)\n",
    "\n",
    "# dictionaries of the baseline groups, keys are elemental baselines\n",
    "training_baselines_dict = good_redundant_baselines_dict(training_redundant_baselines, training_elemental_baselines)\n",
    "testing_baselines_dict = good_redundant_baselines_dict(testing_redundant_baselines, testing_elemental_baselines)\n",
    "\n",
    "# baselines as keys\n",
    "training_baselines_data = {}\n",
    "for elemental in training_elemental_baselines:\n",
    "    for bl in training_baselines_dict[elemental]:\n",
    "        training_baselines_data[bl] = uvd.get_data(bl)\n",
    "\n",
    "training_baselines_data_c = {key : training_baselines_data[key].conjugate() for key in training_baselines_data.keys()}\n",
    "\n",
    "testing_baselines_data = {}\n",
    "for elemental in testing_elemental_baselines:\n",
    "    for bl in testing_baselines_dict[elemental]:\n",
    "        testing_baselines_data[bl] = uvd.get_data(bl)\n",
    "        \n",
    "testing_baselines_data_c = {key : testing_baselines_data[key].conjugate() for key in testing_baselines_data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling tools\n",
    "angle_tx =  lambda x: (np.array(x) + np.pi)/(2*np.pi)\n",
    "angle_itx = lambda x: np.array(x)*2*np.pi - np.pi\n",
    "\n",
    "delay_tx = lambda x: (np.array(x) + 0.040)/(2*0.040)\n",
    "delay_itx = lambda x: np.array(x)*2*0.040 - 0.040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Creator(object):\n",
    "## usage:\n",
    "## data_maker = data_creator(num_flatnesses=250, mode = 'train')\n",
    "## data_maker.gen_data() (before loop)\n",
    "## inputs, targets = data_maker.get_data() (start of loop)\n",
    "## data_maker.gen_data() (immediately after get_data())\n",
    "\n",
    "    def __init__(self, num_flatnesses, mode = 'train'):\n",
    "        \n",
    "        self.num = num_flatnesses\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        \n",
    "    def _gen_data(self):\n",
    "        \n",
    "        nu = np.arange(1024)\n",
    "        \n",
    "        if self.mode == 'clean':\n",
    "            targets = np.random.uniform(low = -.040, high = 0.040, size = (self.num*60,1))\n",
    "            inputs = np.angle(np.exp(-2j*np.pi*(targets*nu + np.random.uniform())))\n",
    "            d = angle_tx(inputs), delay_tx(targets)\n",
    "            self.data.append(d)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            def _flatness(baselines, data, data_c):\n",
    "                \"\"\"Create a flatness from a given pair of baselines, their data & their gains.\"\"\"\n",
    "\n",
    "                a, b = baselines[0][0], baselines[0][1]\n",
    "                c, d = baselines[1][0], baselines[1][1]\n",
    "\n",
    "                return data[baselines[0]]*data_c[baselines[1]] * gains_c[(a,'x')] * gains[(b,'x')] * gains[(c,'x')] * gains_c[(d,'x')]\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                elemental_baselines = training_elemental_baselines\n",
    "                baselines_dict = training_baselines_dict\n",
    "                baseline_data = training_baselines_data\n",
    "                baseline_data_c = training_baselines_data_c\n",
    "\n",
    "            if self.mode == 'test':\n",
    "                elemental_baselines = testing_elemental_baselines\n",
    "                baselines_dict = testing_baselines_dict\n",
    "                baseline_data = testing_baselines_data\n",
    "                baseline_data_c = testing_baselines_data_c\n",
    "\n",
    "            inputs = []\n",
    "            for _ in range(self.num):\n",
    "\n",
    "                elemental_baseline = random.sample(elemental_baselines, 1)[0]\n",
    "                two_baselines = [random.sample(baselines_dict[elemental_baseline], 2)][0]\n",
    "                inputs.append(_flatness(two_baselines, baseline_data, baseline_data_c))\n",
    "\n",
    "            targets = np.random.uniform(low = -.040, high = 0.040, size = (self.num*60, 1))\n",
    "            inputs = np.angle(np.array(inputs).reshape(-1,1024) * np.exp(-2j*np.pi*(targets*nu + np.random.uniform())))\n",
    "\n",
    "            d = angle_tx(inputs), delay_tx(targets)\n",
    "            self.data.append(d)\n",
    "\n",
    "    def gen_data(self):\n",
    "        self.thread = threading.Thread(target = self._gen_data, args=())\n",
    "        self.thread.start()\n",
    "\n",
    "    def get_data(self, timeout = 10):\n",
    "        \n",
    "        if len(self.data) == 0:\n",
    "            self.thread.join(timeout)\n",
    "            \n",
    "        return self.data.pop(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plot(predicted_values, actual_values):\n",
    "    \"\"\"Create a prediction plot and save to byte string.\"\"\"\n",
    "\n",
    "    prediction_unscaled = delay_itx(predicted_values)\n",
    "    actual_unscaled = delay_itx(actual_values)\n",
    "\n",
    "    sorting_idx = np.argsort(actual_unscaled.T[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (5, 3), dpi = 144)\n",
    "\n",
    "    ax.plot(prediction_unscaled.T[0][sorting_idx],\n",
    "            linestyle = 'none', marker = '.', markersize = 1,\n",
    "            color = 'darkblue')\n",
    "\n",
    "    ax.plot(actual_unscaled.T[0][sorting_idx],\n",
    "            linestyle = 'none', marker = '.', markersize = 1, alpha = 0.50,\n",
    "            color = '#E50000')       \n",
    "\n",
    "    ax.set_title('std: %.9f' %np.std(prediction_unscaled.T[0][sorting_idx] - actual_unscaled.T[0][sorting_idx]))\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png', dpi = 144)\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "\n",
    "    return buf.getvalue()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flat_CNN(object):\n",
    "    \"\"\"A neural network of multi-path layers. Filters for each path have shape_height = 1\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 wide_filter_widths = [],\n",
    "                 width_reduction_factors = [],\n",
    "                 dtype = tf.float32,\n",
    "                 num_freq_channels = 1024,\n",
    "                 learning_rate = 0.0001):\n",
    "\n",
    "        self.wide_filter_widths = wide_filter_widths\n",
    "        self.width_reduction_factors = width_reduction_factors\n",
    "        self.dtype = dtype\n",
    "        self.num_freq_channels = num_freq_channels\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.num_layers = len(self.wide_filter_widths)\n",
    "        \n",
    "        \n",
    "    def _quad_path_layer(self, input, wide_conv_width, strides, layer_name, num_1x1_conv_filters = 4):\n",
    "\n",
    "        # convolution filters\n",
    "        conv_filters = lambda shape : tf.get_variable(name = 'filters',\n",
    "                                                      dtype = self.dtype,\n",
    "                                                      shape = shape,\n",
    "                                                      initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "        def _bias_add_scope(input, shape):\n",
    "            \"\"\"Creates a scope around a trainable bias and its addition to input\"\"\"\n",
    "            with tf.variable_scope('add_bias'):\n",
    "\n",
    "                bias = tf.get_variable(name = 'bias', dtype = self.dtype, shape = shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "                bias_add = tf.nn.bias_add(input, bias)\n",
    "\n",
    "            return bias_add\n",
    "\n",
    "\n",
    "        def _conv_scope(input, filter_shape, strides, scope_name = 'convolution'):\n",
    "            \"\"\"Creates a scope around a convolution.\"\"\"\n",
    "            with tf.variable_scope(scope_name):\n",
    "\n",
    "                conv = tf.nn.conv2d(input = input, filter = conv_filters(filter_shape), strides = strides, padding = 'SAME') \n",
    "                conv = _bias_add_scope(conv, [filter_shape[-1]])\n",
    "                conv = tf.nn.relu(conv)\n",
    "                conv = tf.nn.dropout(conv, self.conv_keep_prob)\n",
    "\n",
    "            return conv\n",
    "\n",
    "        def _avg_scope(input, strides, num_conv_filters):\n",
    "            \"\"\"Creates a scope around the average-pool path.\"\"\"\n",
    "            with tf.variable_scope('average'):\n",
    "                avg_pool = tf.nn.avg_pool(value = input, ksize = strides, strides = strides, padding = \"SAME\")\n",
    "\n",
    "                convolution_filter_shape = [1,1,avg_pool.get_shape().as_list()[3], num_conv_filters]\n",
    "                avg = _conv_scope(avg_pool, convolution_filter_shape, [1,1,1,1], scope_name = \"1x1_conv\")\n",
    "\n",
    "            return avg\n",
    "\n",
    "        def _max_scope(input, strides,  num_conv_filters):\n",
    "            \"\"\"Creates a scope around the max-pool path\"\"\"\n",
    "            with tf.variable_scope('max'):\n",
    "                max_pool = tf.nn.max_pool(value = input, ksize = strides, strides = strides, padding = \"SAME\")\n",
    "\n",
    "                convolution_filter_shape = [1,1,max_pool.get_shape().as_list()[3],num_conv_filters]\n",
    "                max_ = _conv_scope(max_pool, convolution_filter_shape, [1,1,1,1], scope_name = \"1x1_conv\")\n",
    "\n",
    "            return max_\n",
    "\n",
    "        def _filter_cat_scope(filters):\n",
    "            \"\"\"Creates a scope around filter concatation (layer output)\"\"\"\n",
    "            with tf.variable_scope('filter_cat'):\n",
    "                filter_cat = tf.concat(filters, 3)\n",
    "            return filter_cat\n",
    "\n",
    "        ######\n",
    "\n",
    "        with tf.variable_scope(layer_name):\n",
    "\n",
    "            narrow_conv_width = wide_conv_width / 2\n",
    "\n",
    "            num_narrow_conv_filters = num_1x1_conv_filters / 2\n",
    "            num_wide_conv_filters = num_narrow_conv_filters / 2\n",
    "\n",
    "            _1x1_strides = [1,1,1,1]\n",
    "\n",
    "            avg_output = _avg_scope(input, strides, num_1x1_conv_filters)\n",
    "            max_output = _max_scope(input, strides, num_1x1_conv_filters)\n",
    "\n",
    "            inital_conv = _conv_scope(input, [1,1,input.get_shape().as_list()[3],num_1x1_conv_filters], [1,1,1,1], '1x1_conv')\n",
    "\n",
    "            narrow_convolution = _conv_scope(inital_conv, [1,narrow_conv_width,inital_conv.get_shape().as_list()[3],num_narrow_conv_filters], strides, scope_name = 'narrow')\n",
    "            wide_convolution = _conv_scope(inital_conv, [1,wide_conv_width,inital_conv.get_shape().as_list()[3],num_wide_conv_filters], strides, scope_name = 'wide')\n",
    "\n",
    "            catted_filters = _filter_cat_scope([avg_output, narrow_convolution, wide_convolution, max_output])\n",
    "\n",
    "        return catted_filters\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def create_graph(self):\n",
    "        # creates the network graph\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Note, tf.placeholder() are assigned by tf.Session()\n",
    "\n",
    "        with tf.variable_scope('keep_probs'):\n",
    "            # Dropout rate = 1 - keep_prob\n",
    "\n",
    "            # probability of keeping sample_keep_prob\n",
    "            # suggest 0.8\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "\n",
    "            # probability of keeping convolution output\n",
    "            # suggest 0.9\n",
    "            self.conv_keep_prob = tf.placeholder(self.dtype, name = 'conv_keep_prob')\n",
    "\n",
    "            # probability of keeping fully connected layer output\n",
    "            # suggest 0.95\n",
    "            self.fcl_keep_prob = tf.placeholder(self.dtype, name = 'fcl_keep_prob')        \n",
    "\n",
    "        with tf.variable_scope('sample'):\n",
    "            # holds the 1 x num_channels samples that are fed into the network\n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1, self.num_freq_channels, 1], name = 'X')\n",
    "            self.X_dropout = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "\n",
    "        self.layers = []\n",
    "        layer_names = ['layer_{}'.format(i) for i in range(self.num_layers)]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # previous layer is input for current layer\n",
    "            input = self.X_dropout if i == 0 else self.layers[i - 1]\n",
    "            strides = [1, 1, self.width_reduction_factors[i], 1]\n",
    "            q_layer = self._quad_path_layer(input, self.wide_filter_widths[i], strides, layer_names[i])\n",
    "            self.layers.append(q_layer)\n",
    "                \n",
    "        with tf.variable_scope('fcl_1'):\n",
    "            \n",
    "            fcl_1 = tf.contrib.layers.flatten(self.layers[-1])\n",
    "            fcl_1 = tf.contrib.layers.fully_connected(fcl_1, 1024)\n",
    "            fcl_1 = tf.nn.dropout(fcl_1, self.fcl_keep_prob)\n",
    "                \n",
    "            self.layers.append(fcl_1)\n",
    "            \n",
    "        with tf.variable_scope('fcl_2'):\n",
    "            \n",
    "            fcl_2 = tf.contrib.layers.flatten(self.layers[-1])\n",
    "            fcl_2 = tf.contrib.layers.fully_connected(fcl_1, 32)\n",
    "            fcl_2 = tf.nn.dropout(fcl_2, self.fcl_keep_prob)\n",
    "                                \n",
    "            self.layers.append(fcl_2)\n",
    "            \n",
    "        with tf.variable_scope('prediction'):\n",
    "            reshape_final_layer = tf.reshape(self.layers[-1], [-1, np.prod(self.layers[-1].get_shape().as_list()[1:])])\n",
    "            prediction_weight = tf.get_variable(name = 'weight', shape = [np.prod(self.layers[-1].get_shape()[1:]), 1.], dtype = self.dtype, initializer = tf.contrib.layers.xavier_initializer())\n",
    "            pred_times_weight = tf.matmul(reshape_final_layer, prediction_weight)\n",
    "            self.predictions = tf.nn.bias_add(pred_times_weight, [1])\n",
    "\n",
    "        with tf.variable_scope('targets'):\n",
    "            self.targets = tf.placeholder(dtype = self.dtype, shape = [None, 1], name = 'targets')\n",
    "\n",
    "        with tf.variable_scope('costs'):\n",
    "\n",
    "            self.error = tf.subtract(self.targets, self.predictions, name = 'error')\n",
    "            self.squared_error = tf.square(self.error, name = 'squared_difference')\n",
    "\n",
    "            with tf.variable_scope('mean_inverse_shifted_gaussian'):\n",
    "                with tf.variable_scope('normal_distribution'):\n",
    "                    self.threshold = 0.00625\n",
    "                    sigma = tf.constant(self.threshold, name = 'sigma')\n",
    "                    normal_dist = tf.contrib.distributions.Normal(0.0, sigma, name = 'normal_dist')\n",
    "                    gaussian_prob = normal_dist.prob(self.error, name = 'gaussian_prob')\n",
    "                    shifted_gaussian = tf.add(gaussian_prob, .01, name = 'shifted_gaussian')        \n",
    "\n",
    "                self.MISG = tf.reduce_mean(tf.divide(1.0, shifted_gaussian), name = 'mean_inverse_shifted_gaussian')\n",
    "\n",
    "            with tf.variable_scope('mean_squared_error'):\n",
    "                self.MSE = tf.reduce_mean(self.squared_error)\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate, epsilon=1e-08).minimize(self.MSE)\n",
    "\n",
    "        with tf.variable_scope('logging'):  \n",
    "\n",
    "            with tf.variable_scope('image'):\n",
    "                self.image_buf = tf.placeholder(tf.string, shape=[])\n",
    "                epoch_image = tf.expand_dims(tf.image.decode_png(self.image_buf, channels=4), 0)\n",
    "\n",
    "            with tf.variable_scope('percent_within_threshold'):\n",
    "                self.PWT = tf.reduce_mean(tf.cast(tf.less_equal(self.targets - self.predictions, self.threshold), self.dtype) )\n",
    "\n",
    "\n",
    "            tf.summary.histogram(name = 'targets', values = self.targets)\n",
    "            tf.summary.histogram(name = 'predictions',values =  self.predictions)\n",
    "            tf.summary.scalar(name = 'MSE', tensor = self.MSE)\n",
    "            tf.summary.scalar(name = 'MISG', tensor = self.MISG)\n",
    "            tf.summary.scalar(name = 'PWT', tensor = self.PWT)\n",
    "            tf.summary.image('prediction_vs_actual', epoch_image)\n",
    "            self.summary = tf.summary.merge_all()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, num_flatnesses, num_epochs, batch_size, log_dir, model_save_interval, pretrained_model_path = None, sample_keep_prob = 0.80, conv_keep_prob = 0.9, fcl_keep_prob = 0.50):\n",
    "\n",
    "    num = num_flatnesses\n",
    "    num_entries = num * 60\n",
    "\n",
    "    MISG = []\n",
    "    MSE = []\n",
    "\n",
    "    train_batcher = Data_Creator(num, mode = 'train')\n",
    "    train_batcher.gen_data()\n",
    "\n",
    "\n",
    "    test_batcher = Data_Creator(num, mode = 'test')\n",
    "    test_batcher.gen_data()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        if pretrained_model_path == None:\n",
    "            session.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            saver.restore(session, pretrained_model_path)\n",
    "\n",
    "        training_writer = tf.summary.FileWriter(log_dir + '/training', session.graph)\n",
    "        testing_writer = tf.summary.FileWriter(log_dir + '/testing', session.graph)\n",
    "        model_save_location = log_dir + '/trained_model.ckpt'   \n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            training_inputs, training_targets = train_batcher.get_data(); train_batcher.gen_data()\n",
    "            testing_inputs, testing_targets = test_batcher.get_data(); test_batcher.gen_data()  \n",
    "\n",
    "            for j in range(int(num_entries/batch_size)):\n",
    "\n",
    "                training_inputs_batch = training_inputs[j*batch_size:(j + 1)*batch_size].reshape(-1,1,1024,1)\n",
    "                training_targets_batch = training_targets[j*batch_size:(j + 1)*batch_size].reshape(-1,1)\n",
    "                \n",
    "\n",
    "                session.run([network.optimizer], feed_dict = {network.X: training_inputs_batch,\n",
    "                                                              network.targets: training_targets_batch,\n",
    "                                                              network.sample_keep_prob : sample_keep_prob,\n",
    "                                                              network.conv_keep_prob : conv_keep_prob,\n",
    "                                                              network.fcl_keep_prob : fcl_keep_prob}) \n",
    "            # Prediction: Scaled Train(ing results)   \n",
    "            PST = session.run(network.predictions,\n",
    "                              feed_dict = {network.X: training_inputs.reshape(-1,1,1024,1),\n",
    "                                           network.sample_keep_prob : 1.,\n",
    "                                           network.conv_keep_prob : 1.,\n",
    "                                           network.fcl_keep_prob : 1.}) \n",
    "\n",
    "\n",
    "            training_MISG, training_MSE, training_summary = session.run([network.MISG, network.MSE, network.summary],\n",
    "                                                                        feed_dict = {network.X: training_inputs.reshape(-1,1,1024,1),\n",
    "                                                                        network.targets: training_targets.reshape(-1,1),\n",
    "                                                                        network.sample_keep_prob : 1.,\n",
    "                                                                        network.conv_keep_prob : 1.,\n",
    "                                                                        network.fcl_keep_prob : 1.,\n",
    "                                                                        network.image_buf: gen_plot(PST,training_targets)}) \n",
    "\n",
    "            sys.stdout.write('\\r' + \"Epoch: \" + str(epoch) + \". Training: MISG = {:.6f}, MSE = {:.6f}\".format(training_MISG, training_MSE))\n",
    "\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            training_writer.flush()  \n",
    "\n",
    "            # Prediction: Scaled test(ing results)   \n",
    "            PSt = session.run(network.predictions,\n",
    "                              feed_dict = {network.X: testing_inputs.reshape(-1,1,1024,1),\n",
    "                                           network.sample_keep_prob : 1.,\n",
    "                                           network.conv_keep_prob : 1.,\n",
    "                                           network.fcl_keep_prob : 1.}) \n",
    "\n",
    "            testing_MISG, testing_MSE, testing_summary = session.run([network.MISG, network.MSE, network.summary],\n",
    "                                                                      feed_dict = {network.X: testing_inputs.reshape(-1,1,1024,1),\n",
    "                                                                                   network.targets: testing_targets.reshape(-1,1),\n",
    "                                                                                   network.sample_keep_prob : 1.,\n",
    "                                        \n",
    "                                                                                   network.conv_keep_prob : 1.,\n",
    "                                                                                   network.fcl_keep_prob : 1.,\n",
    "                                                                                   network.image_buf: gen_plot(PSt,testing_targets)}) \n",
    "\n",
    "            sys.stdout.write('\\r' + \"Epoch: \" + str(epoch) + \". Testing: MISG = {:.6f}, MSE = {:.6f}\".format(testing_MISG, testing_MSE))\n",
    "\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            testing_writer.flush()  \n",
    "\n",
    "            MISG.append((training_MISG, testing_MISG))\n",
    "            MSE.append((training_MSE, testing_MSE))\n",
    "\n",
    "            if (epoch + 1) % model_save_interval == 0:\n",
    "                saver.save(session, model_save_location, epoch)\n",
    "\n",
    "\n",
    "\n",
    "        print('\\rDone')\n",
    "\n",
    "        training_writer.close()\n",
    "        testing_writer.close()\n",
    "\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesnt crash :)\n",
    "# doesnt learn :( \n",
    "\n",
    "# network = Flat_CNN([32,16,8], [4,4,4], learning_rate = 0.0001)\n",
    "# network.create_graph()\n",
    "# train(network, 250, 10, 32, 'logs/TestB', 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
