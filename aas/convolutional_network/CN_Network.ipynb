{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN_Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from CNN_Train.ipynb\n",
      "importing Jupyter notebook from CNN_Data.ipynb\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import prod, savez, load\n",
    "from pprint import pprint\n",
    "import sys, os\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../modules'))\n",
    "import notebook_loading\n",
    "\n",
    "from CNN_Train import CNN_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CN_Network(object):\n",
    "    \"\"\"A neural network of multi-path layers.\n",
    "    Filters for each path have shape_height = 1.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name = 'Flat_CNN',\n",
    "                 wide_filter_widths = [],\n",
    "                 width_reduction_factors = [],\n",
    "                 dtype = tf.float32,\n",
    "                 num_freq_channels = 1024,\n",
    "                 learning_rate = 0.0001,\n",
    "                 cost_name = 'MSE',\n",
    "                 threshold = 0.00625,\n",
    "                 g_shift = 0.01):\n",
    "        \n",
    "        self.name = name\n",
    "        self.wide_filter_widths = wide_filter_widths\n",
    "        self.width_reduction_factors = width_reduction_factors\n",
    "        self.dtype = dtype\n",
    "        self.num_freq_channels = num_freq_channels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cost_name = cost_name\n",
    "        self.threshold = threshold\n",
    "        self.g_shift = g_shift\n",
    "        \n",
    "        self.params = {name : self.name,\n",
    "                       wide_filter_widths : self.wide_filter_widths,\n",
    "                       width_reduction_factors : self.width_reduction_factors,\n",
    "                       dtype : self.dtype\n",
    "                       num_freq_channels : self.num_freq_channels,\n",
    "                       learning_rate : self.learning_rate,\n",
    "                       cost_name : self.cost_name,\n",
    "                       threshold : self.threshold,\n",
    "                       g_shift : self.g_shift}\n",
    "        \n",
    "    def _save_params(self, param_direc = 'network_params/'):\n",
    "        \"\"\"Not safe - will overwrite existing file.\"\"\"\n",
    "        if not os.path.exists(param_direc):\n",
    "            os.makedirs(param_direc)\n",
    "        savez(param_direc + self.name, self.params)   \n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old network. Does not lad the name of the old network.\"\"\"\n",
    "    \n",
    "        a = load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        self.params = d['data1arr_0'][()]\n",
    "        \n",
    "        # make sure to set the name for the current network \n",
    "        self.params['name'] = self.name\n",
    "        self._set_params()\n",
    "        \n",
    "        \n",
    "    def print_params(self):\n",
    "        pprint(self.params)\n",
    "        \n",
    "    def _quad_path_layer(self, input, wide_conv_width, strides, layer_name, num_1x1_conv_filters = 4):\n",
    "\n",
    "        # convolution filters\n",
    "        conv_filters = lambda shape : tf.get_variable(name = 'filters',\n",
    "                                                      dtype = self.dtype,\n",
    "                                                      shape = shape,\n",
    "                                                      initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "        def _bias_add_scope(input, shape):\n",
    "            \"\"\"Creates a scope around a trainable bias and its addition to input\"\"\"\n",
    "            with tf.variable_scope('add_bias'):\n",
    "\n",
    "                bias = tf.get_variable(name = 'bias', dtype = self.dtype, shape = shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "                bias_add = tf.nn.bias_add(input, bias)\n",
    "\n",
    "            return bias_add\n",
    "\n",
    "\n",
    "        def _conv_scope(input, filter_shape, strides, scope_name = 'convolution'):\n",
    "            \"\"\"Creates a scope around a convolution.\"\"\"\n",
    "            with tf.variable_scope(scope_name):\n",
    "\n",
    "                conv = tf.nn.conv2d(input = input, filter = conv_filters(filter_shape), strides = strides, padding = 'SAME') \n",
    "                conv = _bias_add_scope(conv, [filter_shape[-1]])\n",
    "                conv = tf.nn.relu(conv)\n",
    "                conv = tf.nn.dropout(conv, self.conv_keep_prob)\n",
    "\n",
    "            return conv\n",
    "\n",
    "        def _avg_scope(input, strides, num_conv_filters):\n",
    "            \"\"\"Creates a scope around the average-pool path.\"\"\"\n",
    "            with tf.variable_scope('average'):\n",
    "                avg_pool = tf.nn.avg_pool(value = input, ksize = strides, strides = strides, padding = \"SAME\")\n",
    "\n",
    "                convolution_filter_shape = [1,1,avg_pool.get_shape().as_list()[3], num_conv_filters]\n",
    "                avg = _conv_scope(avg_pool, convolution_filter_shape, [1,1,1,1], scope_name = \"1x1_conv\")\n",
    "\n",
    "            return avg\n",
    "\n",
    "        def _max_scope(input, strides,  num_conv_filters):\n",
    "            \"\"\"Creates a scope around the max-pool path\"\"\"\n",
    "            with tf.variable_scope('max'):\n",
    "                max_pool = tf.nn.max_pool(value = input, ksize = strides, strides = strides, padding = \"SAME\")\n",
    "\n",
    "                convolution_filter_shape = [1,1,max_pool.get_shape().as_list()[3],num_conv_filters]\n",
    "                max_ = _conv_scope(max_pool, convolution_filter_shape, [1,1,1,1], scope_name = \"1x1_conv\")\n",
    "\n",
    "            return max_\n",
    "\n",
    "        def _filter_cat_scope(filters):\n",
    "            \"\"\"Creates a scope around filter concatation (layer output)\"\"\"\n",
    "            with tf.variable_scope('filter_cat'):\n",
    "                filter_cat = tf.concat(filters, 3)\n",
    "            return filter_cat\n",
    "\n",
    "        ######\n",
    "\n",
    "        with tf.variable_scope(layer_name):\n",
    "\n",
    "            narrow_conv_width = wide_conv_width / 2\n",
    "\n",
    "            num_narrow_conv_filters = num_1x1_conv_filters / 2\n",
    "            num_wide_conv_filters = num_narrow_conv_filters / 2\n",
    "\n",
    "            _1x1_strides = [1,1,1,1]\n",
    "\n",
    "            avg_output = _avg_scope(input, strides, num_1x1_conv_filters)\n",
    "            max_output = _max_scope(input, strides, num_1x1_conv_filters)\n",
    "\n",
    "            inital_conv = _conv_scope(input, [1,1,input.get_shape().as_list()[3],num_1x1_conv_filters], [1,1,1,1], '1x1_conv')\n",
    "\n",
    "            narrow_convolution = _conv_scope(inital_conv, [1,narrow_conv_width,inital_conv.get_shape().as_list()[3],num_narrow_conv_filters], strides, scope_name = 'narrow')\n",
    "            wide_convolution = _conv_scope(inital_conv, [1,wide_conv_width,inital_conv.get_shape().as_list()[3],num_wide_conv_filters], strides, scope_name = 'wide')\n",
    "\n",
    "            catted_filters = _filter_cat_scope([avg_output, narrow_convolution, wide_convolution, max_output])\n",
    "\n",
    "        return catted_filters\n",
    "        \n",
    "    def create_graph(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # creates the network graph\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Note, tf.placeholder() are assigned by tf.Session()\n",
    "\n",
    "        with tf.variable_scope('keep_probs'):\n",
    "            # Dropout rate = 1 - keep_prob\n",
    "\n",
    "            # probability of keeping sample_keep_prob\n",
    "            # suggest 0.8\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "\n",
    "            # probability of keeping convolution output\n",
    "            # suggest 0.9\n",
    "            self.conv_keep_prob = tf.placeholder(self.dtype, name = 'conv_keep_prob')\n",
    "\n",
    "            # probability of keeping fully connected layer output\n",
    "            # suggest 0.50\n",
    "            self.fcl_keep_prob = tf.placeholder(self.dtype, name = 'fcl_keep_prob')        \n",
    "\n",
    "        with tf.variable_scope('sample'):\n",
    "            # holds the 1 x num_channels samples that are fed into the network\n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1, self.num_freq_channels, 1], name = 'X')\n",
    "            self.X_dropout = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "\n",
    "        self.layers = []\n",
    "        num_layers = len(self.wide_filter_widths)\n",
    "        layer_names = ['layer_{}'.format(i) for i in range(num_layers)]\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            # previous layer is input for current layer\n",
    "            input = self.X_dropout if i == 0 else self.layers[i - 1]\n",
    "            strides = [1, 1, self.width_reduction_factors[i], 1]\n",
    "            q_layer = self._quad_path_layer(input, self.wide_filter_widths[i], strides, layer_names[i])\n",
    "            self.layers.append(q_layer)\n",
    "                \n",
    "        with tf.variable_scope('fcl_1'):\n",
    "            \n",
    "            fcl_1 = tf.contrib.layers.flatten(self.layers[-1])\n",
    "            fcl_1 = tf.contrib.layers.fully_connected(fcl_1, 1024)\n",
    "            fcl_1 = tf.nn.dropout(fcl_1, self.fcl_keep_prob)\n",
    "                \n",
    "            self.layers.append(fcl_1)\n",
    "            \n",
    "        with tf.variable_scope('fcl_2'):\n",
    "            \n",
    "            fcl_2 = tf.contrib.layers.flatten(self.layers[-1])\n",
    "            fcl_2 = tf.contrib.layers.fully_connected(fcl_1, 32)\n",
    "            fcl_2 = tf.nn.dropout(fcl_2, self.fcl_keep_prob)\n",
    "                                \n",
    "            self.layers.append(fcl_2)\n",
    "            \n",
    "        with tf.variable_scope('prediction'):\n",
    "            reshape_final_layer = tf.reshape(self.layers[-1], [-1, prod(self.layers[-1].get_shape().as_list()[1:])])\n",
    "            prediction_weight = tf.get_variable(name = 'weight', shape = [prod(self.layers[-1].get_shape()[1:]), 1.], dtype = self.dtype, initializer = tf.contrib.layers.xavier_initializer())\n",
    "            pred_times_weight = tf.matmul(reshape_final_layer, prediction_weight)\n",
    "            self.predictions = tf.nn.bias_add(pred_times_weight, [1])\n",
    "\n",
    "        with tf.variable_scope('targets'):\n",
    "            self.targets = tf.placeholder(dtype = self.dtype, shape = [None, 1], name = 'targets')\n",
    "\n",
    "        with tf.variable_scope('costs'):\n",
    "\n",
    "            error = tf.subtract(self.targets, self.predictions, name = 'error')\n",
    "            squared_error = tf.square(error, name = 'squared_difference')\n",
    "\n",
    "            with tf.variable_scope('mean_inverse_shifted_gaussian'):\n",
    "                with tf.variable_scope('normal_distribution'):\n",
    "                    \n",
    "                    sigma = tf.constant(self.threshold, name = 'sigma')\n",
    "                    normal_dist = tf.contrib.distributions.Normal(0.0, sigma, name = 'normal_dist')\n",
    "                    gaussian_prob = normal_dist.prob(error, name = 'gaussian_prob')\n",
    "                    shift = tf.constant(self.g_shift, name = 'gaussian_shift_value')\n",
    "                    shifted_gaussian = tf.add(gaussian_prob, shift, name = 'shifted_gaussian')        \n",
    "                self.MISG = tf.reduce_mean(tf.divide(1.0, shifted_gaussian), name = 'mean_inverse_shifted_gaussian')\n",
    "                \n",
    "            with tf.variable_scope('mean_squared_error'):\n",
    "                self.MSE = tf.reduce_mean(squared_error)\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            \n",
    "            cost = self.MSE if self.cost_name == 'MSE' else self.MISG\n",
    "            LR = tf.constant(self.learning_rate, name = 'learning_rate')\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(LR, epsilon=1e-08).minimize(cost)\n",
    "\n",
    "        with tf.variable_scope('logging'):  \n",
    "\n",
    "            with tf.variable_scope('image'):\n",
    "                self.image_buf = tf.placeholder(tf.string, shape=[])\n",
    "                epoch_image = tf.expand_dims(tf.image.decode_png(self.image_buf, channels=4), 0)\n",
    "\n",
    "            with tf.variable_scope('percent_within_threshold'):\n",
    "                self.PWT = 100.*tf.reduce_mean(tf.cast(tf.less_equal(tf.abs(self.targets - self.predictions), sigma), self.dtype) )\n",
    "\n",
    "\n",
    "            tf.summary.histogram(name = 'targets', values = self.targets)\n",
    "            tf.summary.histogram(name = 'predictions',values =  self.predictions)\n",
    "            tf.summary.scalar(name = 'MSE', tensor = self.MSE)\n",
    "            tf.summary.scalar(name = 'MISG', tensor = self.MISG)\n",
    "            tf.summary.scalar(name = 'PWT', tensor = self.PWT)\n",
    "            tf.summary.image('prediction_vs_actual', epoch_image)\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            \n",
    "        print('Graph Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide filters will have widths of...<br>\n",
    "`wide_filter_widths = [64,32,8]`\n",
    "\n",
    "Layers will downsample by ...<br>\n",
    "`width_reduction_factors = [2,3,4]`\n",
    "\n",
    "Kansas is really flat ...<br>\n",
    "`Kansas = Flat_CNN(name = 'Kansas-A',`<br>\n",
    "                  `wide_filter_widths = wide_filter_widths,`<br>\n",
    "                  `width_reduction_factors = width_reduction_factors)`<br>\n",
    "\n",
    "`Kansas.print_params()`<br>\n",
    "\n",
    "`{'cost_name': 'MSE',`<br>\n",
    " `'dtype': tf.float32,`<br>\n",
    " `'g_shift': 0.01,`<br>\n",
    " `'learning_rate': 0.0001,`<br>\n",
    " `'name': 'Kansas-A',`<br>\n",
    " `'num_freq_channels': 1024,`<br>\n",
    " `'threshold': 0.00625,`<br>\n",
    " `'wide_filter_widths': [64, 32, 8],`<br>\n",
    " `'width_reduction_factors': [2, 3, 4]}``<br>\n",
    "\n",
    "Before a training loop:<br>\n",
    "`Kansas.create_graph()`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate a previous network, (create a new network with the same parameters as an old network)\n",
    " - currently the loading of previously trained weights is handed by the separate training function.\n",
    " - On reload can change all settings except for:\n",
    "  - num_freq_channels\n",
    "  - dtype (pretty sure)\n",
    "  - wide_filter_widths\n",
    "  - width_reduction_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pancakes are flat... like Kansas<br>\n",
    "`Pancake = Flat_CNN(name = 'Pancake-A')`<br>\n",
    "\n",
    "`Pancake.load_params('network_params/Kansas-A')`<br>\n",
    "\n",
    "`Pancake.print_params()`<br>\n",
    "\n",
    "Before training loop\n",
    "`Pancake.create_graph()`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
