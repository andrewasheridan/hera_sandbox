{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos_Neg_NN\n",
    "\n",
    " - To determine the sign (positive or negative) of the cable delay only. Mostly practice to see if I can make a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an incoming sample (1x1024) will have a label of either positive or negative\n",
    " - so, two classes, label of type [positive, negative]\n",
    "  - ex: positive = [1, 0]\n",
    "  - ex: negative = [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pos_Neg_NN(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 name, \n",
    "                 num_downsamples,\n",
    "                 log_dir = 'logs/',\n",
    "                 dtype = tf.float32,\n",
    "                 adam_inital_learning_rate = 0.0001,\n",
    "                 adam_epsilon = 1e-8,\n",
    "                 verbose = False):\n",
    "    \n",
    "        self.name = name\n",
    "        self.num_downsamples = num_downsamples\n",
    "        self.log_dir = log_dir\n",
    "        self.dtype = dtype\n",
    "        self.adam_inital_learning_rate = adam_inital_learning_rate\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        self._num_freq_channels = 1024\n",
    "        self._layers = []\n",
    "        self._msg = ''\n",
    "        self._num_classes = 2\n",
    "        \n",
    "        self._vprint = sys.stdout.write if self._verbose else lambda *a, **k: None\n",
    "        \n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self._gen_params_dict())\n",
    "        \n",
    "    def _gen_params_dict(self):\n",
    "        \"\"\"Generates a dictionary of the arguments, the parameters that make this network.\"\"\"\n",
    "        d = self.__dict__\n",
    "        return {key : d[key] for key in d.keys() if key[0] != '_' if 'tensorflow' not in str(type(d[key]))}\n",
    "        \n",
    "    def _save_params(self):\n",
    "        \"\"\"Saves the parameters of the network as a dictionary in a .npz file .\n",
    "           path = log_dir/name/params/Splits.npz\n",
    "         \"\"\"\n",
    "        self._msg += '\\rsaving network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        direc = self.log_dir + self.name + '/params/'\n",
    "\n",
    "        if not os.path.exists(direc):\n",
    "            self._msg += '- creating new directory'\n",
    "            self._vprint(self._msg)\n",
    "            os.makedirs(direc)\n",
    "            \n",
    "        np.savez(direc + self.__class__.__name__, self._gen_params_dict())\n",
    "        self._msg += ' - params saved'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old network, but keep the current network name.\"\"\"\n",
    "    \n",
    "        self._msg = '\\rloading network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        a = np.load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        self._msg += ' - setting params'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        name = self.name\n",
    "        params = d['data1arr_0'][()]\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "        self.name = name\n",
    "        \n",
    "        self._msg = '\\rparams loaded'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def create_graph(self):\n",
    "\n",
    "        self._save_params()\n",
    "        self._msg = '\\rcreating network graph '; self._vprint(self._msg)\n",
    "\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.is_training = tf.placeholder(dtype = tf.bool, shape = [], name = 'is_training')\n",
    "        \n",
    "        with tf.variable_scope('keep_probs'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "            self.downsample_keep_prob = tf.placeholder(self.dtype, name = 'downsample_keep_prob')\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('sample'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1, self._num_freq_channels, 1], name = 'X')\n",
    "            self.X = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "            \n",
    "        trainable = lambda shape, name : tf.get_variable(name = name,\n",
    "                                                         dtype = self.dtype,\n",
    "                                                         shape = shape,\n",
    "                                                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "        for i in range(self.num_downsamples):\n",
    "            self._msg += ' {}'.format(i); self._vprint(self._msg)\n",
    "            \n",
    "            with tf.variable_scope('conv_layer_{}'.format(i)):\n",
    "\n",
    "                layer = self.X if i == 0 else self._layers[-1]\n",
    "                \n",
    "                # filter shape:\n",
    "                fh = 1 # filter height = 1 for 1D convolution\n",
    "                fw = 3 # filter width\n",
    "                fic = 2**(i) # num in channels = number of incoming filters\n",
    "                foc = 2**(i+1) # num out channels = number of outgoing filters\n",
    "                filters = trainable([fh, fw, fic, foc], 'filters')\n",
    "                \n",
    "                # stride shape\n",
    "                sN = 1 # batch = 1 (why anything but 1 ?)\n",
    "                sH = 1 # height of stride = 1 for 1D conv\n",
    "                sW = 1 # width of stride = downsampling factor = 1 for no downsampling or > 1 for downsampling\n",
    "                sC = 1 # depth = number of channels the stride walks over = 1 (why anything but 1 ?)\n",
    "                strides_no_downsample = [sN, sH, sW, sC]\n",
    "                layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "                \n",
    "                # shape of biases = [num outgoing filters]\n",
    "                biases = trainable([foc], 'biases')\n",
    "                layer = tf.nn.bias_add(layer, biases)\n",
    "                layer = tf.nn.leaky_relu(layer)\n",
    "                layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "                \n",
    "                # downsample\n",
    "                with tf.variable_scope('downsample'):\n",
    "                    fw = 5\n",
    "                    filters = trainable([fh, fw, foc, foc], 'filters')\n",
    "\n",
    "                    sW = 2\n",
    "                    strides_no_downsample = [sN, sH, sW, sC]\n",
    "                    layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "\n",
    "                    # shape of biases = [num outgoing filters]\n",
    "                    biases = trainable([foc], 'biases')\n",
    "                    layer = tf.nn.bias_add(layer, biases)\n",
    "                    layer = tf.nn.leaky_relu(layer)\n",
    "                    layer = tf.nn.dropout(layer, self.downsample_keep_prob)\n",
    "                    layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "\n",
    "                self._layers.append(layer)\n",
    "                    \n",
    "\n",
    "        self.layer_nodes = 2**np.arange(0,10)[:self.num_downsamples][::-1]\n",
    "        for j in range(len(self.layer_nodes)):\n",
    "            \n",
    "            self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "            \n",
    "            with tf.variable_scope('fc_layer_{}'.format(j)):\n",
    "                layer = tf.contrib.layers.fully_connected(self._layers[-1], self.layer_nodes[j])\n",
    "                layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "                self._layers.append(layer)\n",
    "        self._msg += ' '\n",
    "        with tf.variable_scope('labels'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.labels = tf.placeholder(dtype = self.dtype, shape = [None, self._num_classes], name = 'labels')\n",
    "            self.true_cls = tf.argmax(self.labels, axis = 1)\n",
    "            \n",
    "        with tf.variable_scope('logits'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self._logits = tf.contrib.layers.fully_connected(self._layers[-1], self._num_classes, activation_fn = None)\n",
    "\n",
    "        with tf.variable_scope('predictions'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.predictions = tf.nn.softmax(self._logits)\n",
    "            \n",
    "            self.pred_cls = tf.argmax(self.predictions, axis = 1)\n",
    "\n",
    "            self.correct_prediction = tf.equal(self.pred_cls, self.true_cls)\n",
    "            \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, self.dtype))\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('costs'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self._cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.labels, logits = self._logits)\n",
    " \n",
    "            self.cost = tf.reduce_mean(self._cross_entropy)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.adam_inital_learning_rate, epsilon=self.adam_epsilon).minimize(self.cost)\n",
    "            \n",
    "        with tf.variable_scope('logging'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            tf.summary.scalar(name = 'cost', tensor = self.cost)\n",
    "            tf.summary.scalar(name = 'accuracy', tensor = self.accuracy)\n",
    "\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            \n",
    "        self._msg += ' done'\n",
    "        self._vprint(self._msg)\n",
    "        self._msg = ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
