{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos_Neg_NN\n",
    "\n",
    " - To determine the sign (positive or negative) of the cable delay only. Mostly practice to see if I can make a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- an incoming sample (1x1024) will have a label of either positive or negative\n",
    " - so, two classes, label of type [positive, negative]\n",
    "  - ex: positive = [1, 0]\n",
    "  - ex: negative = [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pos_Neg_NN(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 name, \n",
    "                 num_downsamples,\n",
    "                 log_dir = 'logs/',\n",
    "                 dtype = tf.float32,\n",
    "                 adam_inital_learning_rate = 0.0001,\n",
    "                 adam_epsilon = 1e-8,\n",
    "                 verbose = False):\n",
    "    \n",
    "        self.name = name\n",
    "        self.num_downsamples = num_downsamples\n",
    "        self.log_dir = log_dir\n",
    "        self.dtype = dtype\n",
    "        self.adam_inital_learning_rate = adam_inital_learning_rate\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        self._num_freq_channels = 1024\n",
    "        self._layers = []\n",
    "        self._msg = ''\n",
    "        self._num_classes = 2\n",
    "        \n",
    "        self._vprint = sys.stdout.write if self._verbose else lambda *a, **k: None\n",
    "        \n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self._gen_params_dict())\n",
    "        \n",
    "    def _gen_params_dict(self):\n",
    "        \"\"\"Generates a dictionary of the arguments, the parameters that make this network.\"\"\"\n",
    "        d = self.__dict__\n",
    "        return {key : d[key] for key in d.keys() if key[0] != '_' if 'tensorflow' not in str(type(d[key]))}\n",
    "        \n",
    "    def _save_params(self):\n",
    "        \"\"\"Saves the parameters of the network as a dictionary in a .npz file .\n",
    "           path = log_dir/name/params/Splits.npz\n",
    "         \"\"\"\n",
    "        self._msg += '\\rsaving network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        direc = self.log_dir + self.name + '/params/'\n",
    "\n",
    "        if not os.path.exists(direc):\n",
    "            self._msg += '- creating new directory'\n",
    "            self._vprint(self._msg)\n",
    "            os.makedirs(direc)\n",
    "            \n",
    "        np.savez(direc + self.__class__.__name__, self._gen_params_dict())\n",
    "        self._msg += ' - params saved'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old network, but keep the current network name.\"\"\"\n",
    "    \n",
    "        self._msg = '\\rloading network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        a = np.load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        self._msg += ' - setting params'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        name = self.name\n",
    "        params = d['data1arr_0'][()]\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "        self.name = name\n",
    "        \n",
    "        self._msg = '\\rparams loaded'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def create_graph(self):\n",
    "\n",
    "        self._save_params()\n",
    "        self._msg = '\\rcreating network graph '; self._vprint(self._msg)\n",
    "\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.is_training = tf.placeholder(dtype = tf.bool, shape = [], name = 'is_training')\n",
    "        \n",
    "        with tf.variable_scope('keep_probs'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "            self.downsample_keep_prob = tf.placeholder(self.dtype, name = 'downsample_keep_prob')\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('sample'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1, self._num_freq_channels, 1], name = 'X')\n",
    "            self.X = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "            \n",
    "        trainable = lambda shape, name : tf.get_variable(name = name,\n",
    "                                                         dtype = self.dtype,\n",
    "                                                         shape = shape,\n",
    "                                                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "        for i in range(self.num_downsamples):\n",
    "            self._msg += ' {}'.format(i); self._vprint(self._msg)\n",
    "            \n",
    "            with tf.variable_scope('conv_layer_{}'.format(i)):\n",
    "\n",
    "                layer = self.X if i == 0 else self._layers[-1]\n",
    "                \n",
    "                # filter shape:\n",
    "                fh = 1 # filter height = 1 for 1D convolution\n",
    "                fw = 3 # filter width\n",
    "                fic = 2**(i) # num in channels = number of incoming filters\n",
    "                foc = 2**(i+1) # num out channels = number of outgoing filters\n",
    "                filters = trainable([fh, fw, fic, foc], 'filters')\n",
    "                \n",
    "                # stride shape\n",
    "                sN = 1 # batch = 1 (why anything but 1 ?)\n",
    "                sH = 1 # height of stride = 1 for 1D conv\n",
    "                sW = 1 # width of stride = downsampling factor = 1 for no downsampling or > 1 for downsampling\n",
    "                sC = 1 # depth = number of channels the stride walks over = 1 (why anything but 1 ?)\n",
    "                strides_no_downsample = [sN, sH, sW, sC]\n",
    "                layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "                \n",
    "                # shape of biases = [num outgoing filters]\n",
    "                biases = trainable([foc], 'biases')\n",
    "                layer = tf.nn.bias_add(layer, biases)\n",
    "                layer = tf.nn.leaky_relu(layer)\n",
    "                layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "                \n",
    "                # downsample\n",
    "                with tf.variable_scope('downsample'):\n",
    "                    fw = 5\n",
    "                    filters = trainable([fh, fw, foc, foc], 'filters')\n",
    "\n",
    "                    sW = 2\n",
    "                    strides_no_downsample = [sN, sH, sW, sC]\n",
    "                    layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "\n",
    "                    # shape of biases = [num outgoing filters]\n",
    "                    biases = trainable([foc], 'biases')\n",
    "                    layer = tf.nn.bias_add(layer, biases)\n",
    "                    layer = tf.nn.leaky_relu(layer)\n",
    "                    layer = tf.nn.dropout(layer, self.downsample_keep_prob)\n",
    "                    layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "\n",
    "                self._layers.append(layer)\n",
    "                    \n",
    "\n",
    "        self.layer_nodes = 2**np.arange(0,10)[:self.num_downsamples][::-1]\n",
    "        for j in range(len(self.layer_nodes)):\n",
    "            \n",
    "            self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "            \n",
    "            with tf.variable_scope('fc_layer_{}'.format(j)):\n",
    "                layer = tf.contrib.layers.fully_connected(self._layers[-1], self.layer_nodes[j])\n",
    "                layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "                self._layers.append(layer)\n",
    "            \n",
    "            \n",
    "        self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "\n",
    "        with tf.variable_scope('final_layer'):\n",
    "            layer = tf.contrib.layers.fully_connected(tf.layers.flatten(self._layers[-1]), 2)\n",
    "            layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "            self._layers.append(layer)                \n",
    "        \n",
    "                \n",
    "        self._msg += ' '\n",
    "        with tf.variable_scope('labels'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.labels = tf.placeholder(dtype = self.dtype, shape = [None, self._num_classes], name = 'labels')\n",
    "            self.true_cls = tf.argmax(self.labels, axis = 1)\n",
    "            \n",
    "        with tf.variable_scope('logits'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self._logits = tf.contrib.layers.fully_connected(self._layers[-1], self._num_classes, activation_fn = None)\n",
    "\n",
    "        with tf.variable_scope('predictions'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.predictions = tf.nn.softmax(self._logits)\n",
    "            \n",
    "            self.pred_cls = tf.argmax(self.predictions, axis = 1)\n",
    "\n",
    "            self.correct_prediction = tf.equal(self.pred_cls, self.true_cls)\n",
    "            \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, self.dtype))\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('costs'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self._cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.labels, logits = self._logits)\n",
    " \n",
    "            self.cost = tf.reduce_mean(self._cross_entropy)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.adam_inital_learning_rate, epsilon=self.adam_epsilon).minimize(self.cost)\n",
    "            \n",
    "        with tf.variable_scope('logging'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            tf.summary.scalar(name = 'cost', tensor = self.cost)\n",
    "            tf.summary.scalar(name = 'accuracy', tensor = self.accuracy)\n",
    "\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            \n",
    "        self._msg += ' done'\n",
    "        self._vprint(self._msg)\n",
    "        self._msg = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pos_Neg_NN_2(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 name, \n",
    "                 num_downsamples,\n",
    "                 log_dir = 'logs/',\n",
    "                 dtype = tf.float32,\n",
    "                 adam_inital_learning_rate = 0.0001,\n",
    "                 adam_epsilon = 1e-8,\n",
    "                 verbose = False):\n",
    "    \n",
    "        self.name = name\n",
    "        self.num_downsamples = num_downsamples\n",
    "        self.log_dir = log_dir\n",
    "        self.dtype = dtype\n",
    "        self.adam_inital_learning_rate = adam_inital_learning_rate\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        self._num_freq_channels = 1024\n",
    "        self._layers = []\n",
    "        self._msg = ''\n",
    "        self._num_classes = 2\n",
    "        \n",
    "        self._vprint = sys.stdout.write if self._verbose else lambda *a, **k: None\n",
    "        \n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self._gen_params_dict())\n",
    "        \n",
    "    def _gen_params_dict(self):\n",
    "        \"\"\"Generates a dictionary of the arguments, the parameters that make this network.\"\"\"\n",
    "        d = self.__dict__\n",
    "        return {key : d[key] for key in d.keys() if key[0] != '_' if 'tensorflow' not in str(type(d[key]))}\n",
    "        \n",
    "    def _save_params(self):\n",
    "        \"\"\"Saves the parameters of the network as a dictionary in a .npz file .\n",
    "           path = log_dir/name/params/Splits.npz\n",
    "         \"\"\"\n",
    "        self._msg += '\\rsaving network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        direc = self.log_dir + self.name + '/params/'\n",
    "\n",
    "        if not os.path.exists(direc):\n",
    "            self._msg += '- creating new directory'\n",
    "            self._vprint(self._msg)\n",
    "            os.makedirs(direc)\n",
    "            \n",
    "        np.savez(direc + self.__class__.__name__, self._gen_params_dict())\n",
    "        self._msg += ' - params saved'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old network, but keep the current network name.\"\"\"\n",
    "    \n",
    "        self._msg = '\\rloading network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        a = np.load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        self._msg += ' - setting params'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        name = self.name\n",
    "        params = d['data1arr_0'][()]\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "        self.name = name\n",
    "        \n",
    "        self._msg = '\\rparams loaded'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def create_graph(self):\n",
    "\n",
    "        self._save_params()\n",
    "        self._msg = '\\rcreating network graph '; self._vprint(self._msg)\n",
    "\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.is_training = tf.placeholder(dtype = tf.bool, shape = [], name = 'is_training')\n",
    "        \n",
    "        with tf.variable_scope('keep_probs'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "            self.downsample_keep_prob = tf.placeholder(self.dtype, name = 'downsample_keep_prob')\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('sample'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1, self._num_freq_channels, 1], name = 'X')\n",
    "            self.X = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "            \n",
    "        trainable = lambda shape, name : tf.get_variable(name = name,\n",
    "                                                         dtype = self.dtype,\n",
    "                                                         shape = shape,\n",
    "                                                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "        for i in range(self.num_downsamples):\n",
    "            self._msg += ' {}'.format(i); self._vprint(self._msg)\n",
    "            \n",
    "            with tf.variable_scope('conv_layer_{}'.format(i)):\n",
    "\n",
    "                layer = self.X if i == 0 else self._layers[-1]\n",
    "                \n",
    "                # filter shape:\n",
    "                fh = 1 # filter height = 1 for 1D convolution\n",
    "                fw = 3 # filter width\n",
    "                fic = 2**(i) # num in channels = number of incoming filters\n",
    "                foc = 2**(i+1) # num out channels = number of outgoing filters\n",
    "                filters = trainable([fh, fw, fic, foc], 'filters')\n",
    "                \n",
    "                # stride shape\n",
    "                sN = 1 # batch = 1 (why anything but 1 ?)\n",
    "                sH = 1 # height of stride = 1 for 1D conv\n",
    "                sW = 1 # width of stride = downsampling factor = 1 for no downsampling or > 1 for downsampling\n",
    "                sC = 1 # depth = number of channels the stride walks over = 1 (why anything but 1 ?)\n",
    "                strides_no_downsample = [sN, sH, sW, sC]\n",
    "                layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "                \n",
    "                # shape of biases = [num outgoing filters]\n",
    "                biases = trainable([foc], 'biases')\n",
    "                layer = tf.nn.bias_add(layer, biases)\n",
    "                layer = tf.nn.leaky_relu(layer)\n",
    "                layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "                \n",
    "                # downsample\n",
    "                with tf.variable_scope('downsample'):\n",
    "                    fw = 5\n",
    "                    filters = trainable([fh, fw, foc, foc], 'filters')\n",
    "\n",
    "                    sW = 2\n",
    "                    strides_no_downsample = [sN, sH, sW, sC]\n",
    "                    layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "\n",
    "                    # shape of biases = [num outgoing filters]\n",
    "                    biases = trainable([foc], 'biases')\n",
    "                    layer = tf.nn.bias_add(layer, biases)\n",
    "                    layer = tf.nn.leaky_relu(layer)\n",
    "                    layer = tf.nn.dropout(layer, self.downsample_keep_prob)\n",
    "                    layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "\n",
    "                self._layers.append(layer)\n",
    "                    \n",
    "\n",
    "#         self.layer_nodes = 2**np.arange(0,10)[:self.num_downsamples][::-1]\n",
    "#         for j in range(len(self.layer_nodes)):\n",
    "            \n",
    "#             self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "            \n",
    "#             with tf.variable_scope('fc_layer_{}'.format(j)):\n",
    "#                 layer = tf.contrib.layers.fully_connected(self._layers[-1], self.layer_nodes[j])\n",
    "#                 layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "#                 self._layers.append(layer)\n",
    "            \n",
    "            \n",
    "#         self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "\n",
    "#         with tf.variable_scope('final_layer'):\n",
    "#             layer = tf.contrib.layers.fully_connected(tf.layers.flatten(self._layers[-1]), 2)\n",
    "#             layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "#             self._layers.append(layer)                \n",
    "        \n",
    "                \n",
    "        self._msg += ' '\n",
    "        with tf.variable_scope('labels'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.labels = tf.placeholder(dtype = self.dtype, shape = [None, self._num_classes], name = 'labels')\n",
    "            self.true_cls = tf.argmax(self.labels, axis = 1)\n",
    "            \n",
    "        with tf.variable_scope('logits'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self._logits = tf.contrib.layers.fully_connected(tf.layers.flatten(self._layers[-1]), self._num_classes, activation_fn = None)\n",
    "\n",
    "        with tf.variable_scope('predictions'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.predictions = tf.nn.softmax(self._logits)\n",
    "            \n",
    "            self.pred_cls = tf.argmax(self.predictions, axis = 1)\n",
    "\n",
    "            self.correct_prediction = tf.equal(self.pred_cls, self.true_cls)\n",
    "            \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, self.dtype))\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('costs'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self._cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.labels, logits = self._logits)\n",
    " \n",
    "            self.cost = tf.reduce_mean(self._cross_entropy)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.adam_inital_learning_rate, epsilon=self.adam_epsilon).minimize(self.cost)\n",
    "            \n",
    "        with tf.variable_scope('logging'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            tf.summary.scalar(name = 'cost', tensor = self.cost)\n",
    "            tf.summary.scalar(name = 'accuracy', tensor = self.accuracy)\n",
    "\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            \n",
    "        self._msg += ' done'\n",
    "        self._vprint(self._msg)\n",
    "        self._msg = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../modules'))\n",
    "import notebook_loading\n",
    "\n",
    "from Data import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the redundant baselines and their gains and data from miriad and calfits files\n",
    "red_bls, gains, uvd = load_relevant_data('../zen_data/zen.2458098.58037.xx.HH.uv','../zen_data/zen.2458098.58037.xx.HH.uv.abs.calfits')\n",
    "\n",
    "# seperate trining and testing redundant baselines \n",
    "# if we have not already done this, load them from disk\n",
    "training_redundant_baselines_dict, testing_redundant_baselines_dict = get_or_gen_test_train_red_bls_dicts(red_bls, gains.keys())\n",
    "\n",
    "# seperate the visiblites\n",
    "training_baselines_data = get_seps_data(training_redundant_baselines_dict, uvd)\n",
    "testing_baselines_data = get_seps_data(testing_redundant_baselines_dict, uvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "class PN_Trainer(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 network = None,\n",
    "                 abs_min_max_delay = 0.040,\n",
    "                 num_flatnesses = 100,\n",
    "                 num_epochs = 100,\n",
    "                 batch_size = 32,\n",
    "                 log_dir = 'logs/',\n",
    "                 model_save_interval = 25,\n",
    "                 pretrained_model_path = None,\n",
    "                 sample_keep_prob = 0.80,\n",
    "                 downsample_keep_prob = 0.50,\n",
    "                 verbose = False):\n",
    "        \n",
    "        self.network = network\n",
    "        self.abs_min_max_delay = abs_min_max_delay\n",
    "        self.num_flatnesses = num_flatnesses\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.log_dir = log_dir\n",
    "        self.model_save_interval = model_save_interval\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.sample_keep_prob = sample_keep_prob\n",
    "        self.downsample_keep_prob = downsample_keep_prob\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        self._msg = ''\n",
    "        self._vprint = sys.stdout.write if self._verbose else lambda *a, **k: None\n",
    "        \n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self._gen_params_dict())\n",
    "        \n",
    "    def _gen_params_dict(self):\n",
    "        \"\"\"Generates a dictionary of the arguments, the parameters that make this network.\"\"\"\n",
    "        d = self.__dict__\n",
    "        return {key : d[key] for key in d.keys() if key != 'network' if key[0] != '_' if 'tensorflow' not in str(type(d[key]))}\n",
    "        \n",
    "    def _save_params(self):\n",
    "        \"\"\"Saves the parameters of the trainer as a dictionary in a .npz file .\n",
    "           path = log_dir/name/params/Train.npz\n",
    "         \"\"\"\n",
    "        self._msg += '\\rsaving trainer parameters'\n",
    "        self._vprint(self._msg)\n",
    "        direc = self.log_dir + self.network.name + '/params/'\n",
    "\n",
    "        if not os.path.exists(direc):\n",
    "            self._msg += ' - creating new directory'\n",
    "            self._vprint(self._msg)\n",
    "            os.makedirs(direc)\n",
    "            \n",
    "        np.savez(direc + self.__class__.__name__, self._gen_params_dict())\n",
    "        self._msg += ' - params saved'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old trainer.\"\"\"\n",
    "    \n",
    "        self._msg = '\\rloading trainer parameters'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        a = np.load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        self._msg += ' - setting params'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        params = d['data1arr_0'][()]\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "        \n",
    "        self._msg = '\\rparams loaded'\n",
    "        self._vprint(self._msg)\n",
    "\n",
    "\n",
    "\n",
    "    def add_data(self,train_info, test_info, gains):\n",
    "        self._msg = '\\radding data';self._vprint(self._msg)\n",
    "\n",
    "        \n",
    "        self._train_batcher = PN_Data_Creator(self.num_flatnesses,\n",
    "                                           train_info[0],\n",
    "                                           train_info[1],\n",
    "                                           gains,\n",
    "                                           self.abs_min_max_delay)\n",
    "        self._train_batcher.gen_data()\n",
    "\n",
    "\n",
    "        self._test_batcher = PN_Data_Creator(self.num_flatnesses,\n",
    "                                          test_info[0],\n",
    "                                          test_info[1],\n",
    "                                          gains,\n",
    "                                          self.abs_min_max_delay)\n",
    "        \n",
    "        self._test_batcher.gen_data()\n",
    "        self._msg += ' - trainer ready';self._vprint(self._msg)\n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        self._save_params()\n",
    "        self.costs = []\n",
    "        self.accuracies = []\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.network.create_graph()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as session:\n",
    "            \n",
    "            if self.pretrained_model_path == None:\n",
    "                session.run(tf.global_variables_initializer())\n",
    "                \n",
    "            else:\n",
    "                saver.restore(session, self.pretrained_model_path)\n",
    "                \n",
    "            \n",
    "            archive_loc = self.log_dir + self.network.name\n",
    "            training_writer = tf.summary.FileWriter(archive_loc + '/training', session.graph)\n",
    "            testing_writer = tf.summary.FileWriter(archive_loc + '/testing', session.graph)\n",
    "            self.model_save_location = archive_loc + '/trained_model.ckpt'   \n",
    "            \n",
    "            \n",
    "            self._msg = '\\rtraining';self._vprint(self._msg)\n",
    "\n",
    "            try:\n",
    "                for epoch in range(self.num_epochs):\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    training_inputs, training_labels = self._train_batcher.get_data(); self._train_batcher.gen_data()\n",
    "                    testing_inputs, testing_labels = self._test_batcher.get_data(); self._test_batcher.gen_data()  \n",
    "\n",
    "                    training_labels = np.asarray(training_labels)\n",
    "                    testing_labels = np.asarray(testing_labels)\n",
    "                    \n",
    "                    # if the division here has a remainde some values are just truncated\n",
    "                    batch_size = self.batch_size\n",
    "                    num_entries = self.num_flatnesses * 60.\n",
    "\n",
    "                    for j in range(int(num_entries/batch_size)):\n",
    "                        self._msg = '\\rEpoch - {:5.0f}/{:5.0f} - batch: {:4.0f}/{:4.0f}'.format(epoch + 1,self.num_epochs, j + 1, int(num_entries/batch_size))\n",
    "                        if epoch != 0:\n",
    "                            self._msg += ' - (Training, Testing) - '.format(epoch)\n",
    "                            self._msg += \" costs: ({:0.4f}, {:0.4f})\".format(training_cost, testing_cost)\n",
    "                            self._msg += \" accss: ({:2.2f}, {:2.2f})\".format(training_acc, testing_acc)\n",
    "                        self._vprint(self._msg); \n",
    "\n",
    "                        training_inputs_batch = training_inputs[j*batch_size:(j + 1)*batch_size].reshape(-1,1,1024,1)\n",
    "                        training_labels_batch = training_labels[j*batch_size:(j + 1)*batch_size].reshape(-1,2)\n",
    "\n",
    "                        feed_dict = {self.network.X: training_inputs_batch,\n",
    "                                     self.network.labels: training_labels_batch,\n",
    "                                     self.network.sample_keep_prob : self.sample_keep_prob,\n",
    "                                     self.network.downsample_keep_prob : self.downsample_keep_prob,\n",
    "                                     self.network.is_training : True}\n",
    "\n",
    "                        session.run([self.network.optimizer], feed_dict = feed_dict) \n",
    "                            \n",
    "                    train_feed_dict = {self.network.X: training_inputs.reshape(-1,1,1024,1),\n",
    "                                      self.network.labels: training_labels.reshape(-1,2),\n",
    "                                      self.network.sample_keep_prob : 1.,\n",
    "                                      self.network.downsample_keep_prob : 1.,\n",
    "                                      self.network.is_training : False}\n",
    "\n",
    "\n",
    "                    training_cost, training_acc, training_summary = session.run([self.network.cost,\n",
    "                                                                                 self.network.accuracy,\n",
    "                                                                                 self.network.summary],\n",
    "                                                                                 feed_dict = train_feed_dict) \n",
    "\n",
    "                    training_writer.add_summary(training_summary, epoch)\n",
    "                    training_writer.flush()  \n",
    "                \n",
    "                    \n",
    "                    test_feed_dict = {self.network.X: testing_inputs.reshape(-1,1,1024,1),\n",
    "                                      self.network.labels: testing_labels.reshape(-1,2),\n",
    "                                      self.network.sample_keep_prob : 1.,\n",
    "                                      self.network.downsample_keep_prob : 1.,\n",
    "                                      self.network.is_training : False} \n",
    "\n",
    "                    testing_cost, testing_acc, testing_summary = session.run([self.network.cost,\n",
    "                                                                              self.network.accuracy,\n",
    "                                                                              self.network.summary],\n",
    "                                                                              feed_dict = test_feed_dict)\n",
    "\n",
    "                    testing_writer.add_summary(testing_summary, epoch)\n",
    "                    testing_writer.flush()\n",
    "                    \n",
    "                    self.costs.append((training_cost, testing_cost))\n",
    "                    self.accuracies.append((training_acc, testing_acc))\n",
    "                    \n",
    "                    if (epoch + 1) % self.model_save_interval == 0:\n",
    "                        saver.save(session, self.model_save_location, epoch + 1)\n",
    "                \n",
    "                self.msg = ''\n",
    "            except KeyboardInterrupt:\n",
    "                self._msg = ' TRAINING INTERUPPTED' # this never prints I dont know why\n",
    "                pass\n",
    "\n",
    "            self._msg += '\\rtraining ended'; self._vprint(self._msg)\n",
    "            \n",
    "            training_writer.close()\n",
    "            testing_writer.close()\n",
    "\n",
    "\n",
    "        session.close()\n",
    "        self._msg += ' - session closed'; self._vprint(self._msg)\n",
    "        self._msg = ''\n",
    "        \n",
    "    def plot(self, mode = 'all', figsize = (8,6) ):\n",
    "        \n",
    "        xvals = np.arange(len(self.costs))\n",
    "        \n",
    "        \n",
    "        if mode == 'all':\n",
    "            \n",
    "            yvals = [self.costs, self.accuracies]\n",
    "            metric_names = ['costs', 'accuracies']\n",
    "            \n",
    "        fig, axes = plt.subplots(len(yvals), 1, figsize = figsize, dpi = 144)\n",
    "        \n",
    "        for i, ax in enumerate(axes.reshape(-1)):\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.set_title('{}'.format(self.network.name))\n",
    "\n",
    "            ax.plot(xvals, yvals[i], lw = 0.5)\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel(metric_names[i])\n",
    "            \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def save_results(self):\n",
    "        \n",
    "        results = {'MISGs': self.MISGs, 'MSEs': self.MSEs, 'PWTs': self.PWTs}\n",
    "        direc = self.log_dir + self.network.name + '/results/'\n",
    "        \"\"\"Not safe - will overwrite existing file.\"\"\"\n",
    "        if not os.path.exists(direc):\n",
    "            os.makedirs(direc)\n",
    "        np.savez(direc + 'results', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_A = Pos_Neg_NN('network_A', 2, adam_inital_learning_rate = 0.001)\n",
    "network_A.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abs_min_max_delay': 0.04,\n",
      " 'batch_size': 2,\n",
      " 'downsample_keep_prob': 0.9,\n",
      " 'log_dir': 'logs/',\n",
      " 'model_save_interval': 25,\n",
      " 'num_epochs': 1000,\n",
      " 'num_flatnesses': 1,\n",
      " 'pretrained_model_path': None,\n",
      " 'sample_keep_prob': 0.8}\n"
     ]
    }
   ],
   "source": [
    "trainer = PN_Trainer(network = network_A, num_flatnesses = 1, downsample_keep_prob = 0.9, batch_size=2, verbose=True, num_epochs = 1000)\n",
    "trainer.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding data - trainer ready"
     ]
    }
   ],
   "source": [
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "                 (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "                 gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adam_epsilon': 1e-08,\n",
      " 'adam_inital_learning_rate': 0.0001,\n",
      " 'log_dir': 'logs/',\n",
      " 'name': 'network_B',\n",
      " 'num_downsamples': 2}\n"
     ]
    }
   ],
   "source": [
    "network_B = Pos_Neg_NN('network_B', 2, adam_inital_learning_rate = 0.0001)\n",
    "network_B.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abs_min_max_delay': 0.04,\n",
      " 'batch_size': 2,\n",
      " 'downsample_keep_prob': 0.9,\n",
      " 'log_dir': 'logs/',\n",
      " 'model_save_interval': 25,\n",
      " 'num_epochs': 2000,\n",
      " 'num_flatnesses': 1,\n",
      " 'pretrained_model_path': None,\n",
      " 'sample_keep_prob': 0.8}\n"
     ]
    }
   ],
   "source": [
    "trainer = PN_Trainer(network = network_B, num_flatnesses = 1, downsample_keep_prob = 0.9, batch_size=2, verbose=True, num_epochs = 2000)\n",
    "trainer.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding data - trainer ready"
     ]
    }
   ],
   "source": [
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "                 (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "                 gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logs/network_B/trained_model.ckpt-2000\n",
      "0.0265673063898\n",
      "positive\n",
      "probs: [0.71064574 0.28935426]\n"
     ]
    }
   ],
   "source": [
    "tau = np.random.uniform(low = -0.040, high = 0.040)\n",
    "row = np.angle(np.exp(-2j*np.pi*(np.arange(1024)*tau + np.random.uniform())))\n",
    "angle_tx  = lambda x: (np.asarray(x) + np.pi) / (2. * np.pi)\n",
    "row = angle_tx(row)\n",
    "\n",
    "network = Pos_Neg_NN('network_B_eval', 2)\n",
    "network.create_graph()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as session:\n",
    "    saver.restore(session, 'logs/network_B/trained_model.ckpt-2000')\n",
    "    \n",
    "    pred_cls, cls_probs = session.run([network.pred_cls, network.predictions],\n",
    "                                      feed_dict = {network.X : row.reshape(-1, 1, 1024, 1),\n",
    "                                                   network.sample_keep_prob : 1.,\n",
    "                                                   network.downsample_keep_prob : 1.,\n",
    "                                                   network.is_training : False})\n",
    "\n",
    "# [POSITIVE,NEGATIVE] 0 = POS, 1 = NEG\n",
    "print(tau)\n",
    "if pred_cls[0] == 0:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')\n",
    "print('probs: {}'.format((cls_probs[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adam_epsilon': 1e-08,\n",
      " 'adam_inital_learning_rate': 0.0001,\n",
      " 'log_dir': 'logs/',\n",
      " 'name': 'network_C',\n",
      " 'num_downsamples': 2}\n"
     ]
    }
   ],
   "source": [
    "network_C = Pos_Neg_NN('network_C', 2, adam_inital_learning_rate = 0.0001)\n",
    "network_C.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abs_min_max_delay': 0.04,\n",
      " 'batch_size': 2,\n",
      " 'downsample_keep_prob': 0.9,\n",
      " 'log_dir': 'logs/',\n",
      " 'model_save_interval': 25,\n",
      " 'num_epochs': 1000,\n",
      " 'num_flatnesses': 2,\n",
      " 'pretrained_model_path': None,\n",
      " 'sample_keep_prob': 0.8}\n"
     ]
    }
   ],
   "source": [
    "trainer = PN_Trainer(network = network_C, num_flatnesses = 2, downsample_keep_prob = 0.9, batch_size=2, verbose=True, num_epochs = 1000)\n",
    "trainer.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding data - trainer ready"
     ]
    }
   ],
   "source": [
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "                 (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "                 gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adam_epsilon': 1e-08,\n",
      " 'adam_inital_learning_rate': 0.0001,\n",
      " 'log_dir': 'logs/',\n",
      " 'name': 'network_D',\n",
      " 'num_downsamples': 2}\n"
     ]
    }
   ],
   "source": [
    "network_D = Pos_Neg_NN_2('network_D', 2, adam_inital_learning_rate = 0.0001)\n",
    "network_D.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abs_min_max_delay': 0.04,\n",
      " 'batch_size': 2,\n",
      " 'downsample_keep_prob': 0.9,\n",
      " 'log_dir': 'logs/',\n",
      " 'model_save_interval': 25,\n",
      " 'num_epochs': 1000,\n",
      " 'num_flatnesses': 2,\n",
      " 'pretrained_model_path': None,\n",
      " 'sample_keep_prob': 0.8}\n"
     ]
    }
   ],
   "source": [
    "trainer = PN_Trainer(network = network_D, num_flatnesses = 2, downsample_keep_prob = 0.9, batch_size=2, verbose=True, num_epochs = 1000)\n",
    "trainer.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding data - trainer ready"
     ]
    }
   ],
   "source": [
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "                 (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "                 gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adam_epsilon': 1e-08,\n",
      " 'adam_inital_learning_rate': 0.0001,\n",
      " 'log_dir': 'logs/',\n",
      " 'name': 'network_E',\n",
      " 'num_downsamples': 3}\n"
     ]
    }
   ],
   "source": [
    "network_E = Pos_Neg_NN_2('network_E', 3, adam_inital_learning_rate = 0.0001)\n",
    "network_E.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abs_min_max_delay': 0.04,\n",
      " 'batch_size': 2,\n",
      " 'downsample_keep_prob': 0.9,\n",
      " 'log_dir': 'logs/',\n",
      " 'model_save_interval': 25,\n",
      " 'num_epochs': 1000,\n",
      " 'num_flatnesses': 2,\n",
      " 'pretrained_model_path': None,\n",
      " 'sample_keep_prob': 0.8}\n"
     ]
    }
   ],
   "source": [
    "trainer = PN_Trainer(network = network_E, num_flatnesses = 2, downsample_keep_prob = 0.9, batch_size=2, verbose=True, num_epochs = 1000)\n",
    "trainer.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding data - trainer ready"
     ]
    }
   ],
   "source": [
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "                 (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "                 gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
