{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyuvdata import UVData\n",
    "import hera_cal as hc\n",
    "import random\n",
    "from threading import Thread\n",
    "from glob import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_data(miriad_path, calfits_path):\n",
    "    \"\"\"Loads redundant baselines, gains, and data.\n",
    "    \n",
    "    Arguments:\n",
    "        \n",
    "        miriad_path : string - path to a miriad file for some JD\n",
    "        calfits_path : string - path to a calfits file for the same JD\n",
    "        \n",
    "    Returns:\n",
    "        red_bls, gains, uvd\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # read the data\n",
    "    uvd = UVData()\n",
    "    uvd.read_miriad(miriad_path)\n",
    "\n",
    "    # get the redundancies for that data\n",
    "    aa = hc.utils.get_aa_from_uv(uvd)\n",
    "    info = hc.omni.aa_to_info(aa)\n",
    "    red_bls = np.array(info.get_reds())\n",
    "\n",
    "    # gains for same data \n",
    "    gains, _ = hc.io.load_cal(calfits_path)\n",
    "    \n",
    "    return red_bls, gains, uvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_red_bls(red_bls, gain_keys, min_group_len = 4):\n",
    "    \"\"\"Select all the good antennas from red_bls\n",
    "    \n",
    "    Each baseline group in red_bls will have its bad separations removed.\n",
    "    Groups with less than min_group_len are removed.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        red_bls : list of lists - Each sublist is a group of redundant separations\n",
    "                                  for a unique baseline.\n",
    "        gain_keys : dict - gains.keys() from hc.io.load_cal()\n",
    "        min_group_len: int - Minimum number of separations in a 'good' sublist.\n",
    "                            (Default = 4, so that both training and testing can take two seps)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list of lists - Each sublist is a len >=4 list of separations of good antennas\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def ants_good(sep):\n",
    "        \"\"\"Returns True if both antennas are good.\n",
    "\n",
    "        Because we are using data from firstcal\n",
    "        (is this right? I have trouble rememebring the names\n",
    "        and properties of the different data sources)\n",
    "        we can check for known good or bad antennas by looking to see if the antenna\n",
    "        is represented in gains.keys(). If the antenna is present, its good.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            sep : tuple - antenna indices\n",
    "\n",
    "        Returns :\n",
    "\n",
    "            bool - True if both antennas are in gain_keys\n",
    "        \"\"\"\n",
    "\n",
    "        ants = [a[0] for a in gain_keys]\n",
    "\n",
    "        if sep[0] in ants and sep[1] in ants:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    good_redundant_baselines = []\n",
    "    \n",
    "    for group in red_bls:\n",
    "        \n",
    "        new_group = []\n",
    "        \n",
    "        for sep in group:\n",
    "            \n",
    "            # only retain seps made from good antennas\n",
    "            if ants_good(sep) == True:\n",
    "                new_group.append(sep)\n",
    "                \n",
    "        new_group_len = len(new_group)\n",
    "        \n",
    "        # make sure groups are large enough that both the training set\n",
    "        # and the testing set can take two seps \n",
    "        if new_group_len >= min_group_len:\n",
    "            \n",
    "            # Make sure groups are made from even number of seps\n",
    "            #\n",
    "            # I honestly dont recall why I did this. ¯\\_(ツ)_/¯ \n",
    "#             if new_group_len % 2 != 0:\n",
    "#                 new_group.pop()\n",
    "                \n",
    "            good_redundant_baselines.append(sorted(new_group))\n",
    "            \n",
    "    return good_redundant_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_test_split_red_bls(red_bls, training_percent = 0.80):\n",
    "    \"\"\"Slit a list of redundant baselines into a training set and a testing set. \n",
    "    \n",
    "    Each of the two sets has at least one pair of baselines from every group from red_bls.\n",
    "    However, separations from one set will not appear the other.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        red_bls : list of lists - Each sublist is a group of redundant separations\n",
    "                                  for a unique baseline.\n",
    "                                  Each sublist must have at least 4 separations.\n",
    "        training_percent : float - *Approximate* portion of the separations that will\n",
    "                                   appear in the training set.\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        tuple of dicts - train_red_bls_dict, test_red_bls_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    training_redundant_baselines_dict = {}\n",
    "    testing_redundant_baselines_dict = {}\n",
    "\n",
    "    # make sure that each set has at least 2 seps from each group\n",
    "    #\n",
    "    thinned_groups_dict = {}\n",
    "    for group in red_bls:\n",
    "\n",
    "        # group key is the sep with the lowest antenna indicies\n",
    "        key = sorted(group)[0]\n",
    "        \n",
    "        # pop off seps from group and append them to train or test groups\n",
    "        random.shuffle(group)\n",
    "        training_group = []\n",
    "        training_group.append(group.pop())\n",
    "        training_group.append(group.pop())\n",
    "\n",
    "        testing_group = []\n",
    "        testing_group.append(group.pop())\n",
    "        testing_group.append(group.pop())\n",
    "        \n",
    "        # add the new train & test groups into the dicts\n",
    "        training_redundant_baselines_dict[key] = training_group\n",
    "        testing_redundant_baselines_dict[key] = testing_group\n",
    "        \n",
    "        # if there are still more seps in the group, save them into a dict for later assignment\n",
    "        if len(group) != 0:\n",
    "            thinned_groups_dict[key] = group\n",
    "\n",
    "    # Shuffle and split the group keys into two sets using \n",
    "    #\n",
    "    thinned_dict_keys = thinned_groups_dict.keys()\n",
    "    random.shuffle(thinned_dict_keys)\n",
    "    \n",
    "    \"\"\"Because we are ensuring that each set has some seps from every group,\n",
    "       the ratio of train / test gets reduced a few percent.\n",
    "       This (sort of) accounts for that with an arbitrary shift found by trial and error.\n",
    "       \n",
    "       Without this the a setting of training_percent = 0.80 results in a 65/35 split, not 80/20.\n",
    "       \n",
    "       I assume there is a better way...\"\"\"\n",
    "    t_pct = np.min([0.95, training_percent + 0.15])\n",
    "\n",
    "    \n",
    "    # why did i call this extra?\n",
    "    # these are the keys that each set will extract seps from thinned_groups_dict with\n",
    "    training_red_bls_extra, testing_red_bls_extra = np.split(thinned_dict_keys,\n",
    "                                                             [int(len(thinned_dict_keys)*t_pct)])\n",
    "\n",
    "    # extract seps from thinned_groups_dict and apply to same key in training set\n",
    "    for key in training_red_bls_extra:\n",
    "        key = tuple(key)\n",
    "        group = thinned_groups_dict[key]\n",
    "        training_group = training_redundant_baselines_dict[key]\n",
    "        training_group.extend(group)\n",
    "        training_redundant_baselines_dict[key] = training_group\n",
    "\n",
    "    # extract seps from thinned_groups_dict and apply to same key in testing set\n",
    "    for key in testing_red_bls_extra:\n",
    "        key = tuple(key)\n",
    "        group = thinned_groups_dict[key]\n",
    "        testing_group = testing_redundant_baselines_dict[key]\n",
    "        testing_group.extend(group)\n",
    "        testing_redundant_baselines_dict[key] = testing_group\n",
    "        \n",
    "    return training_redundant_baselines_dict, testing_redundant_baselines_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loadnpz(filename):\n",
    "    \"\"\"Loads up npzs. For dicts do loadnpz(fn)[()]\"\"\"\n",
    "    \n",
    "    a = np.load(filename)\n",
    "    d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "    \n",
    "    return d['data1arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_gen_test_train_red_bls_dicts(red_bls = None,\n",
    "                                        gain_keys = None,\n",
    "                                        training_percent = 0.80):\n",
    "\n",
    "    if len(glob(\"*.npz\")) == 2:\n",
    "        \n",
    "        training_red_bls_dict = _loadnpz('training_redundant_baselines_dict.npz')[()]\n",
    "        testing_red_bls_dict = _loadnpz('testing_redundant_baselines_dict.npz')[()]\n",
    "    else:\n",
    "        \n",
    "        assert type(red_bls) != None, \"Provide a list of redundant baselines\"\n",
    "        assert type(gain_keys) != None, \"Provide a list of gain keys\"\n",
    "        \n",
    "        good_red_bls = get_good_red_bls(red_bls, gain_keys)\n",
    "        training_red_bls_dict, testing_red_bls_dict = _train_test_split_red_bls(good_red_bls,\n",
    "                                                                                training_percent = training_percent)\n",
    "\n",
    "        np.savez('training_redundant_baselines_dict', training_red_bls_dict)\n",
    "        np.savez('testing_redundant_baselines_dict', testing_red_bls_dict)\n",
    "\n",
    "    return training_red_bls_dict, testing_red_bls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seps_data(red_bls_dict, uvd):\n",
    "    \"\"\"Get the data for all the seps in a redundant baselines dictionary.\"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    for key in red_bls_dict.keys():\n",
    "        for sep in red_bls_dict[key]:\n",
    "            data[sep] = uvd.get_data(sep)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block_Data_Creator(object):\n",
    "    \"\"\"Creates data in an alternate thread.\n",
    "    \n",
    "    ## usage:\n",
    "    ## data_maker = data_creator(num_flatnesses=250, mode = 'train')\n",
    "    ## data_maker.gen_data() (before loop)\n",
    "    ## inputs, targets = data_maker.get_data() (start of loop)\n",
    "    ## data_maker.gen_data() (immediately after get_data())\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_flatnesses,\n",
    "                 bl_data = None,\n",
    "                 bl_dict = None,\n",
    "                 gains = None,\n",
    "                 abs_min_max_delay = 0.040):\n",
    "        \n",
    "        \"\"\"\n",
    "        Arguments\n",
    "            num_flatnesses : int - number of flatnesses used to generate data.\n",
    "                                   Number of data samples = 60 * num_flatnesses\n",
    "            bl_data : data source. Output of get_seps_data()\n",
    "            bl_dict : dict - Dictionary of seps with bls as keys. An output of get_or_gen_test_train_red_bls_dicts()\n",
    "            gains : dict - Gains for this data. An output of load_relevant_data()\n",
    "            \n",
    "                                   \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self._num = num_flatnesses\n",
    "                    \n",
    "        self._bl_data = bl_data\n",
    "        self._bl_data_c = None\n",
    "        \n",
    "        self._bl_dict = bl_dict\n",
    "        \n",
    "        self._gains = gains\n",
    "        self._gains_c = None\n",
    "        \n",
    "        self._epoch_batch = []\n",
    "        self._nu = np.arange(1024)\n",
    "        self._tau = abs_min_max_delay\n",
    "        \n",
    "    def _gen_data(self):\n",
    "        \n",
    "        # scaling tools\n",
    "        # the NN likes data in the range (0,1)\n",
    "        angle_tx  = lambda x: (np.asarray(x) + np.pi) / (2. * np.pi)\n",
    "        angle_itx = lambda x: np.asarray(x) * 2. * np.pi - np.pi\n",
    "\n",
    "        delay_tx  = lambda x: (np.array(x) + self._tau) / (2. * self._tau)\n",
    "        delay_itx = lambda x: np.array(x) * 2. * self._tau - self._tau\n",
    "        \n",
    "        targets = np.random.uniform(low = -self._tau, high = self._tau, size = (self._num * 60, 1))\n",
    "        applied_delay = np.exp(-2j * np.pi * (targets * self._nu + np.random.uniform()))\n",
    "\n",
    "\n",
    "\n",
    "        assert type(self._bl_data) != None, \"Provide visibility data\"\n",
    "        assert type(self._bl_dict) != None, \"Provide dict of baselines\"\n",
    "        assert type(self._gains)   != None, \"Provide antenna gains\"\n",
    "\n",
    "        if self._bl_data_c == None:\n",
    "            self._bl_data_c = {key : self._bl_data[key].conjugate() for key in self._bl_data.keys()}\n",
    "\n",
    "        if self._gains_c == None:\n",
    "            self._gains_c = {key : self._gains[key].conjugate() for key in self._gains.keys()}\n",
    "\n",
    "\n",
    "        def _flatness(seps):\n",
    "            \"\"\"Create a flatness from a given pair of seperations, their data & their gains.\"\"\"\n",
    "\n",
    "            a, b = seps[0][0], seps[0][1]\n",
    "            c, d = seps[1][0], seps[1][1]\n",
    "\n",
    "\n",
    "            return self._bl_data[seps[0]]   * self._gains_c[(a,'x')] * self._gains[(b,'x')] * \\\n",
    "                   self._bl_data_c[seps[1]] * self._gains[(c,'x')]   * self._gains_c[(d,'x')]\n",
    "\n",
    "        inputs = []\n",
    "        for _ in range(self._num):\n",
    "\n",
    "            unique_baseline = random.sample(self._bl_dict.keys(), 1)[0]\n",
    "            two_seps = [random.sample(self._bl_dict[unique_baseline], 2)][0]\n",
    "\n",
    "            inputs.append(_flatness(two_seps))\n",
    "\n",
    "        inputs = np.angle(np.array(inputs).reshape(-1,1024) * applied_delay)\n",
    "        \n",
    "        permutation_index = np.random.permutation(np.arange(self._num * 60))\n",
    "        \n",
    "        #0.00025 precision\n",
    "        rounded_targets = np.asarray([np.round(abs(np.round(d * 40,2)/40), 5) for d in targets]).reshape(-1)\n",
    "        classes = np.arange(0,0.04025, 0.00025)\n",
    "        # 0.0005 precision\n",
    "        # x = [np.round(abs(np.round(d * 20,2)/20), 5) for d in dels]\n",
    "\n",
    "        # 0.001 precision\n",
    "        # x = [np.round(abs(np.round(d * 10,2)/10), 5) for d in dels]\n",
    "        \n",
    "        # 0.005 precision - 9 blocks\n",
    "#         rounded_targets = np.asarray([np.round(abs(np.round(d * 2,2)/2), 5) for d in targets]).reshape(-1)\n",
    "        # classes = np.arange(0,0.045, 0.005)\n",
    "\n",
    "        eye = np.eye(len(classes), dtype = int)\n",
    "        classes_labels = {}\n",
    "        for i, key in enumerate(classes):\n",
    "            classes_labels[np.round(key,5)] = eye[i].tolist()\n",
    "            \n",
    "#         print(classes_labels)\n",
    "            \n",
    "        labels = [classes_labels[x] for x in rounded_targets]\n",
    "\n",
    "        self._epoch_batch.append((angle_tx(inputs[permutation_index]), labels))\n",
    "\n",
    "    def gen_data(self):\n",
    "        \"\"\"Starts a new thread and generates data there.\"\"\"\n",
    "        \n",
    "        self._thread = Thread(target = self._gen_data, args=())\n",
    "        self._thread.start()\n",
    "\n",
    "    def get_data(self, timeout = 10):\n",
    "        \"\"\"Retrieves the data from the thread.\n",
    "        \n",
    "        Returns:\n",
    "            \n",
    "            list of shape (num_flatnesses, 60, 1024)\n",
    "             - needs to be reshaped for training\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self._epoch_batch) == 0:\n",
    "            self._thread.join(timeout)\n",
    "            \n",
    "        return self._epoch_batch.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "classes = np.arange(0,0.04025, 0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye = np.eye(len(classes), dtype = int)\n",
    "classes_labels = {}\n",
    "for i, key in enumerate(classes):\n",
    "    classes_labels[np.round(key,5)] = eye[i].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes_labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
