{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# block_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block_NN(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 name, \n",
    "                 num_downsamples,\n",
    "                 log_dir = 'logs/',\n",
    "                 dtype = tf.float32,\n",
    "                 adam_inital_learning_rate = 0.0001,\n",
    "                 adam_epsilon = 1e-8,\n",
    "                 verbose = False):\n",
    "    \n",
    "        self.name = name\n",
    "        self.num_downsamples = num_downsamples\n",
    "        self.log_dir = log_dir\n",
    "        self.dtype = dtype\n",
    "        self.adam_inital_learning_rate = adam_inital_learning_rate\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        self._num_freq_channels = 1024\n",
    "        self._layers = []\n",
    "        self._msg = ''\n",
    "        self._num_classes = 161\n",
    "        \n",
    "        self._vprint = sys.stdout.write if self._verbose else lambda *a, **k: None\n",
    "        \n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self._gen_params_dict())\n",
    "        \n",
    "    def _gen_params_dict(self):\n",
    "        \"\"\"Generates a dictionary of the arguments, the parameters that make this network.\"\"\"\n",
    "        d = self.__dict__\n",
    "        return {key : d[key] for key in d.keys() if key[0] != '_' if 'tensorflow' not in str(type(d[key]))}\n",
    "        \n",
    "    def _save_params(self):\n",
    "        \"\"\"Saves the parameters of the network as a dictionary in a .npz file .\n",
    "           path = log_dir/name/params/Splits.npz\n",
    "         \"\"\"\n",
    "        self._msg += '\\rsaving network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        direc = self.log_dir + self.name + '/params/'\n",
    "\n",
    "        if not os.path.exists(direc):\n",
    "            self._msg += '- creating new directory'\n",
    "            self._vprint(self._msg)\n",
    "            os.makedirs(direc)\n",
    "            \n",
    "        np.savez(direc + self.__class__.__name__, self._gen_params_dict())\n",
    "        self._msg += ' - params saved'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old network, but keep the current network name.\"\"\"\n",
    "    \n",
    "        self._msg = '\\rloading network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        a = np.load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        self._msg += ' - setting params'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        name = self.name\n",
    "        params = d['data1arr_0'][()]\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "        self.name = name\n",
    "        \n",
    "        self._msg = '\\rparams loaded'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def create_graph(self):\n",
    "\n",
    "        self._save_params()\n",
    "        self._msg = '\\rcreating network graph '; self._vprint(self._msg)\n",
    "\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.is_training = tf.placeholder(dtype = tf.bool, shape = [], name = 'is_training')\n",
    "        \n",
    "        with tf.variable_scope('keep_probs'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "            self.downsample_keep_prob = tf.placeholder(self.dtype, name = 'downsample_keep_prob')\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('sample'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1, self._num_freq_channels, 1], name = 'X')\n",
    "            self.X = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "            \n",
    "        trainable = lambda shape, name : tf.get_variable(name = name,\n",
    "                                                         dtype = self.dtype,\n",
    "                                                         shape = shape,\n",
    "                                                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "        for i in range(self.num_downsamples):\n",
    "            self._msg += ' {}'.format(i); self._vprint(self._msg)\n",
    "            \n",
    "            with tf.variable_scope('conv_layer_{}'.format(i)):\n",
    "\n",
    "                layer = self.X if i == 0 else self._layers[-1]\n",
    "                \n",
    "                # filter shape:\n",
    "                fh = 1 # filter height = 1 for 1D convolution\n",
    "                fw = np.max([layer.get_shape().as_list()[2] / 4, 2])# filter width\n",
    "                fic = 4**(i) # num in channels = number of incoming filters\n",
    "                foc = 4**(i+1) # num out channels = number of outgoing filters\n",
    "                filters = trainable([fh, fw, fic, foc], 'filters')\n",
    "                \n",
    "                # stride shape\n",
    "                sN = 1 # batch = 1 (why anything but 1 ?)\n",
    "                sH = 1 # height of stride = 1 for 1D conv\n",
    "                sW = 1 # width of stride = downsampling factor = 1 for no downsampling or > 1 for downsampling\n",
    "                sC = 1 # depth = number of channels the stride walks over = 1 (why anything but 1 ?)\n",
    "                strides_no_downsample = [sN, sH, sW, sC]\n",
    "                layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "                \n",
    "                # shape of biases = [num outgoing filters]\n",
    "                biases = trainable([foc], 'biases')\n",
    "                layer = tf.nn.bias_add(layer, biases)\n",
    "                layer = tf.nn.leaky_relu(layer)\n",
    "                layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "                \n",
    "                # downsample\n",
    "                with tf.variable_scope('downsample'):\n",
    "                    fw = fw*2\n",
    "                    filters = trainable([fh, fw, foc, foc], 'filters')\n",
    "\n",
    "                    sW = 2\n",
    "                    strides_no_downsample = [sN, sH, sW, sC]\n",
    "                    layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "\n",
    "                    # shape of biases = [num outgoing filters]\n",
    "                    biases = trainable([foc], 'biases')\n",
    "                    layer = tf.nn.bias_add(layer, biases)\n",
    "                    layer = tf.nn.leaky_relu(layer)\n",
    "                    layer = tf.nn.dropout(layer, self.downsample_keep_prob)\n",
    "                    layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "\n",
    "                self._layers.append(layer)\n",
    "                    \n",
    "\n",
    "#         self.layer_nodes = 2**np.arange(0,10)[:self.num_downsamples][::-1]\n",
    "#         for j in range(len(self.layer_nodes)):\n",
    "            \n",
    "#             self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "            \n",
    "#             with tf.variable_scope('fc_layer_{}'.format(j)):\n",
    "#                 layer = tf.contrib.layers.fully_connected(self._layers[-1], self.layer_nodes[j])\n",
    "#                 layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "#                 self._layers.append(layer)\n",
    "            \n",
    "            \n",
    "#         self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "\n",
    "#         with tf.variable_scope('final_layer'):\n",
    "#             layer = tf.contrib.layers.fully_connected(tf.layers.flatten(self._layers[-1]), self._num_classes)\n",
    "#             layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "#             self._layers.append(layer)                \n",
    "        \n",
    "                \n",
    "        self._msg += ' '\n",
    "        with tf.variable_scope('labels'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.labels = tf.placeholder(dtype = self.dtype, shape = [None, self._num_classes], name = 'labels')\n",
    "            self.true_cls = tf.argmax(self.labels, axis = 1)\n",
    "            \n",
    "        with tf.variable_scope('logits'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self._logits = tf.contrib.layers.fully_connected(tf.layers.flatten(self._layers[-1]), self._num_classes, activation_fn = None)\n",
    "\n",
    "        with tf.variable_scope('predictions'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.predictions = tf.nn.softmax(self._logits)\n",
    "            \n",
    "            self.pred_cls = tf.argmax(self.predictions, axis = 1)\n",
    "\n",
    "            self.correct_prediction = tf.equal(self.pred_cls, self.true_cls)\n",
    "            \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, self.dtype))\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('costs'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self._cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.labels, logits = self._logits)\n",
    " \n",
    "            self.cost = tf.reduce_mean(self._cross_entropy)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.adam_inital_learning_rate, epsilon=self.adam_epsilon).minimize(self.cost)\n",
    "            \n",
    "        with tf.variable_scope('logging'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            tf.summary.scalar(name = 'cost', tensor = self.cost)\n",
    "            tf.summary.scalar(name = 'accuracy', tensor = self.accuracy)\n",
    "\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            \n",
    "        self._msg += ' done'\n",
    "        self._vprint(self._msg)\n",
    "        self._msg = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block_NN_2(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 name, \n",
    "                 num_downsamples,\n",
    "                 log_dir = 'logs/',\n",
    "                 dtype = tf.float32,\n",
    "                 adam_inital_learning_rate = 0.0001,\n",
    "                 adam_epsilon = 1e-8,\n",
    "                 verbose = False):\n",
    "    \n",
    "        self.name = name\n",
    "        self.num_downsamples = num_downsamples\n",
    "        self.log_dir = log_dir\n",
    "        self.dtype = dtype\n",
    "        self.adam_inital_learning_rate = adam_inital_learning_rate\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        self._num_freq_channels = 1024\n",
    "        self._layers = []\n",
    "        self._msg = ''\n",
    "        self._num_classes = 161\n",
    "        \n",
    "        self._vprint = sys.stdout.write if self._verbose else lambda *a, **k: None\n",
    "        \n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self._gen_params_dict())\n",
    "        \n",
    "    def _gen_params_dict(self):\n",
    "        \"\"\"Generates a dictionary of the arguments, the parameters that make this network.\"\"\"\n",
    "        d = self.__dict__\n",
    "        return {key : d[key] for key in d.keys() if key[0] != '_' if 'tensorflow' not in str(type(d[key]))}\n",
    "        \n",
    "    def _save_params(self):\n",
    "        \"\"\"Saves the parameters of the network as a dictionary in a .npz file .\n",
    "           path = log_dir/name/params/Splits.npz\n",
    "         \"\"\"\n",
    "        self._msg += '\\rsaving network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        direc = self.log_dir + self.name + '/params/'\n",
    "\n",
    "        if not os.path.exists(direc):\n",
    "            self._msg += '- creating new directory'\n",
    "            self._vprint(self._msg)\n",
    "            os.makedirs(direc)\n",
    "            \n",
    "        np.savez(direc + self.__class__.__name__, self._gen_params_dict())\n",
    "        self._msg += ' - params saved'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old network, but keep the current network name.\"\"\"\n",
    "    \n",
    "        self._msg = '\\rloading network parameters'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        a = np.load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        self._msg += ' - setting params'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        name = self.name\n",
    "        params = d['data1arr_0'][()]\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "        self.name = name\n",
    "        \n",
    "        self._msg = '\\rparams loaded'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def create_graph(self):\n",
    "\n",
    "        self._save_params()\n",
    "        self._msg = '\\rcreating network graph '; self._vprint(self._msg)\n",
    "\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.is_training = tf.placeholder(dtype = tf.bool, shape = [], name = 'is_training')\n",
    "        \n",
    "        with tf.variable_scope('keep_probs'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self.sample_keep_prob = tf.placeholder(self.dtype, name = 'sample_keep_prob')\n",
    "            self.downsample_keep_prob = tf.placeholder(self.dtype, name = 'downsample_keep_prob')\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('sample'):\n",
    "            \n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.X = tf.placeholder(self.dtype, shape = [None, 1, self._num_freq_channels, 1], name = 'X')\n",
    "            self.X = tf.nn.dropout(self.X, self.sample_keep_prob)\n",
    "            \n",
    "        trainable = lambda shape, name : tf.get_variable(name = name,\n",
    "                                                         dtype = self.dtype,\n",
    "                                                         shape = shape,\n",
    "                                                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "#         for i in range(self.num_downsamples):\n",
    "#             self._msg += ' {}'.format(i); self._vprint(self._msg)\n",
    "            \n",
    "#             with tf.variable_scope('conv_layer_{}'.format(i)):\n",
    "\n",
    "#                 layer = self.X if i == 0 else self._layers[-1]\n",
    "                \n",
    "#                 # filter shape:\n",
    "#                 fh = 1 # filter height = 1 for 1D convolution\n",
    "#                 fw = 3 # filter width\n",
    "#                 fic = 2**(i) # num in channels = number of incoming filters\n",
    "#                 foc = 2**(i+1) # num out channels = number of outgoing filters\n",
    "#                 filters = trainable([fh, fw, fic, foc], 'filters')\n",
    "                \n",
    "#                 # stride shape\n",
    "#                 sN = 1 # batch = 1 (why anything but 1 ?)\n",
    "#                 sH = 1 # height of stride = 1 for 1D conv\n",
    "#                 sW = 1 # width of stride = downsampling factor = 1 for no downsampling or > 1 for downsampling\n",
    "#                 sC = 1 # depth = number of channels the stride walks over = 1 (why anything but 1 ?)\n",
    "#                 strides_no_downsample = [sN, sH, sW, sC]\n",
    "#                 layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "                \n",
    "#                 # shape of biases = [num outgoing filters]\n",
    "#                 biases = trainable([foc], 'biases')\n",
    "#                 layer = tf.nn.bias_add(layer, biases)\n",
    "#                 layer = tf.nn.leaky_relu(layer)\n",
    "#                 layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "                \n",
    "#                 # downsample\n",
    "#                 with tf.variable_scope('downsample'):\n",
    "#                     fw = 7\n",
    "#                     filters = trainable([fh, fw, foc, foc], 'filters')\n",
    "\n",
    "#                     sW = 2\n",
    "#                     strides_no_downsample = [sN, sH, sW, sC]\n",
    "#                     layer = tf.nn.conv2d(layer, filters, strides_no_downsample, 'SAME')\n",
    "\n",
    "#                     # shape of biases = [num outgoing filters]\n",
    "#                     biases = trainable([foc], 'biases')\n",
    "#                     layer = tf.nn.bias_add(layer, biases)\n",
    "#                     layer = tf.nn.leaky_relu(layer)\n",
    "#                     layer = tf.nn.dropout(layer, self.downsample_keep_prob)\n",
    "#                     layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "\n",
    "#                 self._layers.append(layer)\n",
    "                    \n",
    "\n",
    "        self.layer_nodes = 2**np.arange(0,10)[:self.num_downsamples][::-1]\n",
    "        for j in range(len(self.layer_nodes)):\n",
    "            \n",
    "            self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "            \n",
    "            with tf.variable_scope('fc_layer_{}'.format(j)):\n",
    "                input = self.X if j == 0 else self._layers[-1]\n",
    "                layer = tf.contrib.layers.fully_connected(tf.layers.flatten(input), self.layer_nodes[j])\n",
    "                layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "                self._layers.append(layer)\n",
    "            \n",
    "            \n",
    "#         self._msg += ' {}'.format(i + 1 + j);self._vprint(self._msg)\n",
    "\n",
    "#         with tf.variable_scope('final_layer'):\n",
    "#             layer = tf.contrib.layers.fully_connected(tf.layers.flatten(self._layers[-1]), self._num_classes)\n",
    "#             layer = tf.contrib.layers.batch_norm(layer, is_training = self.is_training)\n",
    "#             self._layers.append(layer)                \n",
    "        \n",
    "                \n",
    "        self._msg += ' '\n",
    "        with tf.variable_scope('labels'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.labels = tf.placeholder(dtype = self.dtype, shape = [None, self._num_classes], name = 'labels')\n",
    "            self.true_cls = tf.argmax(self.labels, axis = 1)\n",
    "            \n",
    "        with tf.variable_scope('logits'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self._logits = tf.contrib.layers.fully_connected(tf.layers.flatten(self._layers[-1]), self._num_classes, activation_fn = None)\n",
    "\n",
    "        with tf.variable_scope('predictions'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            self.predictions = tf.nn.softmax(self._logits)\n",
    "            \n",
    "            self.pred_cls = tf.argmax(self.predictions, axis = 1)\n",
    "\n",
    "            self.correct_prediction = tf.equal(self.pred_cls, self.true_cls)\n",
    "            \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, self.dtype))\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('costs'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "\n",
    "            self._cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.labels, logits = self._logits)\n",
    " \n",
    "            self.cost = tf.reduce_mean(self._cross_entropy)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.adam_inital_learning_rate, epsilon=self.adam_epsilon).minimize(self.cost)\n",
    "            \n",
    "        with tf.variable_scope('logging'):\n",
    "            self._msg += '.'; self._vprint(self._msg)\n",
    "            \n",
    "            tf.summary.scalar(name = 'cost', tensor = self.cost)\n",
    "            tf.summary.scalar(name = 'accuracy', tensor = self.accuracy)\n",
    "\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            \n",
    "        self._msg += ' done'\n",
    "        self._vprint(self._msg)\n",
    "        self._msg = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'fc_layer_0/BatchNorm/Reshape_1:0' shape=(?, 512) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_1/BatchNorm/Reshape_1:0' shape=(?, 256) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_2/BatchNorm/Reshape_1:0' shape=(?, 128) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_3/BatchNorm/Reshape_1:0' shape=(?, 64) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_4/BatchNorm/Reshape_1:0' shape=(?, 32) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_5/BatchNorm/Reshape_1:0' shape=(?, 16) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_6/BatchNorm/Reshape_1:0' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_7/BatchNorm/Reshape_1:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_8/BatchNorm/Reshape_1:0' shape=(?, 2) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_9/BatchNorm/Reshape_1:0' shape=(?, 1) dtype=float32>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Block_NN_2('x',10)\n",
    "x.create_graph()\n",
    "x._layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from block_Data.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../modules'))\n",
    "import notebook_loading\n",
    "\n",
    "from block_Data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the redundant baselines and their gains and data from miriad and calfits files\n",
    "red_bls, gains, uvd = load_relevant_data('../zen_data/zen.2458098.58037.xx.HH.uv','../zen_data/zen.2458098.58037.xx.HH.uv.abs.calfits')\n",
    "\n",
    "# seperate trining and testing redundant baselines \n",
    "# if we have not already done this, load them from disk\n",
    "training_redundant_baselines_dict, testing_redundant_baselines_dict = get_or_gen_test_train_red_bls_dicts(red_bls, gains.keys())\n",
    "\n",
    "# seperate the visiblites\n",
    "training_baselines_data = get_seps_data(training_redundant_baselines_dict, uvd)\n",
    "testing_baselines_data = get_seps_data(testing_redundant_baselines_dict, uvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "class Block_Trainer(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 network = None,\n",
    "                 abs_min_max_delay = 0.040,\n",
    "                 num_flatnesses = 100,\n",
    "                 num_epochs = 100,\n",
    "                 batch_size = 32,\n",
    "                 log_dir = 'logs/',\n",
    "                 model_save_interval = 25,\n",
    "                 pretrained_model_path = None,\n",
    "                 sample_keep_prob = 0.80,\n",
    "                 downsample_keep_prob = 0.50,\n",
    "                 verbose = False):\n",
    "        \n",
    "        self.network = network\n",
    "        self.abs_min_max_delay = abs_min_max_delay\n",
    "        self.num_flatnesses = num_flatnesses\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.log_dir = log_dir\n",
    "        self.model_save_interval = model_save_interval\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.sample_keep_prob = sample_keep_prob\n",
    "        self.downsample_keep_prob = downsample_keep_prob\n",
    "        \n",
    "        self._num_classes = 161\n",
    "        \n",
    "        self._verbose = verbose\n",
    "        self._msg = ''\n",
    "        self._vprint = sys.stdout.write if self._verbose else lambda *a, **k: None\n",
    "        \n",
    "    def print_params(self):\n",
    "        \"\"\"Prints netwwork parameters\"\"\"\n",
    "        pprint(self._gen_params_dict())\n",
    "        \n",
    "    def _gen_params_dict(self):\n",
    "        \"\"\"Generates a dictionary of the arguments, the parameters that make this network.\"\"\"\n",
    "        d = self.__dict__\n",
    "        return {key : d[key] for key in d.keys() if key != 'network' if key[0] != '_' if 'tensorflow' not in str(type(d[key]))}\n",
    "        \n",
    "    def _save_params(self):\n",
    "        \"\"\"Saves the parameters of the trainer as a dictionary in a .npz file .\n",
    "           path = log_dir/name/params/Train.npz\n",
    "         \"\"\"\n",
    "        self._msg += '\\rsaving trainer parameters'\n",
    "        self._vprint(self._msg)\n",
    "        direc = self.log_dir + self.network.name + '/params/'\n",
    "\n",
    "        if not os.path.exists(direc):\n",
    "            self._msg += ' - creating new directory'\n",
    "            self._vprint(self._msg)\n",
    "            os.makedirs(direc)\n",
    "            \n",
    "        np.savez(direc + self.__class__.__name__, self._gen_params_dict())\n",
    "        self._msg += ' - params saved'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "    def load_params(self, path):\n",
    "        \"\"\"Load in the parameters of an old trainer.\"\"\"\n",
    "    \n",
    "        self._msg = '\\rloading trainer parameters'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        a = np.load(path + '.npz')\n",
    "        d = dict(zip((\"data1{}\".format(k) for k in a), (a[k] for k in a)))\n",
    "        \n",
    "        self._msg += ' - setting params'\n",
    "        self._vprint(self._msg)\n",
    "        \n",
    "        params = d['data1arr_0'][()]\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "        \n",
    "        self._msg = '\\rparams loaded'\n",
    "        self._vprint(self._msg)\n",
    "\n",
    "\n",
    "\n",
    "    def add_data(self,train_info, test_info, gains):\n",
    "        self._msg = '\\radding data';self._vprint(self._msg)\n",
    "\n",
    "        \n",
    "        self._train_batcher = block_Data_Creator(self.num_flatnesses,\n",
    "                                           train_info[0],\n",
    "                                           train_info[1],\n",
    "                                           gains,\n",
    "                                           self.abs_min_max_delay)\n",
    "        self._train_batcher.gen_data()\n",
    "\n",
    "\n",
    "        self._test_batcher = block_Data_Creator(self.num_flatnesses,\n",
    "                                          test_info[0],\n",
    "                                          test_info[1],\n",
    "                                          gains,\n",
    "                                          self.abs_min_max_delay)\n",
    "        \n",
    "        self._test_batcher.gen_data()\n",
    "        self._msg += ' - trainer ready';self._vprint(self._msg)\n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        self._save_params()\n",
    "        self.costs = []\n",
    "        self.accuracies = []\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.network.create_graph()\n",
    "        print(np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as session:\n",
    "            \n",
    "            if self.pretrained_model_path == None:\n",
    "                session.run(tf.global_variables_initializer())\n",
    "                \n",
    "            else:\n",
    "                saver.restore(session, self.pretrained_model_path)\n",
    "                \n",
    "            \n",
    "            archive_loc = self.log_dir + self.network.name\n",
    "            training_writer = tf.summary.FileWriter(archive_loc + '/training', session.graph)\n",
    "            testing_writer = tf.summary.FileWriter(archive_loc + '/testing', session.graph)\n",
    "            self.model_save_location = archive_loc + '/trained_model.ckpt'   \n",
    "            \n",
    "            \n",
    "            self._msg = '\\rtraining';self._vprint(self._msg)\n",
    "\n",
    "            try:\n",
    "                for epoch in range(self.num_epochs):\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    training_inputs, training_labels = self._train_batcher.get_data(); self._train_batcher.gen_data()\n",
    "                    testing_inputs, testing_labels = self._test_batcher.get_data(); self._test_batcher.gen_data()  \n",
    "\n",
    "                    training_labels = np.asarray(training_labels)\n",
    "                    testing_labels = np.asarray(testing_labels).reshape(-1,self._num_classes)\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    # if the division here has a remainde some values are just truncated\n",
    "                    batch_size = self.batch_size\n",
    "                    num_entries = self.num_flatnesses * 60.\n",
    "\n",
    "                    for j in range(int(num_entries/batch_size)):\n",
    "                        self._msg = '\\rEpoch - {:5.0f}/{:5.0f} - batch: {:4.0f}/{:4.0f}'.format(epoch + 1,self.num_epochs, j + 1, int(num_entries/batch_size))\n",
    "                        if epoch != 0:\n",
    "                            self._msg += ' - (Training, Testing) - '.format(epoch)\n",
    "                            self._msg += \" costs: ({:0.4f}, {:0.4f})\".format(training_cost, testing_cost)\n",
    "                            self._msg += \" accss: ({:0.4f}, {:0.4f})\".format(training_acc, testing_acc)\n",
    "                        self._vprint(self._msg); \n",
    "\n",
    "                        training_inputs_batch = training_inputs[j*batch_size:(j + 1)*batch_size].reshape(-1,1,1024,1)\n",
    "                        training_labels_batch = training_labels[j*batch_size:(j + 1)*batch_size].reshape(-1,self._num_classes)\n",
    "\n",
    "                        feed_dict = {self.network.X: training_inputs_batch,\n",
    "                                     self.network.labels: training_labels_batch,\n",
    "                                     self.network.sample_keep_prob : self.sample_keep_prob,\n",
    "                                     self.network.downsample_keep_prob : self.downsample_keep_prob,\n",
    "                                     self.network.is_training : True}\n",
    "\n",
    "                        session.run([self.network.optimizer], feed_dict = feed_dict) \n",
    "                            \n",
    "                    train_feed_dict = {self.network.X: training_inputs.reshape(-1,1,1024,1),\n",
    "                                      self.network.labels: training_labels.reshape(-1,self._num_classes),\n",
    "                                      self.network.sample_keep_prob : 1.,\n",
    "                                      self.network.downsample_keep_prob : 1.,\n",
    "                                      self.network.is_training : False}\n",
    "\n",
    "\n",
    "                    training_cost, training_acc, training_summary = session.run([self.network.cost,\n",
    "                                                                                 self.network.accuracy,\n",
    "                                                                                 self.network.summary],\n",
    "                                                                                 feed_dict = train_feed_dict) \n",
    "\n",
    "                    training_writer.add_summary(training_summary, epoch)\n",
    "                    training_writer.flush()  \n",
    "                \n",
    "                    \n",
    "                    test_feed_dict = {self.network.X: testing_inputs.reshape(-1,1,1024,1),\n",
    "                                      self.network.labels: testing_labels.reshape(-1,self._num_classes),\n",
    "                                      self.network.sample_keep_prob : 1.,\n",
    "                                      self.network.downsample_keep_prob : 1.,\n",
    "                                      self.network.is_training : False} \n",
    "\n",
    "                    testing_cost, testing_acc, testing_summary = session.run([self.network.cost,\n",
    "                                                                              self.network.accuracy,\n",
    "                                                                              self.network.summary],\n",
    "                                                                              feed_dict = test_feed_dict)\n",
    "\n",
    "                    testing_writer.add_summary(testing_summary, epoch)\n",
    "                    testing_writer.flush()\n",
    "                    \n",
    "                    self.costs.append((training_cost, testing_cost))\n",
    "                    self.accuracies.append((training_acc, testing_acc))\n",
    "                    \n",
    "                    if (epoch + 1) % self.model_save_interval == 0:\n",
    "                        saver.save(session, self.model_save_location, epoch + 1)\n",
    "                \n",
    "                self.msg = ''\n",
    "            except KeyboardInterrupt:\n",
    "                self._msg = ' TRAINING INTERUPPTED' # this never prints I dont know why\n",
    "                pass\n",
    "\n",
    "            self._msg += '\\rtraining ended'; self._vprint(self._msg)\n",
    "            \n",
    "            training_writer.close()\n",
    "            testing_writer.close()\n",
    "\n",
    "\n",
    "        session.close()\n",
    "        self._msg += ' - session closed'; self._vprint(self._msg)\n",
    "        self._msg = ''\n",
    "        \n",
    "    def plot(self, mode = 'all', figsize = (8,6) ):\n",
    "        \n",
    "        xvals = np.arange(len(self.costs))\n",
    "        \n",
    "        \n",
    "        if mode == 'all':\n",
    "            \n",
    "            yvals = [self.costs, self.accuracies]\n",
    "            metric_names = ['costs', 'accuracies']\n",
    "            \n",
    "        fig, axes = plt.subplots(len(yvals), 1, figsize = figsize, dpi = 144)\n",
    "        \n",
    "        for i, ax in enumerate(axes.reshape(-1)):\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.set_title('{}'.format(self.network.name))\n",
    "\n",
    "            ax.plot(xvals, yvals[i], lw = 0.5)\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel(metric_names[i])\n",
    "            \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def save_results(self):\n",
    "        \n",
    "        results = {'MISGs': self.MISGs, 'MSEs': self.MSEs, 'PWTs': self.PWTs}\n",
    "        direc = self.log_dir + self.network.name + '/results/'\n",
    "        \"\"\"Not safe - will overwrite existing file.\"\"\"\n",
    "        if not os.path.exists(direc):\n",
    "            os.makedirs(direc)\n",
    "        np.savez(direc + 'results', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 10\n",
    "batch_size = 100\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 250, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 2\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 3\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 4\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 5\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 6\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 7\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 8\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 9\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 10\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 2\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}_B'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.5, batch_size=batch_size, verbose=True, num_epochs = 50000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 13174/50000 - batch:    1/  30 - (Training, Testing) -  costs: (4.6806, 5.1719) accss: (0.0667, 0.0167)"
     ]
    }
   ],
   "source": [
    "num_downsamples = 2\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}_C'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 1, batch_size=batch_size, verbose=True, num_epochs = 50000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 10\n",
    "batch_size = 2\n",
    "lrp = 4\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN_2('network_{}_{}_{}_2C'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 100, downsample_keep_prob = 1, batch_size=batch_size, verbose=True, num_epochs = 50000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'fc_layer_0/BatchNorm/Reshape_1:0' shape=(?, 2) dtype=float32>,\n",
       " <tf.Tensor 'fc_layer_1/BatchNorm/Reshape_1:0' shape=(?, 1) dtype=float32>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving trainer parameters - creating new directory - params saved1992177\n",
      "training ended - session closed"
     ]
    }
   ],
   "source": [
    "num_downsamples = 3\n",
    "batch_size = 2\n",
    "lrp = 5\n",
    "lr = 1.*10.**(-lrp)\n",
    "network = Block_NN('network_{}_{}_{}'.format(num_downsamples,batch_size, lrp),num_downsamples, adam_inital_learning_rate=lr)\n",
    "trainer = Block_Trainer(network = network, num_flatnesses = 1, downsample_keep_prob = 0.8, batch_size=batch_size, verbose=True, num_epochs = 5000)\n",
    "trainer.add_data((training_baselines_data, training_redundant_baselines_dict),\n",
    "         (testing_baselines_data, testing_redundant_baselines_dict),\n",
    "         gains)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network._layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
