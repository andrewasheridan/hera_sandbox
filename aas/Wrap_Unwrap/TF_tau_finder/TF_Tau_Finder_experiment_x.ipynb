{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:38:30.199851Z",
     "start_time": "2018-05-25T05:38:30.191924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fullwidth notebook cells\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:38:31.165821Z",
     "start_time": "2018-05-25T05:38:31.158681Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# for reading & slicing data\n",
    "import pandas as pd\n",
    "\n",
    "# for data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:38:39.693175Z",
     "start_time": "2018-05-25T05:38:32.064425Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'data/wrap_data_test_1.csv'\n",
    "data_df = pd.read_csv(data_path, dtype = float)\n",
    "\n",
    "training_data_df = data_df.sample(frac = 0.8)\n",
    "testing_data_df = data_df.loc[~data_df.index.isin(training_data_df.index)]\n",
    "\n",
    "X_training = training_data_df.drop('tau', axis = 1).values\n",
    "Y_training = training_data_df[['tau']].values\n",
    "\n",
    "X_testing = testing_data_df.drop('tau', axis = 1).values\n",
    "Y_testing = testing_data_df[['tau']].values\n",
    "\n",
    "X_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "Y_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "number_of_inputs = X_scaled_training.shape[1]\n",
    "number_of_outputs = Y_scaled_training.shape[1]\n",
    "\n",
    "learning_rate = 0.0015\n",
    "training_epochs = 200\n",
    "display_step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:38:40.170390Z",
     "start_time": "2018-05-25T05:38:40.138065Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_1_node_vals = np.arange(50,550,50)\n",
    "layer_2_node_vals = np.arange(50,550,50)\n",
    "layer_3_node_vals = np.arange(50,550,50)\n",
    "\n",
    "log_paths = []\n",
    "for layer_1_nodes in layer_1_node_vals:\n",
    "    for layer_2_nodes in layer_2_node_vals:\n",
    "        for layer_3_nodes in layer_3_node_vals:\n",
    "            \n",
    "            log_path = str(layer_1_nodes) + \"_\" + str(layer_2_nodes) +  \"_\" + str(layer_3_nodes)\n",
    "                \n",
    "            log_paths.append(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:38:40.373123Z",
     "start_time": "2018-05-25T05:38:40.367370Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_layers = np.random.choice(log_paths, size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T06:17:28.926627Z",
     "start_time": "2018-05-25T06:17:28.300120Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(layer_1_nodes,layer_2_nodes,layer_3_nodes, logdir = ''):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Input Layer\n",
    "    with tf.variable_scope('input'):\n",
    "        X  = tf.placeholder(tf.float32, shape = (None, number_of_inputs))\n",
    "        \n",
    "    # Layer 1\n",
    "    with tf.variable_scope('layer_1'):\n",
    "        \n",
    "        biases = tf.get_variable(name = \"biases1\",\n",
    "                                 shape = [layer_1_nodes],\n",
    "                                 initializer = tf.zeros_initializer())\n",
    "        \n",
    "        weights = tf.get_variable(name = \"weights1\",\n",
    "                                  shape  = [number_of_inputs, layer_1_nodes],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    \n",
    "        layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "        \n",
    "    # Layer 2\n",
    "    with tf.variable_scope('layer_2'):\n",
    "        \n",
    "        biases = tf.get_variable(name = \"biases2\",\n",
    "                                 shape = [layer_2_nodes],\n",
    "                                 initializer = tf.zeros_initializer())\n",
    "        \n",
    "        weights = tf.get_variable(name = \"weights2\",\n",
    "                                  shape  = [layer_1_nodes, layer_2_nodes],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    \n",
    "        layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "        \n",
    "    # Layer 3\n",
    "    with tf.variable_scope('layer_3'):\n",
    "        \n",
    "        biases = tf.get_variable(name = \"biases3\",\n",
    "                                 shape = [layer_3_nodes],\n",
    "                                 initializer = tf.zeros_initializer())\n",
    "        \n",
    "        weights = tf.get_variable(name = \"weights3\",\n",
    "                                  shape  = [layer_2_nodes, layer_3_nodes],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    \n",
    "        layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "        \n",
    "    # Output layer\n",
    "    \n",
    "    with tf.variable_scope('output'):\n",
    "        \n",
    "        biases = tf.get_variable(name = \"biases_out\",\n",
    "                                 shape = [number_of_outputs],\n",
    "                                 initializer = tf.zeros_initializer())\n",
    "        \n",
    "        weights = tf.get_variable(name = \"weights_out\",\n",
    "                                  shape  = [layer_3_nodes, number_of_outputs],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    \n",
    "        prediction = tf.nn.relu(tf.matmul(layer_3_output, weights) + biases)\n",
    "        \n",
    "    with tf.variable_scope('cost'):\n",
    "        \n",
    "        Y = tf.placeholder(tf.float32, shape = (None, 1))\n",
    "        cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
    "    \n",
    "        \n",
    "    with tf.variable_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=1e-05).minimize(cost)\n",
    "        \n",
    "    with tf.variable_scope('logging'):\n",
    "        tf.summary.scalar('current_cost', cost)\n",
    "        summary = tf.summary.merge_all()\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # initialize a session to run TF operations\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        # Run the global variable initilizer to init all variables and layers\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Create log writers to record training progress\n",
    "        # Store training and testing data separately\n",
    "        training_writer = tf.summary.FileWriter('logs/' + logdir + random_layer + '/training', session.graph)\n",
    "        testing_writer = tf.summary.FileWriter('logs/' + logdir + random_layer + '/testing', session.graph)\n",
    "\n",
    "\n",
    "        # Run the optimizer over and over to train the network\n",
    "        # One epoch is one full run through the training data set\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            # Feed in the training data and do one stepf NN training\n",
    "            session.run(optimizer,\n",
    "                        feed_dict = {X: X_scaled_training, Y: Y_scaled_training})\n",
    "\n",
    "            # Every display_step steps, log our process\n",
    "            if epoch % 10 == 0:\n",
    "\n",
    "                training_feed = {X: X_scaled_training, Y: Y_scaled_training}\n",
    "                training_cost, training_summary = session.run([cost, summary],\n",
    "                                                        feed_dict = training_feed)\n",
    "\n",
    "                testing_feed =  {X: X_scaled_testing, Y: Y_scaled_testing}\n",
    "                testing_cost, testing_summary = session.run([cost, summary],\n",
    "                                                         feed_dict = testing_feed)\n",
    "\n",
    "                print(epoch, training_cost, testing_cost)\n",
    "\n",
    "                # write the current status to the log files\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            testing_writer.flush()\n",
    "            training_writer.flush()\n",
    "    #         print('Training done')\n",
    "\n",
    "        final_training_cost = session.run(cost,\n",
    "                                          feed_dict = {X: X_scaled_training,\n",
    "                                                       Y: Y_scaled_training})\n",
    "\n",
    "        final_testing_cost = session.run(cost,\n",
    "                                         feed_dict = {X: X_scaled_testing,\n",
    "                                                      Y: Y_scaled_testing})\n",
    "\n",
    "#         print('Final Training Cost: {}'.format(final_training_cost))\n",
    "#         print('Final Testing Cost: {}'.format(final_testing_cost))\n",
    "\n",
    "        model_save_location = 'logs/' + logdir + random_layer + 'model' + '/trained_model.ckpt'\n",
    "        save_path = saver.save(session, model_save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T04:52:33.382408Z",
     "start_time": "2018-05-25T04:18:16.536050Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.950407 4.9488955\n",
      "10 0.092782214 0.09390253\n",
      "20 0.08792446 0.08885831\n",
      "30 0.08650418 0.08705993\n",
      "40 0.08539292 0.08592592\n",
      "50 0.0849574 0.085475385\n",
      "60 0.084172525 0.08471407\n",
      "70 0.08310406 0.083835036\n",
      "80 0.081669465 0.08272992\n",
      "90 0.079245426 0.080549836\n",
      "100 0.07623921 0.07754141\n",
      "110 0.07292085 0.0741587\n",
      "120 0.06884875 0.07018775\n",
      "130 0.06389319 0.06537057\n",
      "140 0.05873826 0.060682923\n",
      "150 0.054106556 0.055812694\n",
      "160 0.052716155 0.05485922\n",
      "170 0.045441926 0.04682956\n",
      "180 0.041895043 0.04330854\n",
      "190 0.043108482 0.044810675\n",
      "200 0.038088385 0.039587583\n",
      "210 0.032992378 0.03512038\n",
      "220 0.028579516 0.030302636\n",
      "230 0.034929328 0.037163075\n",
      "240 0.028512143 0.030854516\n",
      "250 0.02178 0.023638528\n",
      "260 0.018994648 0.020786129\n",
      "270 0.015854888 0.018019589\n",
      "280 0.021765104 0.02351233\n",
      "290 0.013753706 0.015376572\n",
      "300 0.010465577 0.012149467\n",
      "310 0.010093743 0.011804394\n",
      "320 0.00965171 0.011041803\n",
      "330 0.0070971046 0.008507437\n",
      "340 0.0060792635 0.007344389\n",
      "350 0.00786021 0.009211924\n",
      "360 0.0053348816 0.0065269447\n",
      "370 0.004751363 0.0058582886\n",
      "380 0.0044340286 0.0055870092\n",
      "390 0.0038089 0.0048947963\n",
      "400 0.00485551 0.005897537\n",
      "410 0.005908018 0.0069809253\n",
      "420 0.0037028508 0.0046597617\n",
      "430 0.0031498359 0.004079563\n",
      "440 0.003089046 0.00403642\n",
      "450 0.0026105708 0.0035265451\n",
      "460 0.003566968 0.0044445368\n",
      "470 0.0024959096 0.0033666103\n",
      "480 0.002828989 0.0036853098\n",
      "490 0.002553293 0.003338799\n",
      "Training done\n",
      "Final Training Cost: 0.0024501075968146324\n",
      "Final Testing Cost: 0.0032285619527101517\n",
      "0 2.8635519 2.850379\n",
      "10 0.1215676 0.122139886\n",
      "20 0.090578794 0.09054826\n",
      "30 0.08598085 0.086180635\n",
      "40 0.08454303 0.084868774\n",
      "50 0.08221105 0.08249902\n",
      "60 0.07949075 0.07968493\n",
      "70 0.07644463 0.07652054\n",
      "80 0.07358633 0.07379533\n",
      "90 0.06918108 0.06987349\n",
      "100 0.06352016 0.06465322\n",
      "110 0.05547224 0.05708245\n",
      "120 0.048118666 0.04946933\n",
      "130 0.036890488 0.037841097\n",
      "140 0.03495578 0.034895714\n",
      "150 0.032264505 0.032490663\n",
      "160 0.020312607 0.02141466\n",
      "170 0.016219297 0.017782768\n",
      "180 0.013279323 0.014898526\n",
      "190 0.011005773 0.012779297\n",
      "200 0.009499148 0.011246889\n",
      "210 0.008257129 0.009871479\n",
      "220 0.007249462 0.008797997\n",
      "230 0.006429359 0.007968629\n",
      "240 0.0067074858 0.008228065\n",
      "250 0.0054233884 0.0068566687\n",
      "260 0.0051410454 0.0064669596\n",
      "270 0.004429485 0.005782001\n",
      "280 0.0040649725 0.0053455937\n",
      "290 0.0039040535 0.0051640333\n",
      "300 0.003492213 0.004617034\n",
      "310 0.0032038752 0.004262107\n",
      "320 0.0028355443 0.0038640832\n",
      "330 0.0059290016 0.0066778734\n",
      "340 0.0035107152 0.004544917\n",
      "350 0.0023762328 0.0033254162\n",
      "360 0.0023720774 0.0031990567\n",
      "370 0.002124325 0.0029464595\n",
      "380 0.0019077214 0.0027186398\n",
      "390 0.0018016542 0.0026151438\n",
      "400 0.0016765094 0.002453558\n",
      "410 0.0016164447 0.0023876633\n",
      "420 0.004241419 0.005086358\n",
      "430 0.0029588914 0.0035296693\n",
      "440 0.001998453 0.002768543\n",
      "450 0.0015417294 0.0021687679\n",
      "460 0.0013013127 0.0019630117\n",
      "470 0.0012140798 0.0018771628\n",
      "480 0.0011551745 0.0017997117\n",
      "490 0.0011079989 0.0017378893\n",
      "Training done\n",
      "Final Training Cost: 0.0010615712963044643\n",
      "Final Testing Cost: 0.001684232265688479\n",
      "0 32.48881 32.21965\n",
      "10 0.33241737 0.32945666\n",
      "20 0.33241737 0.32945666\n",
      "30 0.33241737 0.32945666\n",
      "40 0.33241737 0.32945666\n",
      "50 0.33241737 0.32945666\n",
      "60 0.33241737 0.32945666\n",
      "70 0.33241737 0.32945666\n",
      "80 0.33241737 0.32945666\n",
      "90 0.33241737 0.32945666\n",
      "100 0.33241737 0.32945666\n",
      "110 0.33241737 0.32945666\n",
      "120 0.33241737 0.32945666\n",
      "130 0.33241737 0.32945666\n",
      "140 0.33241737 0.32945666\n",
      "150 0.33241737 0.32945666\n",
      "160 0.33241737 0.32945666\n",
      "170 0.33241737 0.32945666\n",
      "180 0.33241737 0.32945666\n",
      "190 0.33241737 0.32945666\n",
      "200 0.33241737 0.32945666\n",
      "210 0.33241737 0.32945666\n",
      "220 0.33241737 0.32945666\n",
      "230 0.33241737 0.32945666\n",
      "240 0.33241737 0.32945666\n",
      "250 0.33241737 0.32945666\n",
      "260 0.33241737 0.32945666\n",
      "270 0.33241737 0.32945666\n",
      "280 0.33241737 0.32945666\n",
      "290 0.33241737 0.32945666\n",
      "300 0.33241737 0.32945666\n",
      "310 0.33241737 0.32945666\n",
      "320 0.33241737 0.32945666\n",
      "330 0.33241737 0.32945666\n",
      "340 0.33241737 0.32945666\n",
      "350 0.33241737 0.32945666\n",
      "360 0.33241737 0.32945666\n",
      "370 0.33241737 0.32945666\n",
      "380 0.33241737 0.32945666\n",
      "390 0.33241737 0.32945666\n",
      "400 0.33241737 0.32945666\n",
      "410 0.33241737 0.32945666\n",
      "420 0.33241737 0.32945666\n",
      "430 0.33241737 0.32945666\n",
      "440 0.33241737 0.32945666\n",
      "450 0.33241737 0.32945666\n",
      "460 0.33241737 0.32945666\n",
      "470 0.33241737 0.32945666\n",
      "480 0.33241737 0.32945666\n",
      "490 0.33241737 0.32945666\n",
      "Training done\n",
      "Final Training Cost: 0.332417368888855\n",
      "Final Testing Cost: 0.3294566571712494\n",
      "0 18.34758 18.204702\n",
      "10 0.08746574 0.08820902\n",
      "20 0.08596835 0.08544211\n",
      "30 0.08574249 0.08536866\n",
      "40 0.08421031 0.08400203\n",
      "50 0.08415553 0.08396945\n",
      "60 0.081795074 0.081888266\n",
      "70 0.07952164 0.07995286\n",
      "80 0.075141005 0.07625536\n",
      "90 0.07074546 0.07198999\n",
      "100 0.07906727 0.08175607\n",
      "110 0.060513705 0.061895624\n",
      "120 0.04911522 0.051572915\n",
      "130 0.061697435 0.06338311\n",
      "140 0.06713121 0.06693646\n",
      "150 0.046048295 0.04699983\n",
      "160 0.032368653 0.033546433\n",
      "170 0.022971557 0.0242155\n",
      "180 0.025539326 0.026694367\n",
      "190 0.025586767 0.02669342\n",
      "200 0.014903191 0.015937803\n",
      "210 0.0124325305 0.013374288\n",
      "220 0.009382269 0.010498916\n",
      "230 0.0075762323 0.008580657\n",
      "240 0.007330028 0.008099934\n",
      "250 0.008803957 0.009869703\n",
      "260 0.0066234265 0.0072635897\n",
      "270 0.007837429 0.009056606\n",
      "280 0.0067376182 0.0072781234\n",
      "290 0.0042475048 0.0051894705\n",
      "300 0.0039060363 0.004795633\n",
      "310 0.0038704914 0.0045273886\n",
      "320 0.003315254 0.004171901\n",
      "330 0.0030867322 0.0039022944\n",
      "340 0.0028928337 0.0036788892\n",
      "350 0.002834781 0.0035325321\n",
      "360 0.0028003387 0.0036610693\n",
      "370 0.002491383 0.0032020598\n",
      "380 0.002769045 0.0033814588\n",
      "390 0.004190251 0.005305495\n",
      "400 0.0039351727 0.0044007557\n",
      "410 0.0033276335 0.0043209726\n",
      "420 0.0027190286 0.0032514809\n",
      "430 0.0022819866 0.0030911122\n",
      "440 0.0019963486 0.0026060953\n",
      "450 0.0018133976 0.002522768\n",
      "460 0.0017357016 0.0024318784\n",
      "470 0.0019586952 0.0025271205\n",
      "480 0.0025697951 0.003463231\n",
      "490 0.0029100657 0.0033531692\n",
      "Training done\n",
      "Final Training Cost: 0.002212062943726778\n",
      "Final Testing Cost: 0.0027017893735319376\n",
      "0 42.525265 42.25403\n",
      "10 0.33241737 0.32945666\n",
      "20 0.33241737 0.32945666\n",
      "30 0.33241737 0.32945666\n",
      "40 0.33241737 0.32945666\n",
      "50 0.33241737 0.32945666\n",
      "60 0.33241737 0.32945666\n",
      "70 0.33241737 0.32945666\n",
      "80 0.33241737 0.32945666\n",
      "90 0.33241737 0.32945666\n",
      "100 0.33241737 0.32945666\n",
      "110 0.33241737 0.32945666\n",
      "120 0.33241737 0.32945666\n",
      "130 0.33241737 0.32945666\n",
      "140 0.33241737 0.32945666\n",
      "150 0.33241737 0.32945666\n",
      "160 0.33241737 0.32945666\n",
      "170 0.33241737 0.32945666\n",
      "180 0.33241737 0.32945666\n",
      "190 0.33241737 0.32945666\n",
      "200 0.33241737 0.32945666\n",
      "210 0.33241737 0.32945666\n",
      "220 0.33241737 0.32945666\n",
      "230 0.33241737 0.32945666\n",
      "240 0.33241737 0.32945666\n",
      "250 0.33241737 0.32945666\n",
      "260 0.33241737 0.32945666\n",
      "270 0.33241737 0.32945666\n",
      "280 0.33241737 0.32945666\n",
      "290 0.33241737 0.32945666\n",
      "300 0.33241737 0.32945666\n",
      "310 0.33241737 0.32945666\n",
      "320 0.33241737 0.32945666\n",
      "330 0.33241737 0.32945666\n",
      "340 0.33241737 0.32945666\n",
      "350 0.33241737 0.32945666\n",
      "360 0.33241737 0.32945666\n",
      "370 0.33241737 0.32945666\n",
      "380 0.33241737 0.32945666\n",
      "390 0.33241737 0.32945666\n",
      "400 0.33241737 0.32945666\n",
      "410 0.33241737 0.32945666\n",
      "420 0.33241737 0.32945666\n",
      "430 0.33241737 0.32945666\n",
      "440 0.33241737 0.32945666\n",
      "450 0.33241737 0.32945666\n",
      "460 0.33241737 0.32945666\n",
      "470 0.33241737 0.32945666\n",
      "480 0.33241737 0.32945666\n",
      "490 0.33241737 0.32945666\n",
      "Training done\n",
      "Final Training Cost: 0.332417368888855\n",
      "Final Testing Cost: 0.3294566571712494\n",
      "0 0.43989965 0.43726373\n",
      "10 0.12010754 0.11851206\n",
      "20 0.096053116 0.09491343\n",
      "30 0.089057036 0.08823545\n",
      "40 0.08541156 0.08472495\n",
      "50 0.08303259 0.08247078\n",
      "60 0.08163005 0.08132614\n",
      "70 0.07973833 0.07953337\n",
      "80 0.07649813 0.07651026\n",
      "90 0.07200324 0.072518475\n",
      "100 0.066899434 0.0680024\n",
      "110 0.060891565 0.062388275\n",
      "120 0.05520263 0.05702514\n",
      "130 0.049116965 0.051378746\n",
      "140 0.045056984 0.04720916\n",
      "150 0.041032135 0.043436706\n",
      "160 0.038885202 0.04114729\n",
      "170 0.03532129 0.03834495\n",
      "180 0.032797106 0.036272626\n",
      "190 0.030963963 0.03457322\n",
      "200 0.029246617 0.032864533\n",
      "210 0.028957603 0.032929968\n",
      "220 0.0272387 0.030660365\n",
      "230 0.025840405 0.02984036\n",
      "240 0.024136908 0.027929114\n",
      "250 0.025068996 0.02838945\n",
      "260 0.022531064 0.0263532\n",
      "270 0.021985205 0.025997395\n",
      "280 0.021239582 0.025156647\n",
      "290 0.020548869 0.024261793\n",
      "300 0.0233734 0.027507806\n",
      "310 0.020018857 0.023671135\n",
      "320 0.01949442 0.022895373\n",
      "330 0.018979672 0.022396874\n",
      "340 0.018633537 0.022100983\n",
      "350 0.0187477 0.022328\n",
      "360 0.017996645 0.021388894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 0.0186172 0.021781616\n",
      "380 0.018060546 0.021301895\n",
      "390 0.017421117 0.021007175\n",
      "400 0.016851906 0.020317784\n",
      "410 0.019140022 0.022277098\n",
      "420 0.016801441 0.020231357\n",
      "430 0.016759617 0.020446239\n",
      "440 0.01590882 0.019525593\n",
      "450 0.016878007 0.020299887\n",
      "460 0.016504101 0.020329967\n",
      "470 0.01531435 0.019016078\n",
      "480 0.016500542 0.020364428\n",
      "490 0.015513591 0.019162606\n",
      "Training done\n",
      "Final Training Cost: 0.014979911968111992\n",
      "Final Testing Cost: 0.018741684034466743\n",
      "0 16.372572 16.33058\n",
      "10 0.33241737 0.32945666\n",
      "20 0.33241737 0.32945666\n",
      "30 0.33241737 0.32945666\n",
      "40 0.33241737 0.32945666\n",
      "50 0.33241737 0.32945666\n",
      "60 0.33241737 0.32945666\n",
      "70 0.33241737 0.32945666\n",
      "80 0.33241737 0.32945666\n",
      "90 0.33241737 0.32945666\n",
      "100 0.33241737 0.32945666\n",
      "110 0.33241737 0.32945666\n",
      "120 0.33241737 0.32945666\n",
      "130 0.33241737 0.32945666\n",
      "140 0.33241737 0.32945666\n",
      "150 0.33241737 0.32945666\n",
      "160 0.33241737 0.32945666\n",
      "170 0.33241737 0.32945666\n",
      "180 0.33241737 0.32945666\n",
      "190 0.33241737 0.32945666\n",
      "200 0.33241737 0.32945666\n",
      "210 0.33241737 0.32945666\n",
      "220 0.33241737 0.32945666\n",
      "230 0.33241737 0.32945666\n",
      "240 0.33241737 0.32945666\n",
      "250 0.33241737 0.32945666\n",
      "260 0.33241737 0.32945666\n",
      "270 0.33241737 0.32945666\n",
      "280 0.33241737 0.32945666\n",
      "290 0.33241737 0.32945666\n",
      "300 0.33241737 0.32945666\n",
      "310 0.33241737 0.32945666\n",
      "320 0.33241737 0.32945666\n",
      "330 0.33241737 0.32945666\n",
      "340 0.33241737 0.32945666\n",
      "350 0.33241737 0.32945666\n",
      "360 0.33241737 0.32945666\n",
      "370 0.33241737 0.32945666\n",
      "380 0.33241737 0.32945666\n",
      "390 0.33241737 0.32945666\n",
      "400 0.33241737 0.32945666\n",
      "410 0.33241737 0.32945666\n",
      "420 0.33241737 0.32945666\n",
      "430 0.33241737 0.32945666\n",
      "440 0.33241737 0.32945666\n",
      "450 0.33241737 0.32945666\n",
      "460 0.33241737 0.32945666\n",
      "470 0.33241737 0.32945666\n",
      "480 0.33241737 0.32945666\n",
      "490 0.33241737 0.32945666\n",
      "Training done\n",
      "Final Training Cost: 0.332417368888855\n",
      "Final Testing Cost: 0.3294566571712494\n",
      "0 2.790995 2.7752357\n",
      "10 0.33241737 0.32945666\n",
      "20 0.33241737 0.32945666\n",
      "30 0.33241737 0.32945666\n",
      "40 0.33241737 0.32945666\n",
      "50 0.33241737 0.32945666\n",
      "60 0.33241737 0.32945666\n",
      "70 0.33241737 0.32945666\n",
      "80 0.33241737 0.32945666\n",
      "90 0.33241737 0.32945666\n",
      "100 0.33241737 0.32945666\n",
      "110 0.33241737 0.32945666\n",
      "120 0.33241737 0.32945666\n",
      "130 0.33241737 0.32945666\n",
      "140 0.33241737 0.32945666\n",
      "150 0.33241737 0.32945666\n",
      "160 0.33241737 0.32945666\n",
      "170 0.33241737 0.32945666\n",
      "180 0.33241737 0.32945666\n",
      "190 0.33241737 0.32945666\n",
      "200 0.33241737 0.32945666\n",
      "210 0.33241737 0.32945666\n",
      "220 0.33241737 0.32945666\n",
      "230 0.33241737 0.32945666\n",
      "240 0.33241737 0.32945666\n",
      "250 0.33241737 0.32945666\n",
      "260 0.33241737 0.32945666\n",
      "270 0.33241737 0.32945666\n",
      "280 0.33241737 0.32945666\n",
      "290 0.33241737 0.32945666\n",
      "300 0.33241737 0.32945666\n",
      "310 0.33241737 0.32945666\n",
      "320 0.33241737 0.32945666\n",
      "330 0.33241737 0.32945666\n",
      "340 0.33241737 0.32945666\n",
      "350 0.33241737 0.32945666\n",
      "360 0.33241737 0.32945666\n",
      "370 0.33241737 0.32945666\n",
      "380 0.33241737 0.32945666\n",
      "390 0.33241737 0.32945666\n",
      "400 0.33241737 0.32945666\n",
      "410 0.33241737 0.32945666\n",
      "420 0.33241737 0.32945666\n",
      "430 0.33241737 0.32945666\n",
      "440 0.33241737 0.32945666\n",
      "450 0.33241737 0.32945666\n",
      "460 0.33241737 0.32945666\n",
      "470 0.33241737 0.32945666\n",
      "480 0.33241737 0.32945666\n",
      "490 0.33241737 0.32945666\n",
      "Training done\n",
      "Final Training Cost: 0.332417368888855\n",
      "Final Testing Cost: 0.3294566571712494\n",
      "0 18.218597 18.148054\n",
      "10 0.33241737 0.32945666\n",
      "20 0.33241737 0.32945666\n",
      "30 0.33241737 0.32945666\n",
      "40 0.33241737 0.32945666\n",
      "50 0.33241737 0.32945666\n",
      "60 0.33241737 0.32945666\n",
      "70 0.33241737 0.32945666\n",
      "80 0.33241737 0.32945666\n",
      "90 0.33241737 0.32945666\n",
      "100 0.33241737 0.32945666\n",
      "110 0.33241737 0.32945666\n",
      "120 0.33241737 0.32945666\n",
      "130 0.33241737 0.32945666\n",
      "140 0.33241737 0.32945666\n",
      "150 0.33241737 0.32945666\n",
      "160 0.33241737 0.32945666\n",
      "170 0.33241737 0.32945666\n",
      "180 0.33241737 0.32945666\n",
      "190 0.33241737 0.32945666\n",
      "200 0.33241737 0.32945666\n",
      "210 0.33241737 0.32945666\n",
      "220 0.33241737 0.32945666\n",
      "230 0.33241737 0.32945666\n",
      "240 0.33241737 0.32945666\n",
      "250 0.33241737 0.32945666\n",
      "260 0.33241737 0.32945666\n",
      "270 0.33241737 0.32945666\n",
      "280 0.33241737 0.32945666\n",
      "290 0.33241737 0.32945666\n",
      "300 0.33241737 0.32945666\n",
      "310 0.33241737 0.32945666\n",
      "320 0.33241737 0.32945666\n",
      "330 0.33241737 0.32945666\n",
      "340 0.33241737 0.32945666\n",
      "350 0.33241737 0.32945666\n",
      "360 0.33241737 0.32945666\n",
      "370 0.33241737 0.32945666\n",
      "380 0.33241737 0.32945666\n",
      "390 0.33241737 0.32945666\n",
      "400 0.33241737 0.32945666\n",
      "410 0.33241737 0.32945666\n",
      "420 0.33241737 0.32945666\n",
      "430 0.33241737 0.32945666\n",
      "440 0.33241737 0.32945666\n",
      "450 0.33241737 0.32945666\n",
      "460 0.33241737 0.32945666\n",
      "470 0.33241737 0.32945666\n",
      "480 0.33241737 0.32945666\n",
      "490 0.33241737 0.32945666\n",
      "Training done\n",
      "Final Training Cost: 0.332417368888855\n",
      "Final Testing Cost: 0.3294566571712494\n",
      "0 0.33241737 0.32945666\n",
      "10 0.33241737 0.32945666\n",
      "20 0.33241737 0.32945666\n",
      "30 0.33241737 0.32945666\n",
      "40 0.33241737 0.32945666\n",
      "50 0.33241737 0.32945666\n",
      "60 0.33241737 0.32945666\n",
      "70 0.33241737 0.32945666\n",
      "80 0.33241737 0.32945666\n",
      "90 0.33241737 0.32945666\n",
      "100 0.33241737 0.32945666\n",
      "110 0.33241737 0.32945666\n",
      "120 0.33241737 0.32945666\n",
      "130 0.33241737 0.32945666\n",
      "140 0.33241737 0.32945666\n",
      "150 0.33241737 0.32945666\n",
      "160 0.33241737 0.32945666\n",
      "170 0.33241737 0.32945666\n",
      "180 0.33241737 0.32945666\n",
      "190 0.33241737 0.32945666\n",
      "200 0.33241737 0.32945666\n",
      "210 0.33241737 0.32945666\n",
      "220 0.33241737 0.32945666\n",
      "230 0.33241737 0.32945666\n",
      "240 0.33241737 0.32945666\n",
      "250 0.33241737 0.32945666\n",
      "260 0.33241737 0.32945666\n",
      "270 0.33241737 0.32945666\n",
      "280 0.33241737 0.32945666\n",
      "290 0.33241737 0.32945666\n",
      "300 0.33241737 0.32945666\n",
      "310 0.33241737 0.32945666\n",
      "320 0.33241737 0.32945666\n",
      "330 0.33241737 0.32945666\n",
      "340 0.33241737 0.32945666\n",
      "350 0.33241737 0.32945666\n",
      "360 0.33241737 0.32945666\n",
      "370 0.33241737 0.32945666\n",
      "380 0.33241737 0.32945666\n",
      "390 0.33241737 0.32945666\n",
      "400 0.33241737 0.32945666\n",
      "410 0.33241737 0.32945666\n",
      "420 0.33241737 0.32945666\n",
      "430 0.33241737 0.32945666\n",
      "440 0.33241737 0.32945666\n",
      "450 0.33241737 0.32945666\n",
      "460 0.33241737 0.32945666\n",
      "470 0.33241737 0.32945666\n",
      "480 0.33241737 0.32945666\n",
      "490 0.33241737 0.32945666\n",
      "Training done\n",
      "Final Training Cost: 0.332417368888855\n",
      "Final Testing Cost: 0.3294566571712494\n"
     ]
    }
   ],
   "source": [
    "for i, random_layer in enumerate(random_layers):\n",
    "    \n",
    "    nodes = np.fromstring(random_layer, sep = '_')\n",
    "    layer_1_nodes, layer_2_nodes, layer_3_nodes = int(nodes[0]), int(nodes[1]), int(nodes[2])\n",
    "    f(layer_1_nodes,layer_2_nodes,layer_2_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:16:07.028495Z",
     "start_time": "2018-05-25T05:04:57.412837Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 1.092518 1.0838659\n",
      "10 0.108982146 0.106353134\n",
      "20 0.09020366 0.088460326\n",
      "30 0.08680518 0.0853833\n",
      "40 0.08346019 0.08243761\n",
      "50 0.08148037 0.08079836\n",
      "60 0.077842735 0.07809026\n",
      "70 0.073062144 0.07414793\n",
      "80 0.06746452 0.06923722\n",
      "90 0.0619455 0.06465176\n",
      "100 0.0529656 0.056200363\n",
      "110 0.042191207 0.045547165\n",
      "120 0.050681308 0.054325454\n",
      "130 0.03105121 0.033514332\n",
      "140 0.018628463 0.021141188\n",
      "150 0.013243215 0.015516875\n",
      "160 0.009899633 0.011699475\n",
      "170 0.008121009 0.009827097\n",
      "180 0.0075607575 0.009084788\n",
      "190 0.005954163 0.007558496\n",
      "Training done\n",
      "Final Training Cost: 0.004937790334224701\n",
      "Final Testing Cost: 0.006422697100788355\n",
      "1\n",
      "0 0.6492391 0.64567804\n",
      "10 0.09879422 0.09854036\n",
      "20 0.091977045 0.09185806\n",
      "30 0.08558762 0.085616134\n",
      "40 0.08297661 0.08362246\n",
      "50 0.078740925 0.07986132\n",
      "60 0.071908355 0.073281206\n",
      "70 0.063247174 0.06534017\n",
      "80 0.052824393 0.055335175\n",
      "90 0.045228273 0.04708666\n",
      "100 0.03900267 0.040696442\n",
      "110 0.035284553 0.03722091\n",
      "120 0.032055214 0.03375241\n",
      "130 0.02923407 0.03145053\n",
      "140 0.02684021 0.029172013\n",
      "150 0.024777517 0.027092991\n",
      "160 0.02320663 0.025818722\n",
      "170 0.021907955 0.024601325\n",
      "180 0.020243004 0.022815503\n",
      "190 0.018936388 0.021614745\n",
      "Training done\n",
      "Final Training Cost: 0.018395651131868362\n",
      "Final Testing Cost: 0.021323857828974724\n",
      "2\n",
      "0 1.9632062 1.9541072\n",
      "10 0.09213474 0.090656266\n",
      "20 0.09356728 0.09342368\n",
      "30 0.085093096 0.084329136\n",
      "40 0.084288314 0.08382062\n",
      "50 0.08224611 0.08281434\n",
      "60 0.07975589 0.08096919\n",
      "70 0.07711446 0.078940965\n",
      "80 0.073853806 0.075946815\n",
      "90 0.06886606 0.07139809\n",
      "100 0.060855225 0.064161554\n",
      "110 0.050031327 0.054058712\n",
      "120 0.040881805 0.045379892\n",
      "130 0.032948107 0.03718089\n",
      "140 0.028420804 0.03244022\n",
      "150 0.022868102 0.026202979\n",
      "160 0.019072186 0.022422291\n",
      "170 0.016797153 0.019923154\n",
      "180 0.015634803 0.018739546\n",
      "190 0.014915641 0.017794626\n",
      "Training done\n",
      "Final Training Cost: 0.013648257590830326\n",
      "Final Testing Cost: 0.016485540196299553\n",
      "3\n",
      "0 0.1815773 0.18078019\n",
      "10 0.09889339 0.09833752\n",
      "20 0.08648158 0.08804497\n",
      "30 0.08438465 0.08478945\n",
      "40 0.08085608 0.08208087\n",
      "50 0.07710312 0.078506835\n",
      "60 0.07104956 0.0728282\n",
      "70 0.064026214 0.06585114\n",
      "80 0.05642119 0.059097502\n",
      "90 0.050336298 0.053400498\n",
      "100 0.042345263 0.045820404\n",
      "110 0.034726907 0.038593285\n",
      "120 0.029866712 0.033745013\n",
      "130 0.026676549 0.03041215\n",
      "140 0.024343174 0.02814322\n",
      "150 0.022355912 0.026244175\n",
      "160 0.021230042 0.02497107\n",
      "170 0.0196786 0.02365913\n",
      "180 0.018493684 0.022550609\n",
      "190 0.017783737 0.021699643\n",
      "Training done\n",
      "Final Training Cost: 0.01622125320136547\n",
      "Final Testing Cost: 0.02025928907096386\n",
      "4\n",
      "0 0.9470739 0.9589589\n",
      "10 0.10308899 0.10226963\n",
      "20 0.085445665 0.08404036\n",
      "30 0.086411186 0.085352145\n",
      "40 0.08298545 0.0822919\n",
      "50 0.08128114 0.080893286\n",
      "60 0.07900992 0.07900574\n",
      "70 0.07540398 0.0755661\n",
      "80 0.069163404 0.06912372\n",
      "90 0.05861226 0.05818099\n",
      "100 0.046696085 0.04564026\n",
      "110 0.037105713 0.035717253\n",
      "120 0.03088894 0.029763673\n",
      "130 0.025732793 0.025131509\n",
      "140 0.021776829 0.021529973\n",
      "150 0.018636696 0.01854454\n",
      "160 0.016138114 0.01618911\n",
      "170 0.013982394 0.01420717\n",
      "180 0.012752753 0.013381992\n",
      "190 0.0109733315 0.011670194\n",
      "Training done\n",
      "Final Training Cost: 0.009729776531457901\n",
      "Final Testing Cost: 0.010492119938135147\n",
      "5\n",
      "0 4.562634 4.5521626\n",
      "10 0.12689915 0.12460291\n",
      "20 0.09736635 0.09512979\n",
      "30 0.08915019 0.08692377\n",
      "40 0.08608954 0.08370517\n",
      "50 0.08452478 0.082048\n",
      "60 0.081212305 0.078382805\n",
      "70 0.07841359 0.07529013\n",
      "80 0.07589564 0.07300721\n",
      "90 0.07326098 0.07076879\n",
      "100 0.07004411 0.06821399\n",
      "110 0.066205464 0.06515802\n",
      "120 0.06421052 0.06357119\n",
      "130 0.058395825 0.058590308\n",
      "140 0.05252803 0.053908154\n",
      "150 0.047147263 0.050035797\n",
      "160 0.042293623 0.046383575\n",
      "170 0.03957128 0.04419799\n",
      "180 0.038839363 0.043525904\n",
      "190 0.036824632 0.041656107\n",
      "Training done\n",
      "Final Training Cost: 0.03527113422751427\n",
      "Final Testing Cost: 0.0401744619011879\n",
      "6\n",
      "0 0.6429249 0.64761204\n",
      "10 0.115273386 0.11370111\n",
      "20 0.089252494 0.08807532\n",
      "30 0.085636504 0.08529922\n",
      "40 0.08432029 0.08442722\n",
      "50 0.08340428 0.08370207\n",
      "60 0.082257554 0.08280018\n",
      "70 0.08071771 0.08154584\n",
      "80 0.07859866 0.079939485\n",
      "90 0.07527149 0.077399805\n",
      "100 0.07025346 0.0732869\n",
      "110 0.06274661 0.06601402\n",
      "120 0.05380576 0.056521595\n",
      "130 0.043349594 0.046007324\n",
      "140 0.037211675 0.040074304\n",
      "150 0.031993344 0.035205547\n",
      "160 0.028484566 0.031974647\n",
      "170 0.025716824 0.029420504\n",
      "180 0.023398492 0.027104832\n",
      "190 0.021588532 0.025281066\n",
      "Training done\n",
      "Final Training Cost: 0.02043600007891655\n",
      "Final Testing Cost: 0.023890458047389984\n",
      "7\n",
      "0 1.5858312 1.5803723\n",
      "10 0.112651825 0.11076065\n",
      "20 0.08688644 0.08536627\n",
      "30 0.086815126 0.0853628\n",
      "40 0.08638355 0.085051514\n",
      "50 0.08548383 0.084217586\n",
      "60 0.08437246 0.08327975\n",
      "70 0.08193323 0.08112178\n",
      "80 0.07696949 0.077361554\n",
      "90 0.0729482 0.074553035\n",
      "100 0.07035947 0.072221346\n",
      "110 0.06823815 0.0706494\n",
      "120 0.065926984 0.06850401\n",
      "130 0.06340267 0.0664768\n",
      "140 0.059787475 0.063439764\n",
      "150 0.05479396 0.05877138\n",
      "160 0.048572253 0.052883934\n",
      "170 0.042884316 0.04766954\n",
      "180 0.039492916 0.043910958\n",
      "190 0.039076004 0.043784633\n",
      "Training done\n",
      "Final Training Cost: 0.034690845757722855\n",
      "Final Testing Cost: 0.0397028811275959\n",
      "8\n",
      "0 0.43793908 0.43459415\n",
      "10 0.09723788 0.09566122\n",
      "20 0.08667308 0.08557451\n",
      "30 0.08448129 0.08384757\n",
      "40 0.082352705 0.08187167\n",
      "50 0.08006395 0.07974686\n",
      "60 0.07601729 0.07619169\n",
      "70 0.06926571 0.06990638\n",
      "80 0.0603003 0.060904328\n",
      "90 0.046877477 0.04910279\n",
      "100 0.036324233 0.039596777\n",
      "110 0.028761875 0.031836882\n",
      "120 0.023981 0.026921462\n",
      "130 0.020464268 0.023510823\n",
      "140 0.017985078 0.020782111\n",
      "150 0.016348887 0.018871501\n",
      "160 0.015092502 0.017456338\n",
      "170 0.014088051 0.016331427\n",
      "180 0.013214242 0.015391566\n",
      "190 0.012394987 0.014508528\n",
      "Training done\n",
      "Final Training Cost: 0.012195674702525139\n",
      "Final Testing Cost: 0.014186169020831585\n",
      "9\n",
      "0 1.6074646 1.6016943\n",
      "10 0.11924327 0.11674169\n",
      "20 0.096347176 0.09457074\n",
      "30 0.088314086 0.087236464\n",
      "40 0.08588673 0.08513851\n",
      "50 0.085349366 0.08469316\n",
      "60 0.08430023 0.083574764\n",
      "70 0.08382129 0.08309172\n",
      "80 0.083259255 0.08263556\n",
      "90 0.08263483 0.082040556\n",
      "100 0.08182678 0.08131554\n",
      "110 0.08078179 0.08033128\n",
      "120 0.0794186 0.07897532\n",
      "130 0.07782342 0.07732913\n",
      "140 0.076145194 0.075721495\n",
      "150 0.07444931 0.07422958\n",
      "160 0.07268446 0.07282733\n",
      "170 0.07098809 0.07145015\n",
      "180 0.069411114 0.07004793\n",
      "190 0.06803246 0.06886986\n",
      "Training done\n",
      "Final Training Cost: 0.06684865057468414\n",
      "Final Testing Cost: 0.0679440125823021\n"
     ]
    }
   ],
   "source": [
    "layer_1_node_vals = np.arange(15,90,15)\n",
    "layer_2_node_vals = np.arange(300,500,50)\n",
    "layer_3_node_vals = np.arange(200,450,50)\n",
    "\n",
    "log_paths = []\n",
    "for layer_1_nodes in layer_1_node_vals:\n",
    "    for layer_2_nodes in layer_2_node_vals:\n",
    "        for layer_3_nodes in layer_3_node_vals:\n",
    "            \n",
    "            log_path = str(layer_1_nodes) + \"_\" + str(layer_2_nodes) +  \"_\" + str(layer_3_nodes)\n",
    "                \n",
    "            log_paths.append(log_path)\n",
    "            \n",
    "random_layers = np.random.choice(log_paths, size = 10)\n",
    "\n",
    "for i, random_layer in enumerate(random_layers):\n",
    "    print(i)\n",
    "    \n",
    "    nodes = np.fromstring(random_layer, sep = '_')\n",
    "    layer_1_nodes, layer_2_nodes, layer_3_nodes = int(nodes[0]), int(nodes[1]), int(nodes[2])\n",
    "    f(layer_1_nodes,layer_2_nodes,layer_2_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:53:40.977450Z",
     "start_time": "2018-05-25T05:40:52.430115Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 0.77498907 0.766237\n",
      "10 0.09732775 0.093258716\n",
      "20 0.09164944 0.089056075\n",
      "30 0.08678006 0.08460295\n",
      "40 0.0853721 0.08353206\n",
      "50 0.08372381 0.082156554\n",
      "60 0.08157689 0.08042889\n",
      "70 0.07996355 0.07913939\n",
      "80 0.07843379 0.077443115\n",
      "90 0.0774434 0.076525755\n",
      "100 0.076402836 0.07567772\n",
      "110 0.07583565 0.07534523\n",
      "120 0.07510067 0.07481857\n",
      "130 0.07463617 0.074657716\n",
      "140 0.07351124 0.073775515\n",
      "150 0.072000355 0.07243667\n",
      "160 0.07026247 0.07099222\n",
      "170 0.06803308 0.06946418\n",
      "180 0.0663775 0.06843388\n",
      "190 0.06284814 0.065232635\n",
      "1\n",
      "0 0.42659283 0.4205511\n",
      "10 0.09563686 0.09650145\n",
      "20 0.088516675 0.08803746\n",
      "30 0.08608386 0.08558406\n",
      "40 0.084617816 0.0836542\n",
      "50 0.083297074 0.082246006\n",
      "60 0.082822345 0.08180079\n",
      "70 0.08241768 0.08133022\n",
      "80 0.08191017 0.080967315\n",
      "90 0.08120527 0.08032438\n",
      "100 0.08012067 0.079333365\n",
      "110 0.078348815 0.07771081\n",
      "120 0.07546703 0.07511015\n",
      "130 0.07101743 0.07097125\n",
      "140 0.06487825 0.064920515\n",
      "150 0.058771636 0.058537636\n",
      "160 0.052984554 0.052843552\n",
      "170 0.049992777 0.050095025\n",
      "180 0.04787533 0.047201976\n",
      "190 0.0435622 0.043549918\n",
      "2\n",
      "0 0.28592065 0.2824039\n",
      "10 0.10430103 0.10246382\n",
      "20 0.09037059 0.08770014\n",
      "30 0.08642266 0.084199354\n",
      "40 0.084822 0.08291106\n",
      "50 0.08368122 0.08157717\n",
      "60 0.08096323 0.078757145\n",
      "70 0.07918343 0.07742573\n",
      "80 0.077304736 0.07584123\n",
      "90 0.07529215 0.07380958\n",
      "100 0.07275009 0.071578614\n",
      "110 0.06922192 0.0684883\n",
      "120 0.06416089 0.06389501\n",
      "130 0.057877902 0.057814967\n",
      "140 0.05078287 0.050863914\n",
      "150 0.054746117 0.05633024\n",
      "160 0.04281215 0.042669352\n",
      "170 0.041291144 0.04072464\n",
      "180 0.038744636 0.038935885\n",
      "190 0.036586054 0.03636761\n",
      "3\n",
      "0 0.33185965 0.33168718\n",
      "10 0.33185965 0.33168718\n",
      "20 0.33185965 0.33168718\n",
      "30 0.33185965 0.33168718\n",
      "40 0.33185965 0.33168718\n",
      "50 0.33185965 0.33168718\n",
      "60 0.33185965 0.33168718\n",
      "70 0.33185965 0.33168718\n",
      "80 0.33185965 0.33168718\n",
      "90 0.33185965 0.33168718\n",
      "100 0.33185965 0.33168718\n",
      "110 0.33185965 0.33168718\n",
      "120 0.33185965 0.33168718\n",
      "130 0.33185965 0.33168718\n",
      "140 0.33185965 0.33168718\n",
      "150 0.33185965 0.33168718\n",
      "160 0.33185965 0.33168718\n",
      "170 0.33185965 0.33168718\n",
      "180 0.33185965 0.33168718\n",
      "190 0.33185965 0.33168718\n",
      "4\n",
      "0 1.6539145 1.6600379\n",
      "10 0.10582142 0.103946544\n",
      "20 0.08870106 0.08705857\n",
      "30 0.08748541 0.08581716\n",
      "40 0.086384095 0.084603\n",
      "50 0.085077114 0.08332651\n",
      "60 0.08382412 0.08216054\n",
      "70 0.081710555 0.0801681\n",
      "80 0.0786357 0.0773794\n",
      "90 0.07766583 0.07655044\n",
      "100 0.075109586 0.074439466\n",
      "110 0.07181441 0.071107306\n",
      "120 0.067747585 0.06716037\n",
      "130 0.062065315 0.061664384\n",
      "140 0.05425275 0.0542065\n",
      "150 0.04442568 0.04481907\n",
      "160 0.034918495 0.035497025\n",
      "170 0.028526308 0.02987106\n",
      "180 0.023431761 0.02505534\n",
      "190 0.021109719 0.0226665\n",
      "5\n",
      "0 0.22211707 0.22206447\n",
      "10 0.10816226 0.104351215\n",
      "20 0.09101507 0.08799403\n",
      "30 0.086055264 0.08386961\n",
      "40 0.084671386 0.08261704\n",
      "50 0.0830569 0.08111197\n",
      "60 0.08018718 0.07885049\n",
      "70 0.07961917 0.07860107\n",
      "80 0.07702894 0.07599341\n",
      "90 0.07555035 0.07466149\n",
      "100 0.07459538 0.073813215\n",
      "110 0.07333259 0.07278355\n",
      "120 0.071759015 0.07158699\n",
      "130 0.06965442 0.069800764\n",
      "140 0.06642069 0.066913866\n",
      "150 0.061315894 0.06221892\n",
      "160 0.05523577 0.05681856\n",
      "170 0.051794183 0.053338807\n",
      "180 0.047427338 0.05041269\n",
      "190 0.044351332 0.04777162\n",
      "6\n",
      "0 0.7784263 0.7727786\n",
      "10 0.09665554 0.09531471\n",
      "20 0.08815563 0.08703276\n",
      "30 0.08455211 0.08311044\n",
      "40 0.08248419 0.08101501\n",
      "50 0.07877315 0.077152215\n",
      "60 0.07363736 0.07197896\n",
      "70 0.066471316 0.064180635\n",
      "80 0.057396933 0.055534687\n",
      "90 0.047599368 0.04698354\n",
      "100 0.04046044 0.040645722\n",
      "110 0.034593284 0.03517619\n",
      "120 0.030269263 0.031414982\n",
      "130 0.025312213 0.026664604\n",
      "140 0.022692623 0.024485271\n",
      "150 0.020111186 0.022015898\n",
      "160 0.018595085 0.020757938\n",
      "170 0.015871942 0.01803619\n",
      "180 0.0148732625 0.016747052\n",
      "190 0.013425921 0.015565036\n",
      "7\n",
      "0 0.094208024 0.09330954\n",
      "10 0.09200305 0.08859799\n",
      "20 0.09180136 0.08911433\n",
      "30 0.08815375 0.08601196\n",
      "40 0.08582486 0.08388917\n",
      "50 0.083443984 0.08168912\n",
      "60 0.0818815 0.08028634\n",
      "70 0.079751976 0.07813754\n",
      "80 0.07681535 0.075358175\n",
      "90 0.072139226 0.071083896\n",
      "100 0.06392642 0.06301248\n",
      "110 0.05316283 0.05139787\n",
      "120 0.04614932 0.043504227\n",
      "130 0.04286181 0.039896872\n",
      "140 0.03932666 0.03729939\n",
      "150 0.03744878 0.0355787\n",
      "160 0.03611859 0.034335528\n",
      "170 0.034708083 0.033617187\n",
      "180 0.033534516 0.032551937\n",
      "190 0.032493893 0.031781245\n",
      "8\n",
      "0 2.1615832 2.1522868\n",
      "10 0.20128144 0.20112082\n",
      "20 0.11014303 0.108938836\n",
      "30 0.09155697 0.089898854\n",
      "40 0.0849451 0.083232164\n",
      "50 0.08308 0.081405245\n",
      "60 0.0827319 0.08109622\n",
      "70 0.082582824 0.08096786\n",
      "80 0.08240722 0.08077609\n",
      "90 0.08218329 0.080529466\n",
      "100 0.08186144 0.0801625\n",
      "110 0.08140984 0.07963127\n",
      "120 0.080886856 0.07906264\n",
      "130 0.08032369 0.078531936\n",
      "140 0.07969233 0.077995434\n",
      "150 0.07891401 0.077375695\n",
      "160 0.0779778 0.076672286\n",
      "170 0.07659056 0.07531186\n",
      "180 0.074272655 0.07342388\n",
      "190 0.07240677 0.07169078\n",
      "9\n",
      "0 2.506654 2.5203454\n",
      "10 0.33185965 0.33168718\n",
      "20 0.33185965 0.33168718\n",
      "30 0.33185965 0.33168718\n",
      "40 0.33185965 0.33168718\n",
      "50 0.33185965 0.33168718\n",
      "60 0.33185965 0.33168718\n",
      "70 0.33185965 0.33168718\n",
      "80 0.33185965 0.33168718\n",
      "90 0.33185965 0.33168718\n",
      "100 0.33185965 0.33168718\n",
      "110 0.33185965 0.33168718\n",
      "120 0.33185965 0.33168718\n",
      "130 0.33185965 0.33168718\n",
      "140 0.33185965 0.33168718\n",
      "150 0.33185965 0.33168718\n",
      "160 0.33185965 0.33168718\n",
      "170 0.33185965 0.33168718\n",
      "180 0.33185965 0.33168718\n",
      "190 0.33185965 0.33168718\n"
     ]
    }
   ],
   "source": [
    "layer_1_node_vals = np.arange(15,90,15)\n",
    "layer_2_node_vals = np.arange(300,500,50)\n",
    "layer_3_node_vals = np.arange(200,450,50)\n",
    "\n",
    "log_paths = []\n",
    "for layer_1_nodes in layer_1_node_vals:\n",
    "    for layer_2_nodes in layer_2_node_vals:\n",
    "        for layer_3_nodes in layer_3_node_vals:\n",
    "            \n",
    "            log_path = str(layer_1_nodes) + \"_\" + str(layer_2_nodes) +  \"_\" + str(layer_3_nodes)\n",
    "                \n",
    "            log_paths.append(log_path)\n",
    "            \n",
    "random_layers = np.random.choice(log_paths, size = 10)\n",
    "\n",
    "for i, random_layer in enumerate(random_layers):\n",
    "    print(i)\n",
    "    \n",
    "    nodes = np.fromstring(random_layer, sep = '_')\n",
    "    layer_1_nodes, layer_2_nodes, layer_3_nodes = int(nodes[0]), int(nodes[1]), int(nodes[2])\n",
    "    f(layer_1_nodes,layer_2_nodes,layer_2_nodes, logdir = 'Run_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:56:45.150649Z",
     "start_time": "2018-05-25T05:56:45.137521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': ['logging/current_cost'], 'distributions': [], 'tensors': [], 'graph': True, 'meta_graph': True, 'run_metadata': []}\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "event_acc = EventAccumulator('logs/Run_1/50_400_300/testing/events.out.tfevents.1527222922.Andrews-MacBook.local')\n",
    "event_acc.Reload()\n",
    "# Show all tags in the log file\n",
    "print(event_acc.Tags())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T06:39:21.365813Z",
     "start_time": "2018-05-25T06:18:24.856567Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 0.7841674 0.7849297\n",
      "10 0.14716199 0.14669153\n",
      "20 0.09487278 0.09344031\n",
      "30 0.08514947 0.08347885\n",
      "40 0.08381155 0.08220157\n",
      "50 0.08310977 0.081511\n",
      "60 0.08297729 0.08136915\n",
      "70 0.082795136 0.081228055\n",
      "80 0.082740076 0.081145175\n",
      "90 0.08272541 0.08114774\n",
      "100 0.082718104 0.08113066\n",
      "110 0.08271497 0.08113024\n",
      "120 0.08271357 0.081131\n",
      "130 0.082711354 0.08112723\n",
      "140 0.082709834 0.08112593\n",
      "150 0.0827084 0.08112529\n",
      "160 0.08270721 0.08112431\n",
      "170 0.082705945 0.08112306\n",
      "180 0.08270476 0.08112198\n",
      "190 0.08270347 0.08112082\n",
      "1\n",
      "0 0.7929619 0.7955028\n",
      "10 0.10155539 0.103805326\n",
      "20 0.09930985 0.10043191\n",
      "30 0.08828131 0.08849667\n",
      "40 0.08401255 0.08363603\n",
      "50 0.08315819 0.08224492\n",
      "60 0.08271019 0.08146532\n",
      "70 0.08245946 0.0812224\n",
      "80 0.08226578 0.080845386\n",
      "90 0.08206681 0.08063574\n",
      "100 0.08184552 0.08036759\n",
      "110 0.08153705 0.08008336\n",
      "120 0.08110127 0.079707265\n",
      "130 0.080407925 0.079123676\n",
      "140 0.07936334 0.07824754\n",
      "150 0.07783441 0.077020876\n",
      "160 0.075619444 0.07520778\n",
      "170 0.07234781 0.07248872\n",
      "180 0.0676082 0.06848089\n",
      "190 0.06643039 0.06838599\n",
      "2\n",
      "0 3.8603992 3.879703\n",
      "10 0.11047429 0.10983656\n",
      "20 0.101568714 0.1020734\n",
      "30 0.08776448 0.08697114\n",
      "40 0.08484546 0.08412549\n",
      "50 0.083295956 0.082698576\n",
      "60 0.08188803 0.08144366\n",
      "70 0.07999145 0.079767734\n",
      "80 0.0773012 0.07742593\n",
      "90 0.07332361 0.07395442\n",
      "100 0.0671811 0.068438105\n",
      "110 0.05788117 0.059861682\n",
      "120 0.045677718 0.048155572\n",
      "130 0.033721656 0.036096383\n",
      "140 0.025086429 0.027080106\n",
      "150 0.018940566 0.020599267\n",
      "160 0.014752066 0.016202427\n",
      "170 0.0116873775 0.013045205\n",
      "180 0.009636027 0.010968428\n",
      "190 0.0085753435 0.009945428\n",
      "3\n",
      "0 3.9484563 3.9378474\n",
      "10 0.09512317 0.09168773\n",
      "20 0.09616514 0.09400045\n",
      "30 0.09138467 0.08941159\n",
      "40 0.08610989 0.0844325\n",
      "50 0.08339194 0.08201216\n",
      "60 0.08166463 0.0804669\n",
      "70 0.07909437 0.078123644\n",
      "80 0.07754314 0.07649054\n",
      "90 0.07599282 0.07480454\n",
      "100 0.07427491 0.07286404\n",
      "110 0.07269246 0.07107361\n",
      "120 0.071189985 0.06982455\n",
      "130 0.06938889 0.06820169\n",
      "140 0.067238726 0.06601785\n",
      "150 0.06428464 0.062829405\n",
      "160 0.06086131 0.06027435\n",
      "170 0.05633633 0.05588069\n",
      "180 0.050915267 0.05069429\n",
      "190 0.04655394 0.04620933\n",
      "4\n",
      "0 0.33185965 0.33168718\n",
      "10 0.33185965 0.33168718\n",
      "20 0.33185965 0.33168718\n",
      "30 0.33185965 0.33168718\n",
      "40 0.33185965 0.33168718\n",
      "50 0.33185965 0.33168718\n",
      "60 0.33185965 0.33168718\n",
      "70 0.33185965 0.33168718\n",
      "80 0.33185965 0.33168718\n",
      "90 0.33185965 0.33168718\n",
      "100 0.33185965 0.33168718\n",
      "110 0.33185965 0.33168718\n",
      "120 0.33185965 0.33168718\n",
      "130 0.33185965 0.33168718\n",
      "140 0.33185965 0.33168718\n",
      "150 0.33185965 0.33168718\n",
      "160 0.33185965 0.33168718\n",
      "170 0.33185965 0.33168718\n",
      "180 0.33185965 0.33168718\n",
      "190 0.33185965 0.33168718\n",
      "5\n",
      "0 1.7456439 1.749606\n",
      "10 0.08825728 0.08750881\n",
      "20 0.09155816 0.09041521\n",
      "30 0.085730985 0.08454211\n",
      "40 0.08563562 0.08447397\n",
      "50 0.08387172 0.08277131\n",
      "60 0.08280025 0.081827424\n",
      "70 0.08143701 0.08052999\n",
      "80 0.0795158 0.07875664\n",
      "90 0.076566525 0.076074176\n",
      "100 0.07189915 0.07200119\n",
      "110 0.06476499 0.0654086\n",
      "120 0.055888962 0.056192897\n",
      "130 0.048441518 0.048893664\n",
      "140 0.043008447 0.044061184\n",
      "150 0.035882086 0.036831614\n",
      "160 0.03071886 0.031512607\n",
      "170 0.025703583 0.026662773\n",
      "180 0.021960752 0.023000179\n",
      "190 0.019173928 0.020272816\n",
      "6\n",
      "0 4.277435 4.273694\n",
      "10 0.09000775 0.08890517\n",
      "20 0.08737553 0.08646527\n",
      "30 0.08687413 0.0861914\n",
      "40 0.08462469 0.08395233\n",
      "50 0.08157583 0.081097156\n",
      "60 0.07705924 0.07717574\n",
      "70 0.07054263 0.071126096\n",
      "80 0.06264918 0.06436722\n",
      "90 0.06272393 0.06417531\n",
      "100 0.046965126 0.049956847\n",
      "110 0.043846086 0.045152985\n",
      "120 0.036216307 0.039706483\n",
      "130 0.024903432 0.026605006\n",
      "140 0.021701721 0.02303526\n",
      "150 0.016097698 0.017498925\n",
      "160 0.014759943 0.016620705\n",
      "170 0.011121102 0.012475481\n",
      "180 0.011935547 0.012808356\n",
      "190 0.008617829 0.009773784\n",
      "7\n",
      "0 2.9318807 2.9246228\n",
      "10 0.09392601 0.09156465\n",
      "20 0.08633077 0.08418497\n",
      "30 0.08456996 0.08275497\n",
      "40 0.08226137 0.080740996\n",
      "50 0.07835587 0.07752475\n",
      "60 0.0722145 0.07200684\n",
      "70 0.0662923 0.06684689\n",
      "80 0.060235932 0.061207004\n",
      "90 0.056080215 0.057482366\n",
      "100 0.051296372 0.05283159\n",
      "110 0.04461558 0.046587512\n",
      "120 0.03762915 0.039700612\n",
      "130 0.03102657 0.034175787\n",
      "140 0.023865845 0.026807625\n",
      "150 0.02037389 0.022897188\n",
      "160 0.017071515 0.019561077\n",
      "170 0.014051043 0.016607046\n",
      "180 0.0124019105 0.014545638\n",
      "190 0.0108477175 0.013305231\n",
      "8\n",
      "0 0.097740084 0.0977104\n",
      "10 0.09848212 0.09709505\n",
      "20 0.09263793 0.09102753\n",
      "30 0.086369716 0.0846227\n",
      "40 0.085548185 0.08388761\n",
      "50 0.08491256 0.08334126\n",
      "60 0.08414308 0.082798526\n",
      "70 0.08254848 0.08143423\n",
      "80 0.07951901 0.078409515\n",
      "90 0.0794679 0.07873908\n",
      "100 0.07727351 0.07648498\n",
      "110 0.07629464 0.07536057\n",
      "120 0.07549117 0.07449838\n",
      "130 0.07509126 0.0741281\n",
      "140 0.074566424 0.07365506\n",
      "150 0.07409908 0.07326926\n",
      "160 0.073665634 0.07288842\n",
      "170 0.07328377 0.07255993\n",
      "180 0.07290144 0.07227298\n",
      "190 0.0728243 0.07230968\n",
      "9\n",
      "0 0.33185965 0.33168718\n",
      "10 0.33185965 0.33168718\n",
      "20 0.33185965 0.33168718\n",
      "30 0.33185965 0.33168718\n",
      "40 0.33185965 0.33168718\n",
      "50 0.33185965 0.33168718\n",
      "60 0.33185965 0.33168718\n",
      "70 0.33185965 0.33168718\n",
      "80 0.33185965 0.33168718\n",
      "90 0.33185965 0.33168718\n",
      "100 0.33185965 0.33168718\n",
      "110 0.33185965 0.33168718\n",
      "120 0.33185965 0.33168718\n",
      "130 0.33185965 0.33168718\n",
      "140 0.33185965 0.33168718\n",
      "150 0.33185965 0.33168718\n",
      "160 0.33185965 0.33168718\n",
      "170 0.33185965 0.33168718\n",
      "180 0.33185965 0.33168718\n",
      "190 0.33185965 0.33168718\n",
      "10\n",
      "0 4.420842 4.4220614\n",
      "10 0.10588656 0.10497545\n",
      "20 0.08853534 0.08615536\n",
      "30 0.086802736 0.08469819\n",
      "40 0.08523393 0.08321956\n",
      "50 0.08226896 0.08051834\n",
      "60 0.08023183 0.078495495\n",
      "70 0.076553755 0.075164124\n",
      "80 0.072225824 0.071284436\n",
      "90 0.06733739 0.06715661\n",
      "100 0.06137567 0.061840218\n",
      "110 0.054524682 0.055737946\n",
      "120 0.045557037 0.047492303\n",
      "130 0.03984891 0.041899398\n",
      "140 0.028494976 0.030111946\n",
      "150 0.02271773 0.024560196\n",
      "160 0.018742865 0.020410487\n",
      "170 0.013369568 0.014926338\n",
      "180 0.010722109 0.01225371\n",
      "190 0.010611767 0.012212246\n",
      "11\n",
      "0 1.2503313 1.2615635\n",
      "10 0.09298954 0.09052023\n",
      "20 0.091272615 0.08941697\n",
      "30 0.08581433 0.08369226\n",
      "40 0.082688764 0.08042123\n",
      "50 0.076601654 0.07490596\n",
      "60 0.070313014 0.06937142\n",
      "70 0.063774824 0.0640018\n",
      "80 0.05795513 0.058871944\n",
      "90 0.05167394 0.052975502\n",
      "100 0.04510919 0.046634037\n",
      "110 0.03812 0.039326705\n",
      "120 0.03151061 0.03289931\n",
      "130 0.025870781 0.027678818\n",
      "140 0.022519836 0.024374988\n",
      "150 0.018701231 0.020390568\n",
      "160 0.016932476 0.018714624\n",
      "170 0.013675533 0.015119688\n",
      "180 0.011963682 0.01335342\n",
      "190 0.012925727 0.014098894\n",
      "12\n",
      "0 0.4800713 0.48135808\n",
      "10 0.10307408 0.09925513\n",
      "20 0.08941856 0.08680635\n",
      "30 0.08576437 0.083589636\n",
      "40 0.08378536 0.08164604\n",
      "50 0.082922824 0.0811207\n",
      "60 0.08253216 0.08091918\n",
      "70 0.082118966 0.08057932\n",
      "80 0.081453994 0.07999507\n",
      "90 0.0804347 0.0790784\n",
      "100 0.078839466 0.077682726\n",
      "110 0.07606018 0.07519585\n",
      "120 0.071269915 0.070920475\n",
      "130 0.06442364 0.06488216\n",
      "140 0.057570618 0.058968566\n",
      "150 0.052208222 0.05502659\n",
      "160 0.04788958 0.05242788\n",
      "170 0.050789803 0.055473648\n",
      "180 0.042771593 0.047839765\n",
      "190 0.042315193 0.047303446\n",
      "13\n",
      "0 0.33185965 0.33168718\n",
      "10 0.33185965 0.33168718\n",
      "20 0.33185965 0.33168718\n",
      "30 0.33185965 0.33168718\n",
      "40 0.33185965 0.33168718\n",
      "50 0.33185965 0.33168718\n",
      "60 0.33185965 0.33168718\n",
      "70 0.33185965 0.33168718\n",
      "80 0.33185965 0.33168718\n",
      "90 0.33185965 0.33168718\n",
      "100 0.33185965 0.33168718\n",
      "110 0.33185965 0.33168718\n",
      "120 0.33185965 0.33168718\n",
      "130 0.33185965 0.33168718\n",
      "140 0.33185965 0.33168718\n",
      "150 0.33185965 0.33168718\n",
      "160 0.33185965 0.33168718\n",
      "170 0.33185965 0.33168718\n",
      "180 0.33185965 0.33168718\n",
      "190 0.33185965 0.33168718\n",
      "14\n",
      "0 1.6987507 1.713306\n",
      "10 0.12086484 0.119669855\n",
      "20 0.089490876 0.088407196\n",
      "30 0.08763037 0.08589818\n",
      "40 0.08605977 0.08437829\n",
      "50 0.08506032 0.08333079\n",
      "60 0.08315163 0.08171381\n",
      "70 0.08065991 0.07987735\n",
      "80 0.07930654 0.07844757\n",
      "90 0.077918775 0.077516235\n",
      "100 0.07714467 0.076826654\n",
      "110 0.07640948 0.07617259\n",
      "120 0.075762175 0.07543324\n",
      "130 0.07531195 0.07489577\n",
      "140 0.07502735 0.07461426\n",
      "150 0.07482709 0.07442001\n",
      "160 0.075169116 0.07465292\n",
      "170 0.074885964 0.0745342\n",
      "180 0.077121064 0.07628914\n",
      "190 0.07809453 0.07801813\n",
      "15\n",
      "0 0.10360524 0.10115888\n",
      "10 0.09344893 0.08989674\n",
      "20 0.08690243 0.08574187\n",
      "30 0.082864545 0.08199978\n",
      "40 0.078083694 0.07792726\n",
      "50 0.0723774 0.07275545\n",
      "60 0.063632786 0.064565\n",
      "70 0.053094596 0.053492274\n",
      "80 0.044687048 0.04620987\n",
      "90 0.0355325 0.036766876\n",
      "100 0.026837593 0.028111316\n",
      "110 0.021969497 0.0234161\n",
      "120 0.01848172 0.020459652\n",
      "130 0.015986677 0.018293854\n",
      "140 0.014187456 0.0164731\n",
      "150 0.013027233 0.015280506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 0.011846073 0.014223602\n",
      "170 0.01115736 0.013368486\n",
      "180 0.010195303 0.0125300195\n",
      "190 0.009778306 0.011979475\n",
      "16\n",
      "0 2.834365 2.841525\n",
      "10 0.10211123 0.10041694\n",
      "20 0.092757136 0.09120726\n",
      "30 0.0851514 0.083443224\n",
      "40 0.08308894 0.0818871\n",
      "50 0.0795648 0.07856903\n",
      "60 0.07578369 0.07530424\n",
      "70 0.07105794 0.07136026\n",
      "80 0.06552269 0.06647107\n",
      "90 0.057947684 0.059403278\n",
      "100 0.048607938 0.050125778\n",
      "110 0.0396663 0.040995385\n",
      "120 0.033573043 0.034936886\n",
      "130 0.027500706 0.028396493\n",
      "140 0.021437464 0.02266158\n",
      "150 0.0180314 0.019531457\n",
      "160 0.015847037 0.017491423\n",
      "170 0.0132163875 0.01521089\n",
      "180 0.011146696 0.013247659\n",
      "190 0.010126095 0.012114828\n",
      "17\n",
      "0 0.3318538 0.33168718\n",
      "10 0.09298815 0.09110982\n",
      "20 0.084512725 0.08182567\n",
      "30 0.07610579 0.07423656\n",
      "40 0.06269724 0.062103335\n",
      "50 0.04108754 0.04252241\n",
      "60 0.024642086 0.026172075\n",
      "70 0.012557116 0.014497295\n",
      "80 0.0072117983 0.008695833\n",
      "90 0.004747449 0.0058615017\n",
      "100 0.0050046053 0.0059035616\n",
      "110 0.0030771194 0.0038929968\n",
      "120 0.0022422515 0.0030012021\n",
      "130 0.0019449542 0.0026608733\n",
      "140 0.0014346233 0.0020996504\n",
      "150 0.0014172492 0.0020637193\n",
      "160 0.005769606 0.0062034703\n",
      "170 0.0020332907 0.0025815745\n",
      "180 0.0012963994 0.0018707974\n",
      "190 0.0010415652 0.0015836783\n",
      "18\n",
      "0 0.33185965 0.33168718\n",
      "10 0.33185965 0.33168718\n",
      "20 0.33185965 0.33168718\n",
      "30 0.33185965 0.33168718\n",
      "40 0.33185965 0.33168718\n",
      "50 0.33185965 0.33168718\n",
      "60 0.33185965 0.33168718\n",
      "70 0.33185965 0.33168718\n",
      "80 0.33185965 0.33168718\n",
      "90 0.33185965 0.33168718\n",
      "100 0.33185965 0.33168718\n",
      "110 0.33185965 0.33168718\n",
      "120 0.33185965 0.33168718\n",
      "130 0.33185965 0.33168718\n",
      "140 0.33185965 0.33168718\n",
      "150 0.33185965 0.33168718\n",
      "160 0.33185965 0.33168718\n",
      "170 0.33185965 0.33168718\n",
      "180 0.33185965 0.33168718\n",
      "190 0.33185965 0.33168718\n",
      "19\n",
      "0 1.2291458 1.2305039\n",
      "10 0.10824151 0.10725157\n",
      "20 0.09228244 0.09065842\n",
      "30 0.08710532 0.08578998\n",
      "40 0.084962524 0.08389361\n",
      "50 0.08316748 0.08223377\n",
      "60 0.08190111 0.08114877\n",
      "70 0.08010736 0.07958353\n",
      "80 0.07753926 0.077334486\n",
      "90 0.073346585 0.0736785\n",
      "100 0.06506698 0.066091016\n",
      "110 0.05428871 0.055581365\n",
      "120 0.042706985 0.044886414\n",
      "130 0.033130433 0.034409728\n",
      "140 0.025224887 0.025839405\n",
      "150 0.02455964 0.024833225\n",
      "160 0.017633636 0.017918859\n",
      "170 0.014681462 0.015372471\n",
      "180 0.012011247 0.0127446195\n",
      "190 0.01107403 0.011914568\n"
     ]
    }
   ],
   "source": [
    "layer_1_node_vals = np.arange(15,90,5)\n",
    "layer_2_node_vals = np.arange(300,510,5)\n",
    "layer_3_node_vals = np.arange(200,460,5)\n",
    "\n",
    "log_paths = []\n",
    "for layer_1_nodes in layer_1_node_vals:\n",
    "    for layer_2_nodes in layer_2_node_vals:\n",
    "        for layer_3_nodes in layer_3_node_vals:\n",
    "            \n",
    "            log_path = str(layer_1_nodes) + \"_\" + str(layer_2_nodes) +  \"_\" + str(layer_3_nodes)\n",
    "                \n",
    "            log_paths.append(log_path)\n",
    "            \n",
    "random_layers = np.random.choice(log_paths, size = 20)\n",
    "\n",
    "for i, random_layer in enumerate(random_layers):\n",
    "    print(i)\n",
    "    \n",
    "    nodes = np.fromstring(random_layer, sep = '_')\n",
    "    layer_1_nodes, layer_2_nodes, layer_3_nodes = int(nodes[0]), int(nodes[1]), int(nodes[2])\n",
    "    f(layer_1_nodes,layer_2_nodes,layer_2_nodes, logdir = 'Run_4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T06:11:41.993919Z",
     "start_time": "2018-05-25T06:11:41.984343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': True, 'meta_graph': False, 'run_metadata': []}\n"
     ]
    }
   ],
   "source": [
    "# from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "# event_acc = EventAccumulator('logs/Run_3/60_380_340/testing/events.out.tfevents.1527228830.Andrews-MacBook.local')\n",
    "# event_acc.Reload()\n",
    "# # Show all tags in the log file\n",
    "# print(event_acc.Tags())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
