{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Example code and text mostly lifted from  https://www.lynda.com/Google-TensorFlow-tutorials/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:48:31.352385Z",
     "start_time": "2018-05-24T18:48:31.335670Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fullwidth notebook cells\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:48:37.028590Z",
     "start_time": "2018-05-24T18:48:32.218013Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# suppress tensorflow FutureWarning\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "# for reading & slicing data\n",
    "import pandas as pd\n",
    "\n",
    "# for data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sales_data_training.csv` : Dataset of video games sold by an imaginary video game retailer. \n",
    "\n",
    "> We'll use this data to train the neural network that will predict how much money we can expect future video games to earn based on our historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.681809Z",
     "start_time": "2018-05-24T18:28:04.631203Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data_path = \"data/sales_data_training.csv\"\n",
    "training_data_df = pd.read_csv(training_data_path, dtype = float)\n",
    "\n",
    "# get a sense of the features present in the data\n",
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll use TensorFlow to build a neural network that tries to predict the total earnings of a new game, based on the other characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Separate the training data*\n",
    " - Split the training data into the inputs `X_training` and their known outputs `Y_training`\n",
    "\n",
    "> The X group is data about each video game that we'll pass into the neural network, and the Y group are the values we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.696519Z",
     "start_time": "2018-05-24T18:28:04.686607Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the total_earnings column from the X data (data to train with)\n",
    "# axis = 1 drops the column\n",
    "X_training = training_data_df.drop('total_earnings', axis = 1).values\n",
    "\n",
    "# retain only the total_earnings column (value to predict) for the Y data\n",
    "Y_training = training_data_df[['total_earnings']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sales_data_test.csv`: Another dataset of videogames.\n",
    "\n",
    "> Load in and seperate the testing data using the same logic as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.716670Z",
     "start_time": "2018-05-24T18:28:04.701310Z"
    }
   },
   "outputs": [],
   "source": [
    "testing_data_path = \"data/sales_data_test.csv\"\n",
    "testing_data_df = pd.read_csv(testing_data_path, dtype = float)\n",
    "\n",
    "X_testing = testing_data_df.drop('total_earnings', axis = 1).values\n",
    "Y_testing = testing_data_df[['total_earnings']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The machine learning system will only get to see the training data set during the training phase. Then we'll use this test data to check the accuracy of the predictions from our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need to pre-process our data. If the numbers in one column are large but the numbers in another column are small, the neural network training won't work very well. \n",
    "\n",
    "> In order to train the neural network, we want to scale all the numbers in each column of our data set to be between the value of 0 and 1.  One way we can do this is to use the MinMaxScaler object from the popular scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.726709Z",
     "start_time": "2018-05-24T18:28:04.720590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create scalars for inputs and outputs\n",
    "X_scaler = MinMaxScaler(feature_range = (0,1))\n",
    "Y_scaler = MinMaxScaler(feature_range = (0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform()`: fit to our data, and then transform the data using that fit. \n",
    "\n",
    "> The scaler fits the data by multiplying it by a constant and adding a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.738124Z",
     "start_time": "2018-05-24T18:28:04.730314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale training input and output\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transform()`: transform the data using the pre-computed fit\n",
    "> We want to make sure the test data is scaled by the same amount as the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.750607Z",
     "start_time": "2018-05-24T18:28:04.742399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale testing data using same scaler\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scale_`: scalaing factor\n",
    "\n",
    "`min_`: additive term\n",
    "\n",
    "> The predictions made with the NN will be scaled. We'll need to transform back to our original units and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.761907Z",
     "start_time": "2018-05-24T18:28:04.754208Z"
    }
   },
   "outputs": [],
   "source": [
    "msg = 'Note: Y values were scaled by multiplying by {:.10f} and adding {:.4f}'\n",
    "print(msg.format(Y_scaler.scale_[0], Y_scaler.min_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our training data set has nine input features, so we need nine inputs in our neural network. \n",
    "<br>We are only predicting a single value, so have only one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.772083Z",
     "start_time": "2018-05-24T18:28:04.765841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Let's have three layers in their neural network that will train to find the relationship between the inputs and the output. There are many different types of layers you can use in the neural network, but we're going to use the most straightforward type, a fully connected neural network layer. That means that every node in each layer is connected to every node in the following layer.\n",
    "\n",
    "> The first layer will have 50 nodes, the second layer will have 100 nodes, and the third layer will have 50 nodes again. To me, these layer sizes seem like a good starting point, but it's just a guess. Once the neural network is coded we can test out different layer sizes to see what layer size gives us the best accuracy.\n",
    "\n",
    "![title](img/sales_prediction_model_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.785135Z",
     "start_time": "2018-05-24T18:28:04.777434Z"
    }
   },
   "outputs": [],
   "source": [
    "# define how many neurons we want in each layer of our NN\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: Explain the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.795494Z",
     "start_time": "2018-05-24T18:28:04.789664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the layers of the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`variable_scope()`: A context manager for defining operations that creates variables (layers)\n",
    "\n",
    "> Normally in Python we organize our code by creating new functions. In TensorFlow we use variable scopes.\n",
    "\n",
    "<br>\n",
    "> Any variables we create within the scope \"input\" will automatically get a prefix of \"input\" to their name internally in TensorFlow.\n",
    "\n",
    "> TensorFlow has the ability to generate diagrams of the computational graph. By putting our nodes into scopes it helps TensorFlow generate more useful diagrams that are easier to understand. Everything within the same scope will be grouped together within the diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`placeholder()`: Inserts a placeholder for a tensor that will be always fed\n",
    "\n",
    "`X`: The input to our NN, a placeholder.\n",
    "\n",
    "> Our neural network should accept nine floating point numbers as the input for making predictions, but each time we want a new prediction the specific values we pass in will be different. We use a placeholder node to represent that.\n",
    "\n",
    "<br>\n",
    "> When we create a new node we need to tell it what type of tensor it will accept. The data we are passing into our network will be floating point numbers, `tf.float32`. \n",
    "\n",
    "<br>\n",
    "`shape = (None, number_of_inputs)`: The shape of the tensor for the model to expect.\n",
    "> \n",
    " - `None` tells TensorFlow our neural network can mix up batches of any size\n",
    " - `number_of_inputs` tells it to expect nine values for each record in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:04.825405Z",
     "start_time": "2018-05-24T18:28:04.798430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X  = tf.placeholder(tf.float32, shape = (None, number_of_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each fully connected layer of the neural network has three parts.\n",
    " - A bias value for each node\n",
    " - A weight value for each connection between each node and the nodes in the previous layer.\n",
    " - An activation function that outputs the result of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_variable()`: Creates a new variable or gets an existing variable. \n",
    "\n",
    "##### biases\n",
    "`biases`: store the bias values for each node\n",
    ">  This will be a variable instead of a placeholder because we want TensorFlow to remember the value over time.\n",
    "\n",
    "`shape = [layer_1_nodes]`\n",
    "\n",
    "> There's one bias value for each node in this layer, so the shape should be the same as the number of nodes in the layer.\n",
    "\n",
    "`initializer = tf.zeros_initializer()`\n",
    "\n",
    "> We need to tell TensorFlow the initial value of this variable. We can tell TensorFlow how to set the initial value of a variable by passing it one of TensorFlow's built-in initializer functions. We want the bias values for each node to default to zero, so use `tf.zeros_initializer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weights\n",
    "\n",
    "`weights`: Store the weights for this layer\n",
    "\n",
    "`shape  = [number_of_inputs, layer_1_nodes]`\n",
    "> We want to have one weight for each node's connection to each node in the previous layer.  We'll say shape equals an array, one side of the array will be number of inputs, and the other side will be layer_1_nodes.\n",
    "\n",
    "`initializer = tf.contrib.layers.xavier_initializer()`\n",
    "> With neural networks, a lot of research has gone into the best initial values to use for weights. A good choice is an algorithm called Xavier initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### layer output\n",
    "`tf.matmul()`: Multiplies two matrices\n",
    "\n",
    "`tf.nn.relu()`: Recified linear unit https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "`layer_1_output`\n",
    "> The last part of defining this layer is multiplying the weights by the inputs and calling an activation function.\n",
    "\n",
    "> We're going to use matrix multiplication and a standard rectified linear unit or relu activation function. \n",
    "\n",
    "> We multiply the inputs, X, by the weights in this layer. To that we'll add the biases. Then we wrap that with a call to `relu()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:12.515920Z",
     "start_time": "2018-05-24T18:28:04.828518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    \n",
    "    biases = tf.get_variable(name = \"biases1\",\n",
    "                             shape = [layer_1_nodes],\n",
    "                             initializer = tf.zeros_initializer())\n",
    "    \n",
    "    weights = tf.get_variable(name = \"weights1\",\n",
    "                              shape  = [number_of_inputs, layer_1_nodes],\n",
    "                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer 2 \n",
    "> Similar to layer 1\n",
    " - change names\n",
    " - change shapes\n",
    " - change `matmul()` to take in the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:12.560861Z",
     "start_time": "2018-05-24T18:28:12.519324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    \n",
    "    biases = tf.get_variable(name = \"biases2\",\n",
    "                             shape = [layer_2_nodes],\n",
    "                             initializer = tf.zeros_initializer())\n",
    "    \n",
    "    weights = tf.get_variable(name = \"weights2\",\n",
    "                              shape  = [layer_1_nodes, layer_2_nodes],\n",
    "                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer 3\n",
    "> Similar to layer 2\n",
    " - change names\n",
    " - change shapes\n",
    " - change `matmul()` to take in the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:12.593060Z",
     "start_time": "2018-05-24T18:28:12.563884Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    \n",
    "    biases = tf.get_variable(name = \"biases3\",\n",
    "                             shape = [layer_3_nodes],\n",
    "                             initializer = tf.zeros_initializer())\n",
    "    \n",
    "    weights = tf.get_variable(name = \"weights3\",\n",
    "                              shape  = [layer_2_nodes, layer_3_nodes],\n",
    "                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output layer \n",
    "> Similar to layer 3\n",
    " - change names\n",
    " - change shapes - use number_of_outputs\n",
    " - change `matmul()` to take in the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:12.637535Z",
     "start_time": "2018-05-24T18:28:12.596163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output layer\n",
    "\n",
    "with tf.variable_scope('output'):\n",
    "    \n",
    "    biases = tf.get_variable(name = \"biases_out\",\n",
    "                             shape = [number_of_outputs],\n",
    "                             initializer = tf.zeros_initializer())\n",
    "    \n",
    "    weights = tf.get_variable(name = \"weights_out\",\n",
    "                              shape  = [layer_3_nodes, number_of_outputs],\n",
    "                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the cost function that the NN will use to measure predictions\n",
    "\n",
    "`Y`: A node for the expected value that we'll feed in during training\n",
    "\n",
    "> Just like the input values it will be a placeholder node because we'll feed in a new value each time.\n",
    " - For the shape in this case we'll pass in `(None, 1)` since there's just one single output.\n",
    "\n",
    "`cost`\n",
    "> A cost function, also called a lost function tells us how wrong the neural network is when trying to predict the correct output for a single piece of training data.\n",
    "\n",
    "`squared_difference()`: Element wise (x-y)(x-y)\n",
    "`reduce_mean()`: Mean of elements across tensor dimensions\n",
    "> To measure the cost we'll calculate the mean squared error between what the neural network predicted and what we expected it to calculate. To do that we'll call the `squared_difference()` function and pass in the actual predication and the expected value.\n",
    "\n",
    "> To turn it into a mean square difference, we want to get the average value of that difference. So we'll wrap that with a call to `reduce_mean()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:12.659794Z",
     "start_time": "2018-05-24T18:28:12.641454Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('cost'):\n",
    "    \n",
    "    Y = tf.placeholder(tf.float32, shape = (None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer\n",
    "`AdamOptimizer`: Powerful standard optimizer.\n",
    "\n",
    ">The very last step is to create an optimizer operation that TensorFlow can call to train the network.\n",
    "> We just need to pass in the learning rate which we've already pre-defined above. \n",
    "\n",
    "`minimize(cost)`: Which variable we want it to minimize.\n",
    "> Tells TensorFlow that whenever we tell it to execute the optimizer, it should run one iteration of the Adam optimizer in an attempt to make the cost value smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:12.884184Z",
     "start_time": "2018-05-24T18:28:12.663221Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable logging (for TensorBoard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Being able to visualize your data is very helpful. However, you have to tell TensorFlow to log the values you want to visualize.\n",
    "\n",
    "`summary.scalar()`\n",
    "> In TensorFlow, we log values by creating special operations in our graph called summary operations. These operations take in the value and create log data in a format that TensorBoard can understand. Then, we pass that summary data to a TensorFlow file writer object to save it to disk.\n",
    "\n",
    "`tf.summary.scalar('current_cost', cost)`: Represents the value we are logging\n",
    "> We can run this node by calling session.run on it just like any other node in our graph. Running it will generate the log data in the right format.\n",
    "\n",
    "`summary = tf.summary.merge_all()`: Merge all summaries, helper function.\n",
    "> Sometimes you'll want to log many different metrics. It can be tedious to have to call session.run on every single metric so TensorFlow has a shortcut. \n",
    "\n",
    "> Automatically executes all the summary nodes in your graph without you having to explicitly list them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:12.929909Z",
     "start_time": "2018-05-24T18:28:12.887525Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train.Saver()`: To save our model after we train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:13.133081Z",
     "start_time": "2018-05-24T18:28:12.934749Z"
    }
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the model training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Session()`: For running TF sessions. (`InteractiveSession()` for code run over multiple cells))\n",
    "> Within a session, we can ask TensorFlow to execute commands by calling `session.run()` and then we can pass in the command we want TensorFlow to execute. Those can either be global commands that TensorFlow provides or specific nodes in our graph that we want to execute. \n",
    "\n",
    "`session.run(tf.global_variables_initializer())`\n",
    "> The first command we always run is the built in command to tell TensorFlow to initialize all variables in our graph to their default values.\n",
    "\n",
    "`session.run(optimizer)`\n",
    "> To train our neural network, we'll run its optimizer function over and over in the loop, either a fixed number of times or until it hits an accuracy level we want.  Inside the loop we'll tell TensorFlow to execute a single training pass over the training data by calling the optimizer function.\n",
    "\n",
    "> We can do this by calling `session.run()` and then pass in a reference to the operation that we want to call. In this case, that's the optimizer operation that we defined above. \n",
    "\n",
    "`feed_dict`: Dictionary contating the data the optimizer needs to run\n",
    "> It needs the training data and the expected results for this training pass. In our computational graph, we have a placeholder node called `X` that accepts the training data and a placeholder node called `Y` that accepts the expected results. To feed values into a placeholder node, we can pass them in as a parameter called feed_dict.\n",
    "\n",
    "`summary.FileWriter()`: Write summary data to files.\n",
    "> To create the log files to save our data to. \n",
    " - If you put multiple log files in the same top-level folder, TensorBoard will show them all together and let you flip between them.\n",
    "\n",
    "<br>\n",
    "> We run the session once for each epoch, logging data every 5. After training we compare a prediction with known data, and save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T18:28:14.043857Z",
     "start_time": "2018-05-24T18:28:13.136535Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize a session to run TF operations\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # Run the global variable initilizer to init all variables and layers\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log writers to record training progress\n",
    "    # Store training and testing data separately\n",
    "    training_writer = tf.summary.FileWriter('logs/training', session.graph)\n",
    "    testing_writer = tf.summary.FileWriter('logs/testing', session.graph)\n",
    "    \n",
    "    \n",
    "    # Run the optimizer over and over to train the network\n",
    "    # One epoch is one full run through the training data set\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one stepf NN training\n",
    "        session.run(optimizer,\n",
    "                    feed_dict = {X: X_scaled_training, Y: Y_scaled_training})\n",
    "\n",
    "        # Every 5 steps, log our process\n",
    "        if epoch % 5 == 0:\n",
    "            \n",
    "            training_feed = {X: X_scaled_training, Y: Y_scaled_training}\n",
    "            training_cost, training_summary = session.run([cost, summary],\n",
    "                                                    feed_dict = training_feed)\n",
    "            \n",
    "            testing_feed =  {X: X_scaled_testing, Y: Y_scaled_testing}\n",
    "            testing_cost, testing_summary = session.run([cost, summary],\n",
    "                                                     feed_dict = testing_feed)\n",
    "            \n",
    "            print(epoch, training_cost, testing_cost)\n",
    "            \n",
    "            # write the current status to the log files\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "    print('Training done')\n",
    "    \n",
    "    final_training_cost = session.run(cost,\n",
    "                                      feed_dict = {X: X_scaled_training,\n",
    "                                                   Y: Y_scaled_training})\n",
    "    \n",
    "    final_testing_cost = session.run(cost,\n",
    "                                     feed_dict = {X: X_scaled_testing,\n",
    "                                                  Y: Y_scaled_testing})\n",
    "\n",
    "    print('Final Training Cost: {}'.format(final_training_cost))\n",
    "    print('Final Testing Cost: {}'.format(final_testing_cost))\n",
    "    \n",
    "    \n",
    "    # Now that the NN is trained, lets use it to make predictions.\n",
    "    # pass in the X testing data and run the prediction operation\n",
    "    Y_prediction_scaled = session.run(prediction,\n",
    "                                      feed_dict = {X:X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to its original units (dollar$)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_prediction_scaled)\n",
    "    \n",
    "    # actual earnings of 0th game\n",
    "    real_earnings = testing_data_df['total_earnings'].values[0]\n",
    "    \n",
    "    # predicted_earnings of 0th game\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "    \n",
    "    print('The actual earnings of Game #1 were ${}'.format(real_earnings))\n",
    "    \n",
    "    msg = 'The predicted earnings of Game #1 were ${}'\n",
    "    print(msg.format(predicted_earnings))\n",
    "    \n",
    "    model_save_location = \"logs/trained_model.ckpt\"\n",
    "    save_path = saver.save(session, model_save_location)\n",
    "    print('Model saved: {}'.format(save_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The logs can be viewed in tensorboard\n",
    "\n",
    "in terminal:\n",
    "`tensorboard --logdir =logs`\n",
    "\n",
    " - then open a new browser in the link provided: `localhost:6006`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Current Cost](img/logged_current_cost_plot_ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![graph](img/graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
