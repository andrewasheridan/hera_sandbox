{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '../modules'))\n",
    "from data_manipulation import *\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Data_Creator_GAN(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_flatnesses,\n",
    "                 bl_data = None,\n",
    "                 bl_dict = None,\n",
    "                 gains = None):\n",
    "        \n",
    "        self._num = num_flatnesses\n",
    "                    \n",
    "        self._bl_data = bl_data\n",
    "        self._bl_data_c = None\n",
    "        \n",
    "        self._bl_dict = bl_dict\n",
    "        \n",
    "        self._gains = gains\n",
    "        self._gains_c = None\n",
    "        \n",
    "        self._epoch_batch = []\n",
    "\n",
    "    def gen_data(self):\n",
    "        \n",
    "        self._thread = Thread(target = self._gen_data, args=())\n",
    "        self._thread.start()\n",
    "\n",
    "    def get_data(self, timeout = 10):\n",
    "\n",
    "        if len(self._epoch_batch) == 0:\n",
    "            self._thread.join(timeout)\n",
    "            \n",
    "        return self._epoch_batch.pop(0)\n",
    "\n",
    "    def _gen_data(self):\n",
    "\n",
    "        angle_tx  = lambda x: (np.asarray(x) + np.pi) / (2. * np.pi)\n",
    "        angle_itx = lambda x: np.asarray(x) * 2. * np.pi - np.pi\n",
    "\n",
    "        if self._bl_data_c == None:\n",
    "            self._bl_data_c = {key : self._bl_data[key].conjugate() for key in self._bl_data.keys()}\n",
    "\n",
    "        if self._gains_c == None:\n",
    "            self._gains_c = {key : self._gains[key].conjugate() for key in self._gains.keys()}\n",
    "\n",
    "\n",
    "        def _flatness(seps):\n",
    "            \"\"\"Create a flatness from a given pair of seperations, their data & their gains.\"\"\"\n",
    "\n",
    "            a, b = seps[0][0], seps[0][1]\n",
    "            c, d = seps[1][0], seps[1][1]\n",
    "\n",
    "\n",
    "            return self._bl_data[seps[0]]   * self._gains_c[(a,'x')] * self._gains[(b,'x')] * \\\n",
    "                   self._bl_data_c[seps[1]] * self._gains[(c,'x')]   * self._gains_c[(d,'x')]\n",
    "\n",
    "        inputs = []\n",
    "        for _ in range(self._num):\n",
    "\n",
    "            unique_baseline = random.sample(self._bl_dict.keys(), 1)[0]\n",
    "            two_seps = [random.sample(self._bl_dict[unique_baseline], 2)][0]\n",
    "\n",
    "            inputs.append(_flatness(two_seps))\n",
    "            \n",
    "\n",
    "        inputs = np.angle(np.array(inputs).reshape(-1,1024))\n",
    "        \n",
    "\n",
    "        self._epoch_batch.append(angle_tx(inputs))\n",
    "\n",
    "\n",
    "def model_opt(d_loss, g_loss):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    \"\"\"\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): \n",
    "        d_train_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer().minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt\n",
    "\n",
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    inputs_real = tf.placeholder(tf.float32, shape=(None, 60, 1024, 1), name='input_real') \n",
    "    inputs_z = tf.placeholder(tf.float32, (None, 4096), name='input_z')\n",
    "    \n",
    "    return inputs_real, inputs_z\n",
    "\n",
    "def discriminator(inputs_real, reuse=False, is_training = True):\n",
    "    \"\"\"\n",
    "    Create the discriminator network\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('discriminator', reuse = reuse):\n",
    "        with tf.variable_scope('conv_1'):\n",
    "            conv_1 = tf.layers.conv2d(inputs_real, 2, (3, 3), (2, 2), 'SAME')\n",
    "            conv_1 = tf.layers.batch_normalization(conv_1, training = is_training)\n",
    "            conv_1 = tf.nn.leaky_relu(conv_1)\n",
    "\n",
    "        with tf.variable_scope('conv_2'):\n",
    "            conv_2 = tf.layers.conv2d(conv_1, 2, (5, 5), (2, 2), 'SAME')\n",
    "            conv_2 = tf.layers.batch_normalization(conv_2, training = is_training)\n",
    "            conv_2 = tf.nn.leaky_relu(conv_2)\n",
    "            # width = 512, 2 filters\n",
    "\n",
    "        with tf.variable_scope('conv_3'):\n",
    "            conv_3 = tf.layers.conv2d(conv_2, 4, (3, 3), (2, 2), 'SAME')\n",
    "            conv_3 = tf.layers.batch_normalization(conv_3, training = is_training)\n",
    "            conv_3 = tf.nn.leaky_relu(conv_3) \n",
    "            # width = 512, 4 filters\n",
    "\n",
    "        with tf.variable_scope('conv_4'):\n",
    "            conv_4 = tf.layers.conv2d(conv_3, 4, (5, 5), (2, 2), 'SAME')\n",
    "            conv_4 = tf.layers.batch_normalization(conv_4, training = is_training)\n",
    "            conv_4 = tf.nn.leaky_relu(conv_4) \n",
    "            # width = 256, 4 filters\n",
    "\n",
    "        with tf.variable_scope('output'):\n",
    "            logits = tf.layers.dense(tf.layers.flatten(conv_4), 1)\n",
    "            disc_out = tf.nn.softmax(logits)\n",
    "    return disc_out, logits\n",
    "\n",
    "\n",
    "#inputs_z = tf.placeholder(tf.float32, (1, 2048), name='input_z')\n",
    "def generator(z, is_training = True):\n",
    "    \"\"\"\n",
    "    Create the generator network\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope('generator', reuse = False if is_training == True else True):\n",
    "        with tf.variable_scope('deconv_1'):\n",
    "            deconv_1 = tf.layers.dense(z, 2048)\n",
    "            deconv_1 = tf.reshape(deconv_1, (-1, 1, 1, 2048))\n",
    "            deconv_1 = tf.layers.conv2d_transpose(deconv_1, 1792, (5, 5), (1, 3), 'SAME')\n",
    "            deconv_1 = tf.layers.batch_normalization(deconv_1, training = is_training)\n",
    "            deconv_1 = tf.nn.leaky_relu(deconv_1) \n",
    "\n",
    "\n",
    "        with tf.variable_scope('deconv_2'):\n",
    "\n",
    "            deconv_2 = tf.layers.conv2d_transpose(deconv_1, 1536, (3, 3), (1, 2), 'SAME')\n",
    "            deconv_2 = tf.layers.batch_normalization(deconv_2, training = is_training)\n",
    "            deconv_2 = tf.nn.leaky_relu(deconv_2) \n",
    "\n",
    "        with tf.variable_scope('deconv_3'):\n",
    "\n",
    "            deconv_3 = tf.layers.conv2d_transpose(deconv_2, 1280, (5, 5), (1, 2), 'SAME')\n",
    "            deconv_3 = tf.layers.batch_normalization(deconv_3, training = is_training)\n",
    "            deconv_3 = tf.nn.leaky_relu(deconv_3) \n",
    "\n",
    "        with tf.variable_scope('deconv_4'):\n",
    "\n",
    "            deconv_4 = tf.layers.conv2d_transpose(deconv_3, 1024, (3, 3), (1, 5), 'SAME')\n",
    "            deconv_4 = tf.layers.batch_normalization(deconv_4, training = is_training)\n",
    "            deconv_4 = tf.nn.leaky_relu(deconv_4) \n",
    "\n",
    "        with tf.variable_scope('out'):\n",
    "            gen_out = tf.reshape(deconv_4, (-1, 60, 1024, 1))\n",
    "    \n",
    "    return gen_out\n",
    "\n",
    "\n",
    "\n",
    "def model_loss(input_real, input_z):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    \"\"\"\n",
    "    \n",
    "    label_smoothing = 0.9\n",
    "    \n",
    "    g_model = generator(input_z)\n",
    "    d_model_real, d_logits_real = discriminator(input_real)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                labels=tf.ones_like(d_model_real) * label_smoothing))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                labels=tf.zeros_like(d_model_fake)))\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "                                                  \n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                labels=tf.ones_like(d_model_fake) * label_smoothing))\n",
    "    \n",
    "    \n",
    "    return d_loss, g_loss\n",
    "\n",
    "def train(num_epochs, batch_size, bl_data, bl_dict, gains):\n",
    "    \"\"\"\n",
    "    Train the GAN\n",
    "    \"\"\"\n",
    "    input_real, input_z = model_inputs()\n",
    "    d_loss, g_loss  = model_loss(input_real, input_z)\n",
    "    d_opt, g_opt = model_opt(d_loss, g_loss)\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    train_batcher = Data_Creator_GAN(1,\n",
    "                                     bl_data,\n",
    "                                     bl_dict,\n",
    "                                     gains)\n",
    "    train_batcher.gen_data()\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print('epoch {}'.format(epoch))\n",
    "            \n",
    "            batches_real = train_batcher.get_data(); train_batcher.gen_data()\n",
    "            num_entries = batches_real.shape[0]\n",
    "            \n",
    "            for j in range(int(num_entries/batch_size)):\n",
    "                \n",
    "                batch_real = batches_real[j*batch_size:(j + 1)*batch_size].reshape(-1,60,1024,1)\n",
    "                \n",
    "                steps += 1\n",
    "            \n",
    "                batch_z = np.random.uniform(0, 1, size=(batch_size, 4096))\n",
    "                \n",
    "            _ = sess.run(d_opt, feed_dict={input_real: batch_real, input_z: batch_z})\n",
    "            _ = sess.run(g_opt, feed_dict={input_real: batch_real, input_z: batch_z})\n",
    "\n",
    "            # At the end of every 10 epochs, get the losses and print them out\n",
    "            train_loss_d = d_loss.eval({input_z: batch_z, input_real: batch_real})\n",
    "            train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "\n",
    "            print(\"Epoch {}...\".format(epoch),\n",
    "                  \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                  \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "\n",
    "        samples = sess.run(generator(input_z, False),\n",
    "                           feed_dict={input_z: batch_z})\n",
    "    return samples, batch_real\n",
    "\n",
    "test = train(10,60,training_baselines_data,training_redundant_baselines_dict, gains)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
